{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "781290e3-f538-47f4-9182-a6341b0aabf9",
   "metadata": {},
   "source": [
    "Tutorial on using SAE on GPT-2 from scratch.\n",
    "\n",
    "Takeaways:\n",
    "- People will understand how to do mechnanistic interpretability, in an end-to-end manner.\n",
    "- People can apply this same code here, to investigate other models by just changing the Huggingface model name.\n",
    "- The necessary visualizations have been taken care of.\n",
    "- People have the right tooling to do the job. The tooling is even more general-purpose in that it can be used to understand model inner-working, work with many deep learning paradigm (NLP, CV, RL, ES...).\n",
    "- Library functionality:\n",
    "  - Fast inference.\n",
    "  - Can run remote inference.\n",
    "  - Hook up and integrate well to other frameworks, programming langugages (e.g. llama.cpp...)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed164f4b-d517-4932-9a34-b5ba431f1673",
   "metadata": {},
   "source": [
    "### Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68ddda6f-d442-442e-bfa3-2e54050bb59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b29e0af6-6704-43b4-bb28-656f1b0f6fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/john/transformers/src/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2SdpaAttention(\n",
      "          (c_attn): Conv1D(nf=2304, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=768)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D(nf=3072, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=3072)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from dawnet.model import ModelRunner\n",
    "\n",
    "model_id = \"openai-community/gpt2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model = model.eval()\n",
    "runner = ModelRunner(model)\n",
    "print(runner._model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83f1dc61-f3b1-4ee9-92ec-68f4f9fff53d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5eca7b3d-9154-4557-a831-ef2d896d53bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_texts(input_path, output_path):\n",
    "    from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=750, chunk_overlap=20, length_function=lambda x: len(tokenizer.encode(x)), is_separator_regex=False\n",
    "    )\n",
    "    df = pd.read_parquet(input_path)\n",
    "    print(df.shape)\n",
    "    texts = [each['text'] for _, each in df.iterrows() if len(each['text']) > 150]\n",
    "    print(len(texts))\n",
    "\n",
    "    splitted_texts = []\n",
    "    for text in tqdm(texts):\n",
    "        splitted_texts += [each for each in text_splitter.split_text(text) if len(each) > 150]\n",
    "    with open(output_path, 'w') as fo:\n",
    "        json.dump(splitted_texts, fo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfe579dd-71d6-4df9-8fd4-0fe773bfbcb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(156288, 4)\n",
      "153291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 153291/153291 [14:38<00:00, 174.42it/s]\n"
     ]
    }
   ],
   "source": [
    "stem = \"train-00006-of-00041\"\n",
    "split_texts(f\"/data/mech/data/wikipedia/{stem}.parquet\", f\"/data/mech/data/splitted/{stem}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "083729ac-1c43-4039-9fa9-a1810d935706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 287260 text lines\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 287260/287260 [8:28:18<00:00,  9.42it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_intermediate(text_file, path, layers, batch_size=2000):\n",
    "    with open(text_file) as fi:\n",
    "        texts = json.load(fi)\n",
    "    print(f\"There are {len(texts)} text lines\")\n",
    "    try:\n",
    "        handler1 = runner.cache_outputs(*layers)\n",
    "        intermediates = {layer: [] for layer in layers}\n",
    "        with torch.no_grad():\n",
    "            for idx, text in enumerate(tqdm(texts)):\n",
    "                text = \"<|endoftext|>\" + text\n",
    "                input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(model.device)\n",
    "                runner(input_ids)\n",
    "                for layer in layers:\n",
    "                    intermediates[layer].append(runner._output[layer][0].cpu().squeeze().numpy()[1:])\n",
    "                if idx >= batch_size and idx % batch_size == 0:\n",
    "                    for layer in layers:\n",
    "                        output_folder = Path(path) / layer\n",
    "                        output_folder.mkdir(exist_ok=True, parents=True)\n",
    "                        stem = Path(text_file).stem\n",
    "                        np.save(str(output_folder / f\"{stem}_{idx}.pth\"), np.concatenate(intermediates[layer]))\n",
    "                    intermediates = {layer: [] for layer in layers}\n",
    "    finally:\n",
    "        handler1.clear()\n",
    "\n",
    "# get_intermediate(\n",
    "#     \"/data/mech/data/splitted/train-00001-of-00041.json\",\n",
    "#     path=\"/data/mech/data/layers\",\n",
    "#     layers=[\"transformer.h.10\"]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ac1c1d-4fc7-4476-ad0e-c9f71203e9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_autoregressive_intermediates(text_file, path, layers, batch_size=2000):\n",
    "    with open(text_file) as fi:\n",
    "        texts = json.load(fi)\n",
    "    print(f\"There are {len(texts)} text lines\")\n",
    "    try:\n",
    "        handler1 = runner.cache_outputs(*layers)\n",
    "        intermediates = {layer: [] for layer in layers}\n",
    "        with torch.no_grad():\n",
    "            for idx, text in enumerate(tqdm(texts)):\n",
    "                text = \"<|endoftext|>\" + text\n",
    "                input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(model.device)\n",
    "                runner(input_ids)\n",
    "                for layer in layers:\n",
    "                    intermediates[layer].append(runner._output[layer][0].cpu().squeeze().numpy()[1:])\n",
    "                if idx >= batch_size and idx % batch_size == 0:\n",
    "                    for layer in layers:\n",
    "                        output_folder = Path(path) / layer\n",
    "                        output_folder.mkdir(exist_ok=True, parents=True)\n",
    "                        stem = Path(text_file).stem\n",
    "                        np.save(str(output_folder / f\"{stem}_{idx}.pth\"), np.concatenate(intermediates[layer]))\n",
    "                    intermediates = {layer: [] for layer in layers}\n",
    "    finally:\n",
    "        handler1.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d571e234-449a-49e0-9523-649168cfaf50",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m     ot \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(output_tss)\n\u001b[1;32m     23\u001b[0m     np\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/data/mech/data/output/it_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m07d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m, it)\n\u001b[0;32m---> 24\u001b[0m     \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/data/mech/data/output/ot_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43midx\u001b[49m\u001b[38;5;132;43;01m:\u001b[39;49;00m\u001b[38;5;124;43m07d\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.npy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     text_tss, intermediate_tss, output_tss \u001b[38;5;241m=\u001b[39m [], [], []\n\u001b[1;32m     26\u001b[0m idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/dawnet/lib/python3.10/site-packages/numpy/lib/npyio.py:546\u001b[0m, in \u001b[0;36msave\u001b[0;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m file_ctx \u001b[38;5;28;01mas\u001b[39;00m fid:\n\u001b[1;32m    545\u001b[0m     arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masanyarray(arr)\n\u001b[0;32m--> 546\u001b[0m     \u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_pickle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfix_imports\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfix_imports\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dawnet/lib/python3.10/site-packages/numpy/lib/format.py:730\u001b[0m, in \u001b[0;36mwrite_array\u001b[0;34m(fp, array, version, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    729\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m isfileobj(fp):\n\u001b[0;32m--> 730\u001b[0m         \u001b[43marray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtofile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    732\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m numpy\u001b[38;5;241m.\u001b[39mnditer(\n\u001b[1;32m    733\u001b[0m                 array, flags\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexternal_loop\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbuffered\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzerosize_ok\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    734\u001b[0m                 buffersize\u001b[38;5;241m=\u001b[39mbuffersize, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "text_tss, intermediate_tss, output_tss = [], [], []\n",
    "\n",
    "idx = 1001\n",
    "while idx <= 5000:\n",
    "    with torch.no_grad():\n",
    "        # store the input text\n",
    "        # store the intermediate layer\n",
    "        # store the output logits\n",
    "        text = \"<|endoftext|>\" + new_texts[idx]\n",
    "        input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(model.device)\n",
    "        output = runner(input_ids)\n",
    "        intermediate_ts = runner._output[\"transformer.h.10\"][0].cpu().squeeze().numpy()\n",
    "        output_ts = output[\"logits\"].cpu().squeeze().numpy()\n",
    "\n",
    "        text_tss.append(text)\n",
    "        intermediate_tss.append(intermediate_ts)\n",
    "        output_tss.append(output_ts)\n",
    "        if idx % 20 == 0:\n",
    "            with open(f\"/data/mech/data/output/texts_{idx:07d}.json\", \"w\") as fo:\n",
    "                json.dump(text_tss, fo)\n",
    "            it = np.concatenate(intermediate_tss)\n",
    "            ot = np.concatenate(output_tss)\n",
    "            np.save(f\"/data/mech/data/output/it_{idx:07d}.npy\", it)\n",
    "            np.save(f\"/data/mech/data/output/ot_{idx:07d}.npy\", ot)\n",
    "            text_tss, intermediate_tss, output_tss = [], [], []\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1c983cab-5e04-4e42-abd6-a39d325fd907",
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate = runner._output[\"transformer.h.10\"][0].cpu().squeeze().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "056c96ae-b16d-4e7f-9577-4c6630fdf536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(379, 768)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intermediate.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "58692977-8249-4087-86ea-d70bb4c95d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_ts = output[\"logits\"].cpu().squeeze().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "809f9a17-d917-4abd-97eb-e3af57b52802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 379, 50257])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['logits'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6b6a8efb-4140-409f-9017-2eabea322d42",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m79647\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(new_texts) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1000000\u001b[39m:\n\u001b[0;32m----> 3\u001b[0m     new_texts \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m text_splitter\u001b[38;5;241m.\u001b[39msplit_text(\u001b[43mtexts\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m      4\u001b[0m     idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "new_texts = []\n",
    "idx = 0\n",
    "while len(new_texts) < 1000000:\n",
    "    new_texts += text_splitter.split_text(texts[idx])\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c15a735-cc09-44d0-8ee1-1a3e713302bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
