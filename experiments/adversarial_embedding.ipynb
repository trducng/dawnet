{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "459fcf65-03ca-481c-8730-9f1bc92ad7ca",
   "metadata": {},
   "source": [
    "# Adversarial embedding\n",
    "\n",
    "In image, adversarial examples can readily constructed. In LLM, the embedding layer fixes the vector value for each token, making it hard to perform this attack. Now, how should we do this? However, it's still interesting to see how the transformer layers robust against perturbations inside the embedding layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b52cb66c-f099-45a7-ab27-39664bf695d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d07ca718-63e5-475d-abb2-ac60dce28906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d523c5bc45c94b41b6c36d9a567175f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7624a131ff974c929dea1bd9fb2842c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen3ForCausalLM(\n",
      "  (model): Qwen3Model(\n",
      "    (embed_tokens): Embedding(151936, 2560)\n",
      "    (layers): ModuleList(\n",
      "      (0-35): 36 x Qwen3DecoderLayer(\n",
      "        (self_attn): Qwen3Attention(\n",
      "          (q_proj): Linear(in_features=2560, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=2560, bias=False)\n",
      "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "        )\n",
      "        (mlp): Qwen3MLP(\n",
      "          (gate_proj): Linear(in_features=2560, out_features=9728, bias=False)\n",
      "          (up_proj): Linear(in_features=2560, out_features=9728, bias=False)\n",
      "          (down_proj): Linear(in_features=9728, out_features=2560, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
      "    (rotary_emb): Qwen3RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2560, out_features=151936, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_id = \"Qwen/Qwen3-4B\"\n",
    "device = torch.device(\"mps\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=device)\n",
    "original_model = AutoModelForCausalLM.from_pretrained(model_id, device_map=device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13781a46-dfd6-478c-9a74-e5ecb538bbc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When John and Mary went to the shop, John gave the bag to\n",
      "tensor([[ 4498,  3757,   323, 10244,  3937,   311,   279,  8061,    11,  3757,\n",
      "          6551,   279,  8968,   311]], device='mps:0')\n",
      " Mary tensor(10244, device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "prompt = \"When John and Mary went to the shop, John gave the bag to\"\n",
    "toks = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "print(prompt)\n",
    "print(toks)\n",
    "\n",
    "with torch.no_grad():\n",
    "  out = model(toks)\n",
    "  logits = out.logits\n",
    "  print(tokenizer.decode(logits[0,-1].argmax()), logits[0,-1].argmax())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f668ce-6393-4933-a9cf-6955ffd39af0",
   "metadata": {},
   "source": [
    "## Intervene in the last token only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c0b876-e346-4af7-8cf2-f5a4ce6cb43a",
   "metadata": {},
   "source": [
    "### Flip the prediction to a target by changing the embedding of last token\n",
    "\n",
    "Now the question is how minimal the embedding of the last token be modified to sway the result not to \"Mary\" and steer towards \"John\". It takes quite a lot of changes to push the result to \"John\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c25ff10b-0e29-4742-81b9-1a8b729b943d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [' Mary', ' the', ' a'] cosine: 0.9999999403953552\n",
      "1 [' Mary', ' the', ' a'] cosine: 0.999849259853363\n",
      "2 [' Mary', ' the', ' a'] cosine: 0.9994126558303833\n",
      "3 [' Mary', ' the', ' a'] cosine: 0.9987058043479919\n",
      "4 [' Mary', ' the', ' a'] cosine: 0.9977388978004456\n",
      "5 [' Mary', ' the', ' a'] cosine: 0.9965208172798157\n",
      "6 [' Mary', ' the', ' a'] cosine: 0.9950621128082275\n",
      "7 [' Mary', ' the', ' a'] cosine: 0.993376612663269\n",
      "8 [' the', ' Mary', ' a'] cosine: 0.9914787411689758\n",
      "9 [' the', ' Mary', ' a'] cosine: 0.9893377423286438\n",
      "10 [' Mary', ' the', ' a'] cosine: 0.9867074489593506\n",
      "11 [' the', ' Mary', ' a'] cosine: 0.9862926006317139\n",
      "12 [' Mary', ' the', ' a'] cosine: 0.9832582473754883\n",
      "13 [' the', ' Mary', ' a'] cosine: 0.9832898378372192\n",
      "14 [' the', ' Mary', ' a'] cosine: 0.9798016548156738\n",
      "15 [' the', ' Mary', ' a'] cosine: 0.976415753364563\n",
      "16 [' the', ' Mary', ' a'] cosine: 0.9742664098739624\n",
      "17 [' Mary', ' the', ' a'] cosine: 0.9703006744384766\n",
      "18 [' Mary', ' the', ' a'] cosine: 0.9689731001853943\n",
      "19 [' Mary', ' the', ' James'] cosine: 0.9611144065856934\n",
      "20 [' the', ' Mary', ' a'] cosine: 0.9638444185256958\n",
      "21 [' the', ' Mary', ' a'] cosine: 0.9587455987930298\n",
      "22 [' Mary', ' the', ' James'] cosine: 0.9482133388519287\n",
      "23 [' Mary', ' the', ' a'] cosine: 0.9506173133850098\n",
      "24 [' the', ' Mary', ' a'] cosine: 0.9490049481391907\n",
      "25 [' Mary', ' the', ' James'] cosine: 0.9414680004119873\n",
      "26 [' the', ' Mary', ' Alice'] cosine: 0.9187933206558228\n",
      "27 [' the', ' Mary', ' James'] cosine: 0.916545033454895\n",
      "28 [' Mary', ' the', ' James'] cosine: 0.9150677919387817\n",
      "29 [' the', ' Mary', ' a'] cosine: 0.9086806774139404\n",
      "30 [' the', ' Mary', ' James'] cosine: 0.9094659090042114\n",
      "31 [' the', ' Mary', ' James'] cosine: 0.8980247974395752\n",
      "32 [' the', ' Mary', ' his'] cosine: 0.8781951665878296\n",
      "33 [' the', '了', '的'] cosine: 0.8484296202659607\n",
      "34 [' the', ' Mary', ' a'] cosine: 0.36014989018440247\n",
      "35 [' the', ' Mary', ' a'] cosine: 0.3599635660648346\n",
      "36 [' the', ' a', ' '] cosine: 0.3597886562347412\n",
      "37 [' the', ' Mary', ' a'] cosine: 0.35990792512893677\n",
      "38 [' the', ' ', ' a'] cosine: 0.3600198030471802\n",
      "39 [' the', ' Mary', ' '] cosine: 0.36038658022880554\n",
      "40 [' the', ' Mary', ' '] cosine: 0.36064231395721436\n",
      "41 [' the', ' Mary', ' '] cosine: 0.3610658049583435\n",
      "42 [' the', ' Mary', ' '] cosine: 0.36121588945388794\n",
      "43 [' Mary', ' the', ' '] cosine: 0.36189502477645874\n",
      "44 [' of', ' ', ' and'] cosine: 0.3612082600593567\n",
      "45 [' Mary', ' the', ' '] cosine: 0.3631833791732788\n",
      "46 [' Mary', ' the', ' '] cosine: 0.3626118302345276\n",
      "47 [' ', ' Mary', ' the'] cosine: 0.36168330907821655\n",
      "48 [' Mary', ' the', ' '] cosine: 0.36177366971969604\n",
      "49 [' ', ' the', ' and'] cosine: 0.3612053394317627\n",
      "50 [' Mary', ' the', ' '] cosine: 0.3620871901512146\n",
      "51 [' ', ' the', ' and'] cosine: 0.36105483770370483\n",
      "52 [' Mary', ' the', ' '] cosine: 0.36209818720817566\n",
      "53 [' ', ' Mary', ' the'] cosine: 0.3614155948162079\n",
      "54 [' Mary', ' the', ' '] cosine: 0.36204513907432556\n",
      "55 [' ', ' the', ' Mary'] cosine: 0.3612435758113861\n",
      "56 [' Mary', ' the', ' '] cosine: 0.36246398091316223\n",
      "57 [' to', ' ', ' the'] cosine: 0.36178073287010193\n",
      "58 [' Mary', ' the', ' John'] cosine: 0.3608313798904419\n",
      "59 [' to', ' and', ' the'] cosine: 0.35888606309890747\n",
      "60 [' the', ' a', ' '] cosine: 0.35701245069503784\n",
      "61 [' the', ' a', ' '] cosine: 0.35544517636299133\n",
      "62 [' the', ' a', ' '] cosine: 0.35508543252944946\n",
      "63 [' the', ' a', ' '] cosine: 0.35493171215057373\n",
      "64 [' the', ' a', ' '] cosine: 0.354872465133667\n",
      "65 [' the', ' a', ' '] cosine: 0.35480523109436035\n",
      "66 [' the', ' a', ' '] cosine: 0.35471105575561523\n",
      "67 [' the', ' a', ' '] cosine: 0.35458242893218994\n",
      "68 [' the', ' a', ' Mary'] cosine: 0.35443753004074097\n",
      "69 [' the', ' Mary', ' a'] cosine: 0.3542822599411011\n",
      "70 [' the', ' Mary', ' a'] cosine: 0.3541354835033417\n",
      "71 [' the', ' a', ' Mary'] cosine: 0.3539749085903168\n",
      "72 [' Mary', ' the', ' a'] cosine: 0.3538475036621094\n",
      "73 [' and', ',', ' with'] cosine: 0.35334789752960205\n",
      "74 [' and', ' the', ' with'] cosine: 0.35347187519073486\n",
      "75 [' the', ' a', ' Mary'] cosine: 0.353316992521286\n",
      "76 [' Mary', ' the', ' a'] cosine: 0.353140652179718\n",
      "77 [' and', ',', ' with'] cosine: 0.35263723134994507\n",
      "78 [' and', ' the', ' with'] cosine: 0.3527950048446655\n",
      "79 [' the', ' a', ' '] cosine: 0.35271039605140686\n",
      "80 [' Mary', ' the', ' a'] cosine: 0.3524739742279053\n",
      "81 [' and', ' the', ' with'] cosine: 0.3521484136581421\n",
      "82 [' the', ' and', ' a'] cosine: 0.35225576162338257\n",
      "83 [' Mary', ' the', ' John'] cosine: 0.352084219455719\n",
      "84 [' the', ' John', ' a'] cosine: 0.3519043028354645\n",
      "85 [' Mary', ' the', ' John'] cosine: 0.3517179787158966\n",
      "86 [' and', ' the', ','] cosine: 0.35134178400039673\n",
      "87 [' the', ' and', ','] cosine: 0.35155951976776123\n",
      "88 [' John', ' the', ' a'] cosine: 0.3513953387737274\n"
     ]
    }
   ],
   "source": [
    "TARGET_LABEL_IDX = tokenizer.encode(\" John\")[0]\n",
    "LAST_TOKEN_IDX = toks[0,-1].item()\n",
    "mask = torch.zeros(model.model.embed_tokens.weight.shape[0], device=device, requires_grad=False).unsqueeze(1)\n",
    "mask[LAST_TOKEN_IDX,0] = 1\n",
    "\n",
    "for idx in range(100):\n",
    "  out = model(toks)\n",
    "  logits = out.logits\n",
    "  with torch.no_grad():\n",
    "    topk = logits[0,-1].topk(3)\n",
    "    out_toks = tokenizer.batch_decode(topk.indices)\n",
    "    cos = F.cosine_similarity(model.model.embed_tokens.weight[LAST_TOKEN_IDX], original_model.model.embed_tokens.weight[LAST_TOKEN_IDX], dim=0)\n",
    "    print(idx, out_toks, 'cosine:', cos.item())\n",
    "    if topk.indices[0].item() == TARGET_LABEL_IDX:\n",
    "      model.zero_grad()\n",
    "      break\n",
    "\n",
    "  loss = F.cross_entropy(logits[0,-1], torch.tensor(TARGET_LABEL_IDX, device=device))\n",
    "  loss.backward()\n",
    "\n",
    "  model.model.embed_tokens.weight.grad *= mask\n",
    "  model.model.embed_tokens.weight.data = model.model.embed_tokens.weight - 1e-3 * model.model.embed_tokens.weight.grad\n",
    "\n",
    "  model.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c15343-4d39-4395-a4de-3ed382ca96e1",
   "metadata": {},
   "source": [
    "### Make the prediction not correct by changing the embedding of the last token\n",
    "\n",
    "We would want to see if how hard it is to change the prediction from correct to incorrect. We will do so by update the embedding of the last token such that it increases the loss.\n",
    "\n",
    "With Qwen3-4B, it takes only 4 iteration to knock \" Mary\" from the top prediction. After some time, the model predict the input token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f573bb58-f63c-4772-ab53-cf7ac188be21",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.zero_grad()\n",
    "with torch.no_grad():\n",
    "  model.model.embed_tokens.weight.data = original_model.model.embed_tokens.weight.data.clone().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9bbeb42-29d0-4433-a6e2-a7ce74092b9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [' Mary', ' the', ' a'] cosine: 0.9999999403953552\n",
      "1 [' Mary', ' the', ' a'] cosine: 0.9999438524246216\n",
      "2 [' Mary', ' the', ' a'] cosine: 0.9997327327728271\n",
      "3 [' the', ' Mary', ' a'] cosine: 0.9992661476135254\n",
      "4 [' the', ' Mary', ' a'] cosine: 0.9983550310134888\n",
      "5 [' the', ' Mary', ' a'] cosine: 0.9966298341751099\n",
      "6 [' the', ' a', ' Mary'] cosine: 0.9933741688728333\n",
      "7 [' the', ' a', ' Mary'] cosine: 0.9873926639556885\n",
      "8 [' the', ' a', ' Alice'] cosine: 0.9774008989334106\n",
      "9 [' the', ' a', ' an'] cosine: 0.9657547473907471\n",
      "10 [' the', ' a', ' an'] cosine: 0.9581483602523804\n",
      "11 [' the', ' a', ' Mary'] cosine: 0.9406974911689758\n",
      "12 [' to', ' the', ' Mary'] cosine: 0.67897629737854\n",
      "13 [' to', ' the', ' and'] cosine: 0.6499879360198975\n",
      "14 [' to', ' the', ' and'] cosine: 0.6323633790016174\n",
      "15 [' to', ' and', ','] cosine: 0.6142152547836304\n",
      "16 [' to', ' and', ','] cosine: 0.6003431081771851\n",
      "17 [' to', ' and', ','] cosine: 0.5893852710723877\n",
      "18 [' to', ' and', ','] cosine: 0.5773136019706726\n",
      "19 [' to', ',', ' and'] cosine: 0.5647352933883667\n",
      "20 [' to', ',', ' and'] cosine: 0.5522352457046509\n",
      "21 [' to', ',', ' and'] cosine: 0.5405446887016296\n",
      "22 [' to', ',', ' '] cosine: 0.5286524295806885\n",
      "23 [' to', ',', ' '] cosine: 0.5171096324920654\n",
      "24 [' to', ' ', ','] cosine: 0.5060234069824219\n",
      "25 [' to', ',', ' a'] cosine: 0.4958186447620392\n",
      "26 [' to', ',', ' and'] cosine: 0.4828188717365265\n",
      "27 [' to', ' ', ' a'] cosine: 0.4769943058490753\n",
      "28 [' to', ',', ' '] cosine: 0.45899325609207153\n",
      "29 [' to', ' ', ' $'] cosine: 0.4535392224788666\n",
      "30 [' to', ',', ' '] cosine: 0.44492822885513306\n",
      "31 [' to', ' ', ','] cosine: 0.4370431900024414\n",
      "32 [' to', ' and', ','] cosine: 0.42824244499206543\n",
      "33 [' to', ' ', ','] cosine: 0.42093902826309204\n",
      "34 [' to', ' ', ' and'] cosine: 0.4119362235069275\n",
      "35 [' to', ' ', ','] cosine: 0.4044414758682251\n",
      "36 [' to', ' ', ' and'] cosine: 0.3979862928390503\n",
      "37 [' to', ' ', ','] cosine: 0.3910079896450043\n",
      "38 [' to', ' and', ','] cosine: 0.38432061672210693\n",
      "39 [' to', ' ', ' $'] cosine: 0.37459757924079895\n",
      "40 [' to', ',', ' and'] cosine: 0.3659147620201111\n",
      "41 [' to', ' ', ','] cosine: 0.3657016158103943\n",
      "42 [' to', ' ', ' and'] cosine: 0.3625621199607849\n",
      "43 [' to', ' and', '.'] cosine: 0.3552285432815552\n",
      "44 [' to', ' and', ','] cosine: 0.34827208518981934\n",
      "45 [' to', ' ', ','] cosine: 0.3457103371620178\n",
      "46 [' to', ' ', ' $'] cosine: 0.34242820739746094\n",
      "47 [' to', ' ', ','] cosine: 0.33885228633880615\n",
      "48 [' to', ' ', ' $'] cosine: 0.33659911155700684\n",
      "49 [' to', ' and', ','] cosine: 0.3342071771621704\n",
      "50 [' to', ' ', ','] cosine: 0.33047330379486084\n",
      "51 [' to', ',', '.'] cosine: 0.32596758008003235\n",
      "52 [' to', ' ', ' $'] cosine: 0.320128858089447\n",
      "53 [' to', ',', ' '] cosine: 0.31629878282546997\n",
      "54 [' to', ' and', ','] cosine: 0.3135431110858917\n",
      "55 [' to', ' ', ' and'] cosine: 0.31014835834503174\n",
      "56 [' to', ',', ' '] cosine: 0.30555790662765503\n",
      "57 [' to', ' ', ','] cosine: 0.3052757978439331\n",
      "58 [' to', ' ', ' and'] cosine: 0.3002409040927887\n",
      "59 [' to', ' ', ','] cosine: 0.2967095971107483\n",
      "60 [' to', ' and', ','] cosine: 0.294854998588562\n",
      "61 [' to', ',', '.'] cosine: 0.2903446555137634\n",
      "62 [' to', ' $', ' a'] cosine: 0.28502556681632996\n",
      "63 [' to', ',', ' and'] cosine: 0.2833026051521301\n",
      "64 [' to', ' $', ' '] cosine: 0.28237324953079224\n",
      "65 [' to', ' ', ','] cosine: 0.27310073375701904\n",
      "66 [' to', ' ', ','] cosine: 0.27432966232299805\n",
      "67 [' to', ',', ' '] cosine: 0.2713320851325989\n",
      "68 [' to', ',', ' and'] cosine: 0.2673553228378296\n",
      "69 [' to', ' ', ','] cosine: 0.2662753760814667\n",
      "70 [' to', ' ', ','] cosine: 0.2658020257949829\n",
      "71 [' to', ',', ' '] cosine: 0.2650248110294342\n",
      "72 [' to', ',', ' '] cosine: 0.26383233070373535\n",
      "73 [' to', ',', ' and'] cosine: 0.26217323541641235\n",
      "74 [' to', ' ', ','] cosine: 0.26058468222618103\n",
      "75 [' to', ',', '.'] cosine: 0.25752633810043335\n",
      "76 [' to', ' ', ' and'] cosine: 0.2569664716720581\n",
      "77 [' to', ' ', ' $'] cosine: 0.25411316752433777\n",
      "78 [' to', ' the', ','] cosine: 0.25134992599487305\n",
      "79 [' to', ' ', ' $'] cosine: 0.25330686569213867\n",
      "80 [' to', ',', ' '] cosine: 0.2501831650733948\n",
      "81 [' to', ' ', '.'] cosine: 0.24511760473251343\n",
      "82 [' to', '.', ','] cosine: 0.24372956156730652\n",
      "83 [' to', ' ', ','] cosine: 0.24498727917671204\n",
      "84 [' to', ',', ' '] cosine: 0.23771363496780396\n",
      "85 [' to', ',', ' '] cosine: 0.23839981853961945\n",
      "86 [' to', ',', '.'] cosine: 0.237559974193573\n",
      "87 [' to', ',', '.'] cosine: 0.23609544336795807\n",
      "88 [' to', ',', ' '] cosine: 0.23537345230579376\n",
      "89 [' to', ',', ' '] cosine: 0.2338179647922516\n",
      "90 [' to', ',', ' and'] cosine: 0.23230399191379547\n",
      "91 [' to', ' ', ','] cosine: 0.23043769598007202\n",
      "92 [' to', ',', '.'] cosine: 0.22797256708145142\n",
      "93 [' to', ',', ' '] cosine: 0.22874674201011658\n",
      "94 [' to', ',', ' '] cosine: 0.22902119159698486\n",
      "95 [' to', ',', ' '] cosine: 0.2266249805688858\n",
      "96 [' to', ' the', ' $'] cosine: 0.2248561680316925\n",
      "97 [' to', ' ', ','] cosine: 0.2215588241815567\n",
      "98 [' to', ',', ' and'] cosine: 0.21919101476669312\n",
      "99 [' to', ' ', ' $'] cosine: 0.21894976496696472\n"
     ]
    }
   ],
   "source": [
    "CORRECT_LABEL_IDX = tokenizer.encode(\" Mary\")[0]\n",
    "LAST_TOKEN_IDX = toks[0,-1].item()\n",
    "mask = torch.zeros(model.model.embed_tokens.weight.shape[0], device=device, requires_grad=False).unsqueeze(1)\n",
    "mask[LAST_TOKEN_IDX,0] = 1\n",
    "\n",
    "for idx in range(100):\n",
    "  out = model(toks)\n",
    "  logits = out.logits\n",
    "  with torch.no_grad():\n",
    "    topk = logits[0,-1].topk(3)\n",
    "    out_toks = tokenizer.batch_decode(topk.indices)\n",
    "    cos = F.cosine_similarity(model.model.embed_tokens.weight[LAST_TOKEN_IDX], original_model.model.embed_tokens.weight[LAST_TOKEN_IDX], dim=0)\n",
    "    print(idx, out_toks, 'cosine:', cos.item())\n",
    "    # if topk.indices[0].item() != CORRECT_LABEL_IDX:\n",
    "    #   model.zero_grad()\n",
    "    #   break\n",
    "\n",
    "  loss = F.cross_entropy(logits[0,-1], torch.tensor(CORRECT_LABEL_IDX, device=device))\n",
    "  loss.backward()\n",
    "\n",
    "  model.model.embed_tokens.weight.grad *= mask\n",
    "  model.model.embed_tokens.weight.data = model.model.embed_tokens.weight + 1e-3 * model.model.embed_tokens.weight.grad # here we increase the loss)\n",
    "\n",
    "  model.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed26ddcc-0c23-477f-a9a2-8e439074295c",
   "metadata": {},
   "source": [
    "### Randomly jittering the embedding of the last token.\n",
    "\n",
    "The prediction is very robust against random jittering. Randomly jittering within 100% of value doesn't change much the cosine similarity. It seems the before and after embeddings still point to relatively similar direction. Increasing the magnitude of random jittering does decreases before-after cosine similarity, and has some effect on the prediction, but not by much.\n",
    "\n",
    "This is pretty interesting. Maybe during training, the self-attention constantly \"induces noises\", and this makes the model much more robust against random jittering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0cfe0552-f2ec-4d6e-8ec3-1083506a53b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [' Mary', ' the', ' a'] cosine: 0.9999999403953552\n",
      "1 [' Mary', ' the', ' a'] cosine: 0.9999831318855286\n",
      "2 [' Mary', ' the', ' a'] cosine: 0.9999349117279053\n",
      "3 [' Mary', ' the', ' a'] cosine: 0.9998526573181152\n",
      "4 [' Mary', ' the', ' a'] cosine: 0.9997234344482422\n",
      "5 [' Mary', ' the', ' a'] cosine: 0.9995722770690918\n",
      "6 [' Mary', ' the', ' a'] cosine: 0.9994065761566162\n",
      "7 [' Mary', ' the', ' a'] cosine: 0.9992039203643799\n",
      "8 [' Mary', ' the', ' a'] cosine: 0.9988850355148315\n",
      "9 [' Mary', ' the', ' a'] cosine: 0.9987077713012695\n",
      "10 [' Mary', ' the', ' a'] cosine: 0.9983139634132385\n",
      "11 [' Mary', ' the', ' a'] cosine: 0.9979837536811829\n",
      "12 [' Mary', ' the', ' a'] cosine: 0.9976602792739868\n",
      "13 [' Mary', ' the', ' a'] cosine: 0.9971832633018494\n",
      "14 [' Mary', ' the', ' a'] cosine: 0.9967246651649475\n",
      "15 [' Mary', ' the', ' a'] cosine: 0.9962208271026611\n",
      "16 [' Mary', ' the', ' a'] cosine: 0.995842456817627\n",
      "17 [' Mary', ' the', ' a'] cosine: 0.9951059222221375\n",
      "18 [' Mary', ' the', ' a'] cosine: 0.9945223331451416\n",
      "19 [' Mary', ' the', ' a'] cosine: 0.9940631985664368\n",
      "20 [' Mary', ' the', ' a'] cosine: 0.9935961961746216\n",
      "21 [' Mary', ' the', ' a'] cosine: 0.9926183819770813\n",
      "22 [' Mary', ' the', ' a'] cosine: 0.9916558265686035\n",
      "23 [' Mary', ' the', ' a'] cosine: 0.9915971159934998\n",
      "24 [' Mary', ' the', ' a'] cosine: 0.9904476404190063\n",
      "25 [' Mary', ' the', ' a'] cosine: 0.9901000261306763\n",
      "26 [' Mary', ' the', ' a'] cosine: 0.9887847900390625\n",
      "27 [' Mary', ' the', ' a'] cosine: 0.9878615140914917\n",
      "28 [' Mary', ' the', ' a'] cosine: 0.9875451326370239\n",
      "29 [' Mary', ' the', ' a'] cosine: 0.9863771200180054\n",
      "30 [' Mary', ' the', ' a'] cosine: 0.9855272769927979\n",
      "31 [' Mary', ' the', ' a'] cosine: 0.984440267086029\n",
      "32 [' Mary', ' the', ' a'] cosine: 0.9832935333251953\n",
      "33 [' Mary', ' the', ' a'] cosine: 0.9818456768989563\n",
      "34 [' Mary', ' the', ' a'] cosine: 0.9820523262023926\n",
      "35 [' Mary', ' the', ' a'] cosine: 0.9798448085784912\n",
      "36 [' Mary', ' the', ' a'] cosine: 0.9786263704299927\n",
      "37 [' Mary', ' the', ' a'] cosine: 0.9779444932937622\n",
      "38 [' Mary', ' the', ' a'] cosine: 0.9758298397064209\n",
      "39 [' Mary', ' the', ' a'] cosine: 0.974837601184845\n",
      "40 [' Mary', ' the', ' a'] cosine: 0.9740976691246033\n",
      "41 [' Mary', ' the', ' a'] cosine: 0.9739057421684265\n",
      "42 [' Mary', ' the', ' a'] cosine: 0.9714151620864868\n",
      "43 [' Mary', ' the', ' a'] cosine: 0.969628095626831\n",
      "44 [' Mary', ' the', ' a'] cosine: 0.9692096710205078\n",
      "45 [' Mary', ' the', ' a'] cosine: 0.9679369330406189\n",
      "46 [' Mary', ' the', ' a'] cosine: 0.9668226838111877\n",
      "47 [' Mary', ' the', ' a'] cosine: 0.9653088450431824\n",
      "48 [' Mary', ' the', ' a'] cosine: 0.9628252983093262\n",
      "49 [' Mary', ' the', ' a'] cosine: 0.9637744426727295\n",
      "50 [' Mary', ' the', ' a'] cosine: 0.9608001708984375\n",
      "51 [' Mary', ' the', ' a'] cosine: 0.9626790881156921\n",
      "52 [' Mary', ' the', ' a'] cosine: 0.9597929120063782\n",
      "53 [' Mary', ' the', ' a'] cosine: 0.9574875831604004\n",
      "54 [' Mary', ' the', ' a'] cosine: 0.9566929936408997\n",
      "55 [' Mary', ' the', ' a'] cosine: 0.9545285701751709\n",
      "56 [' Mary', ' the', ' a'] cosine: 0.9522667527198792\n",
      "57 [' Mary', ' the', ' a'] cosine: 0.9484013319015503\n",
      "58 [' Mary', ' the', ' a'] cosine: 0.9485383629798889\n",
      "59 [' Mary', ' the', ' a'] cosine: 0.9463399052619934\n",
      "60 [' Mary', ' the', ' a'] cosine: 0.9464560747146606\n",
      "61 [' Mary', ' the', ' a'] cosine: 0.9426110982894897\n",
      "62 [' Mary', ' the', ' a'] cosine: 0.9389479160308838\n",
      "63 [' Mary', ' the', ' a'] cosine: 0.9405699372291565\n",
      "64 [' Mary', ' the', ' a'] cosine: 0.9390592575073242\n",
      "65 [' Mary', ' the', ' a'] cosine: 0.9389755725860596\n",
      "66 [' Mary', ' the', ' a'] cosine: 0.9324511289596558\n",
      "67 [' Mary', ' the', ' a'] cosine: 0.9313551783561707\n",
      "68 [' Mary', ' the', ' a'] cosine: 0.9278631210327148\n",
      "69 [' Mary', ' the', ' a'] cosine: 0.9278048276901245\n",
      "70 [' Mary', ' the', ' a'] cosine: 0.9264423251152039\n",
      "71 [' Mary', ' the', ' a'] cosine: 0.9215337038040161\n",
      "72 [' Mary', ' the', ' a'] cosine: 0.9270069003105164\n",
      "73 [' Mary', ' the', ' a'] cosine: 0.9228658080101013\n",
      "74 [' Mary', ' the', ' a'] cosine: 0.9189380407333374\n",
      "75 [' Mary', ' the', ' a'] cosine: 0.9136472940444946\n",
      "76 [' Mary', ' the', ' a'] cosine: 0.9121155738830566\n",
      "77 [' Mary', ' the', ' a'] cosine: 0.9102820754051208\n",
      "78 [' Mary', ' the', ' a'] cosine: 0.9102659225463867\n",
      "79 [' Mary', ' the', ' a'] cosine: 0.911301851272583\n",
      "80 [' Mary', ' the', ' a'] cosine: 0.903795063495636\n",
      "81 [' Mary', ' the', ' a'] cosine: 0.9044429063796997\n",
      "82 [' Mary', ' the', ' a'] cosine: 0.9053274393081665\n",
      "83 [' Mary', ' the', ' a'] cosine: 0.9064398407936096\n",
      "84 [' Mary', ' the', ' a'] cosine: 0.9041027426719666\n",
      "85 [' Mary', ' the', ' a'] cosine: 0.8920875787734985\n",
      "86 [' Mary', ' the', ' a'] cosine: 0.8999236822128296\n",
      "87 [' Mary', ' the', ' a'] cosine: 0.8938231468200684\n",
      "88 [' Mary', ' the', ' a'] cosine: 0.894995927810669\n",
      "89 [' Mary', ' the', ' his'] cosine: 0.8958907127380371\n",
      "90 [' Mary', ' the', ' a'] cosine: 0.8872644901275635\n",
      "91 [' Mary', ' the', ' a'] cosine: 0.8813782930374146\n",
      "92 [' Mary', ' the', ' a'] cosine: 0.8851220011711121\n",
      "93 [' Mary', ' the', ' a'] cosine: 0.8781830072402954\n",
      "94 [' Mary', ' the', ' a'] cosine: 0.8795216083526611\n",
      "95 [' Mary', ' the', ' a'] cosine: 0.8781030178070068\n",
      "96 [' Mary', ' the', ' a'] cosine: 0.8678917288780212\n",
      "97 [' Mary', ' the', ' a'] cosine: 0.8755459189414978\n",
      "98 [' Mary', ' the', ' a'] cosine: 0.8657993078231812\n",
      "99 [' Mary', ' the', ' a'] cosine: 0.8693617582321167\n",
      "100 [' Mary', ' the', ' a'] cosine: 0.8670876026153564\n",
      "101 [' Mary', ' the', ' a'] cosine: 0.8677204847335815\n",
      "102 [' Mary', ' the', ' a'] cosine: 0.859539270401001\n",
      "103 [' Mary', ' the', ' a'] cosine: 0.8670768141746521\n",
      "104 [' Mary', ' the', ' a'] cosine: 0.850098729133606\n",
      "105 [' Mary', ' the', ' a'] cosine: 0.866845965385437\n",
      "106 [' Mary', ' the', ' a'] cosine: 0.851051926612854\n",
      "107 [' Mary', ' the', ' a'] cosine: 0.8523215055465698\n",
      "108 [' Mary', ' the', ' a'] cosine: 0.8598930835723877\n",
      "109 [' Mary', ' the', ' a'] cosine: 0.841498076915741\n",
      "110 [' Mary', ' the', ' a'] cosine: 0.8482215404510498\n",
      "111 [' Mary', ' the', ' a'] cosine: 0.8409683108329773\n",
      "112 [' Mary', ' the', ' a'] cosine: 0.8296153545379639\n",
      "113 [' Mary', ' the', ' a'] cosine: 0.838062047958374\n",
      "114 [' Mary', ' the', ' a'] cosine: 0.8391447067260742\n",
      "115 [' Mary', ' the', ' a'] cosine: 0.8322128057479858\n",
      "116 [' Mary', ' the', ' a'] cosine: 0.8299782276153564\n",
      "117 [' Mary', ' the', ' a'] cosine: 0.8183771371841431\n",
      "118 [' Mary', ' the', ' a'] cosine: 0.8165737390518188\n",
      "119 [' Mary', ' the', ' a'] cosine: 0.8148374557495117\n",
      "120 [' Mary', ' the', ' a'] cosine: 0.8102564811706543\n",
      "121 [' Mary', ' the', ' a'] cosine: 0.813306450843811\n",
      "122 [' Mary', ' the', ' a'] cosine: 0.8114267587661743\n",
      "123 [' Mary', ' the', ' a'] cosine: 0.8157765865325928\n",
      "124 [' Mary', ' the', ' a'] cosine: 0.8138611316680908\n",
      "125 [' Mary', ' the', ' a'] cosine: 0.8166518211364746\n",
      "126 [' the', ' Mary', ' a'] cosine: 0.8092398643493652\n",
      "127 [' Mary', ' the', ' a'] cosine: 0.7987091541290283\n",
      "128 [' Mary', ' the', ' a'] cosine: 0.7996963262557983\n",
      "129 [' the', ' Mary', ' a'] cosine: 0.7937486171722412\n",
      "130 [' the', ' Mary', ' a'] cosine: 0.7912218570709229\n",
      "131 [' Mary', ' the', ' a'] cosine: 0.7992618680000305\n",
      "132 [' Mary', ' the', ' a'] cosine: 0.8068407773971558\n",
      "133 [' Mary', ' the', ' a'] cosine: 0.8006777763366699\n",
      "134 [' Mary', ' the', ' a'] cosine: 0.7935186624526978\n",
      "135 [' Mary', ' the', ' his'] cosine: 0.7919256687164307\n",
      "136 [' Mary', ' the', ' a'] cosine: 0.7783279418945312\n",
      "137 [' Mary', ' the', ' a'] cosine: 0.7822023630142212\n",
      "138 [' Mary', ' the', ' a'] cosine: 0.7812666296958923\n",
      "139 [' the', ' Mary', ' a'] cosine: 0.7708359956741333\n",
      "140 [' Mary', ' the', ' a'] cosine: 0.7701088190078735\n",
      "141 [' Mary', ' the', ' his'] cosine: 0.7649240493774414\n",
      "142 [' Mary', ' the', ' a'] cosine: 0.7752679586410522\n",
      "143 [' Mary', ' the', ' a'] cosine: 0.7733708620071411\n",
      "144 [' Mary', ' the', ' a'] cosine: 0.7663105726242065\n",
      "145 [' Mary', ' the', ' a'] cosine: 0.7702435255050659\n",
      "146 [' Mary', ' the', ' a'] cosine: 0.7740802764892578\n",
      "147 [' Mary', ' the', ' a'] cosine: 0.7656925916671753\n",
      "148 [' Mary', ' the', ' a'] cosine: 0.7512603402137756\n",
      "149 [' Mary', ' the', ' a'] cosine: 0.7578229904174805\n",
      "150 [' Mary', ' the', ' a'] cosine: 0.7593878507614136\n",
      "151 [' Mary', ' the', ' a'] cosine: 0.7521545886993408\n",
      "152 [' Mary', ' the', ' a'] cosine: 0.7745014429092407\n",
      "153 [' Mary', ' the', ' a'] cosine: 0.7506043910980225\n",
      "154 [' Mary', ' the', ' a'] cosine: 0.7384018898010254\n",
      "155 [' Mary', ' the', ' a'] cosine: 0.7564452290534973\n",
      "156 [' the', ' Mary', ' a'] cosine: 0.7446961998939514\n",
      "157 [' Mary', ' the', ' a'] cosine: 0.7350420355796814\n",
      "158 [' Mary', ' the', ' a'] cosine: 0.7467126846313477\n",
      "159 [' Mary', ' the', ' a'] cosine: 0.7174451351165771\n",
      "160 [' Mary', ' the', ' a'] cosine: 0.7217997312545776\n",
      "161 [' Mary', ' the', ' a'] cosine: 0.7171080708503723\n",
      "162 [' Mary', ' the', ' a'] cosine: 0.7240540981292725\n",
      "163 [' Mary', ' the', ' a'] cosine: 0.722807765007019\n",
      "164 [' Mary', ' the', ' a'] cosine: 0.7501451969146729\n",
      "165 [' Mary', ' the', ' a'] cosine: 0.7344363927841187\n",
      "166 [' Mary', ' the', ' his'] cosine: 0.7016700506210327\n",
      "167 [' Mary', ' the', ' a'] cosine: 0.7244043350219727\n",
      "168 [' Mary', ' the', ' a'] cosine: 0.6896693110466003\n",
      "169 [' Mary', ' the', ' a'] cosine: 0.7382224798202515\n",
      "170 [' Mary', ' the', ' a'] cosine: 0.713697612285614\n",
      "171 [' Mary', ' the', ' a'] cosine: 0.7126091122627258\n",
      "172 [' Mary', ' the', ' a'] cosine: 0.6945210695266724\n",
      "173 [' Mary', ' the', ' a'] cosine: 0.7075056433677673\n",
      "174 [' Mary', ' the', ' his'] cosine: 0.7000576257705688\n",
      "175 [' Mary', ' the', ' a'] cosine: 0.7034608721733093\n",
      "176 [' Mary', ' the', ' a'] cosine: 0.690157413482666\n",
      "177 [' Mary', ' the', ' his'] cosine: 0.7048332095146179\n",
      "178 [' Mary', ' the', ' a'] cosine: 0.7137095928192139\n",
      "179 [' the', ' Mary', ' a'] cosine: 0.6844468116760254\n",
      "180 [' Mary', ' the', ' a'] cosine: 0.6929628849029541\n",
      "181 [' Mary', ' the', ' a'] cosine: 0.6886595487594604\n",
      "182 [' the', ' Mary', ' a'] cosine: 0.671296238899231\n",
      "183 [' Mary', ' the', ' a'] cosine: 0.6834302544593811\n",
      "184 [' Mary', ' the', ' a'] cosine: 0.6951311826705933\n",
      "185 [' Mary', ' the', ' a'] cosine: 0.670181393623352\n",
      "186 [' the', ' Mary', ' his'] cosine: 0.6638643741607666\n",
      "187 [' Mary', ' the', ' a'] cosine: 0.6943552494049072\n",
      "188 [' the', ' Mary', ' a'] cosine: 0.6747397184371948\n",
      "189 [' the', ' Mary', ' a'] cosine: 0.6697224974632263\n",
      "190 [' the', ' Mary', ' a'] cosine: 0.6850194334983826\n",
      "191 [' the', ' Mary', ' to'] cosine: 0.6676977872848511\n",
      "192 [' Mary', ' the', ' a'] cosine: 0.6545487642288208\n",
      "193 [' Mary', ' the', ' a'] cosine: 0.6507009267807007\n",
      "194 [' the', ' Mary', ' a'] cosine: 0.6684027314186096\n",
      "195 [' Mary', ' the', ' his'] cosine: 0.6727471351623535\n",
      "196 [' the', ' Mary', ' a'] cosine: 0.6680089235305786\n",
      "197 [' Mary', ' the', ' his'] cosine: 0.6488011479377747\n",
      "198 [' Mary', ' the', ' a'] cosine: 0.6580020189285278\n",
      "199 [' Mary', ' the', ' a'] cosine: 0.6425575017929077\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad\n",
    "def randomize_embedding(target_emb, src_emb, token_idx, pct: float):\n",
    "  token_emb = src_emb.weight.data[token_idx].clone()\n",
    "  jittering = torch.rand(token_emb.shape) * pct * 2 + 1 - pct\n",
    "  jittering = jittering.to(token_emb.device)\n",
    "  target_emb.weight.data[token_idx] = token_emb * jittering\n",
    "\n",
    "LAST_TOKEN_IDX = toks[0,-1].item()\n",
    "\n",
    "with torch.no_grad():\n",
    "  for idx in range(200):\n",
    "    randomize_embedding(model.model.embed_tokens, original_model.model.embed_tokens, LAST_TOKEN_IDX, idx / 100)\n",
    "    out = model(toks)\n",
    "    logits = out.logits\n",
    "\n",
    "    topk = logits[0,-1].topk(3)\n",
    "    out_toks = tokenizer.batch_decode(topk.indices)\n",
    "    cos = F.cosine_similarity(model.model.embed_tokens.weight[LAST_TOKEN_IDX], original_model.model.embed_tokens.weight[LAST_TOKEN_IDX], dim=0)\n",
    "    print(idx, out_toks, 'cosine:', cos.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "880aecab-5091-45b8-9195-93da45de7555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before tensor([-0.0181, -0.0189,  0.0119,  ...,  0.0098, -0.0247,  0.0221],\n",
      "       device='mps:0', grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"Before\", original_model.model.embed_tokens.weight[LAST_TOKEN_IDX])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "07ee8e57-543c-4914-bc17-efc480db3ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After tensor([-0.0014, -0.0250, -0.0112,  ...,  0.0125, -0.0251,  0.0121],\n",
      "       device='mps:0', grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"After\", model.model.embed_tokens.weight[LAST_TOKEN_IDX])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1f5a1a9b-0834-4baf-a6a9-8b8b494b5bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm before 1.99 jittering tensor(0.8725, device='mps:0')\n",
      "Norm after 1.99 jittering tensor(1.3438, device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "  print(\"Norm before\", idx / 100, \"jittering\", original_model.model.embed_tokens.weight[LAST_TOKEN_IDX].norm(p=2))\n",
    "  print(\"Norm after\", idx / 100, \"jittering\", model.model.embed_tokens.weight[LAST_TOKEN_IDX].norm(p=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f937cc8-d9ac-4ecc-8133-218cd172db0d",
   "metadata": {},
   "source": [
    "## Intervene embedding of all tokens\n",
    "\n",
    "The previous experiments attempt to make minimal change to the last token to see how hard it's to flip the correct answer. The experiments below do the same but allow make changes to embeddings of all tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5c8da4b6-196a-4aee-888b-30134bb16168",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def summarize_grad_accumulation(grad_accumulation, model, original_model):\n",
    "  grad_changes = (grad_accumulation ** 2).sum(dim=1)\n",
    "  grad_changes_mask = (grad_changes > 0.01)\n",
    "  changed_tokens = grad_changes_mask.nonzero().squeeze()\n",
    "\n",
    "  print(changed_tokens.shape[0], \"tokens have embedding changed. They are:\")\n",
    "  _changed = []\n",
    "  for t_ in changed_tokens.cpu().tolist():\n",
    "    cos_ = F.cosine_similarity(original_model.model.embed_tokens.weight[t_], model.model.embed_tokens.weight[t_], dim=0).item()\n",
    "    _changed.append((t_, tokenizer.decode(t_), grad_accumulation[t_,0].norm(p=1).item(), cos_))\n",
    "\n",
    "  print(\"(idx, token str, grad_accumulation p1, cos before-after)\")\n",
    "  print()\n",
    "  for t_, s_, v_, c_ in sorted(_changed, key=lambda obj: obj[3]):\n",
    "    print(t_, s_, v_, c_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1471165-6789-458c-97dd-1b54e0425eaa",
   "metadata": {},
   "source": [
    "### Flip the prediction to a target token\n",
    "\n",
    "It's much easier to flip the token if we change all of the embeddings because it contains the token that we want to flip into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3c1bb1bc-a723-4505-a42d-42dbc2aa234f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.zero_grad()\n",
    "with torch.no_grad():\n",
    "  model.model.embed_tokens.weight.data = original_model.model.embed_tokens.weight.data.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c390fcaf-48e4-48ba-853d-3358d1a0767f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [' Mary', ' the', ' a']\n",
      "1 [' John', 'Mary', ' his']\n"
     ]
    }
   ],
   "source": [
    "TARGET_LABEL_IDX = tokenizer.encode(\" John\")[0]\n",
    "grad_accumulation = torch.zeros(model.model.embed_tokens.weight.data.shape, requires_grad=False).to(device)\n",
    "\n",
    "for idx in range(100):\n",
    "  out = model(toks)\n",
    "  logits = out.logits\n",
    "  with torch.no_grad():\n",
    "    topk = logits[0,-1].topk(3)\n",
    "    out_toks = tokenizer.batch_decode(topk.indices)\n",
    "    print(idx, out_toks)\n",
    "    if topk.indices[0].item() == TARGET_LABEL_IDX:\n",
    "      model.zero_grad()\n",
    "      break\n",
    "\n",
    "  loss = F.cross_entropy(logits[0,-1], torch.tensor(TARGET_LABEL_IDX, device=device))\n",
    "  loss.backward()\n",
    "\n",
    "  with torch.no_grad():\n",
    "    grad_accumulation += model.model.embed_tokens.weight.grad\n",
    "  model.model.embed_tokens.weight.data = model.model.embed_tokens.weight - 1e-3 * model.model.embed_tokens.weight.grad\n",
    "  model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cd8c8fa8-6489-4c85-970f-2b63a343d2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_changes = (grad_accumulation ** 2).sum(dim=1)\n",
    "grad_changes_mask = (grad_changes > 0.01)\n",
    "changed_tokens = grad_changes_mask.nonzero().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9fd1deaf-a973-459c-b276-ca8746999817",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 tokens have embedding changed. They are:\n",
      "idx, token str, grad_accumulation p1, cos before-after\n",
      "\n",
      "3757  John 1.466503381729126 0.9879211783409119\n",
      "10244  Mary 0.27848052978515625 0.9963998794555664\n",
      "279  the 1.0227277278900146 0.9983216524124146\n",
      "4498 When 1.4908576011657715 0.9995152950286865\n",
      "8061  shop 0.0740446001291275 0.9997081160545349\n",
      "3937  went 0.3508458435535431 0.9998031854629517\n",
      "6551  gave 1.1857423782348633 0.9998382329940796\n",
      "323  and 0.7384849190711975 0.9998487234115601\n",
      "311  to 0.00997190922498703 0.999849259853363\n",
      "8968  bag 0.4924362301826477 0.9999417662620544\n",
      "11 , 0.23656544089317322 0.9999722838401794\n",
      "264  a 0.005127974320203066 0.9999829530715942\n",
      "806  his 0.0041706436313688755 0.9999920725822449\n",
      "29405  Alice 0.0010434952564537525 0.9999995827674866\n",
      "8224  Sam 0.00013742889859713614 0.9999998807907104\n",
      "20445  Sarah 0.0005189783405512571 0.9999998807907104\n",
      "7801  James 0.0002439873933326453 0.9999999403953552\n",
      "23016  Maria 0.00013226382725406438 0.9999999403953552\n",
      "458  an 0.00014632524107582867 1.0\n",
      "4325  someone 0.00013345701154321432 1.0\n",
      "21475  Jane 0.00020099429821129888 1.0\n",
      "23223  Anna 0.0002476042718626559 1.0\n",
      "29933  Susan 0.00012020429130643606 1.0\n",
      "34166  Emily 0.00016106815019156784 1.0\n",
      "52291  Sally 0.00020744497305713594 1.0\n"
     ]
    }
   ],
   "source": [
    "print(changed_tokens.shape[0], \"tokens have embedding changed. They are:\")\n",
    "_changed = []\n",
    "for t_ in changed_tokens.cpu().tolist():\n",
    "  cos_ = F.cosine_similarity(original_model.model.embed_tokens.weight[t_], model.model.embed_tokens.weight[t_], dim=0).item()\n",
    "  _changed.append((t_, tokenizer.decode(t_), grad_accumulation[t_,0].norm(p=1).item(), cos_))\n",
    "\n",
    "print(\"idx, token str, grad_accumulation p1, cos before-after\")\n",
    "print()\n",
    "for t_, s_, v_, c_ in sorted(_changed, key=lambda obj: obj[3]):\n",
    "  print(t_, s_, v_, c_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513d2a98-49f1-4ecf-b2e8-5e63ee55d352",
   "metadata": {},
   "source": [
    "It seems the token \" John\" get massive update. What if we suppress changing the token \" John\"? The \" to\" token is not updated as much as we expect (we expect because this token lays the ground for prediction \" Mary\"). The \" gave\" token is updated quite a lot, which is reasonable now when I think of it.\n",
    "\n",
    "One interesting thing is that some tokens that do not appear in this sentence get gradient?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1f055637-30d1-449b-a248-a4d2120a72b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John-Mary cos Before tensor(0.2099, device='mps:0')\n",
      "John-Mary cos After tensor(0.2098, device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "t1 = tokenizer.encode(\" John\")[0]\n",
    "t2 = tokenizer.encode(\" Mary\")[0]\n",
    "\n",
    "with torch.no_grad():\n",
    "  # before\n",
    "  print(\"John-Mary cos Before\", F.cosine_similarity(original_model.model.embed_tokens.weight[t1], original_model.model.embed_tokens.weight[t2], dim=0))\n",
    "  print(\"John-Mary cos After\", F.cosine_similarity(model.model.embed_tokens.weight[t1], model.model.embed_tokens.weight[t2], dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71655446-0c50-4ae9-9640-35f499ef3e33",
   "metadata": {},
   "source": [
    "#### Suppressing update embedding of the target token\n",
    "\n",
    "Let's check how hard it is if the embedding of the target token \" John\" not modified. It takes more effort, but not that much. But the end result is that it requires the embedding of much more tokens to change to get this effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4bdcec44-cbf7-45e8-8722-8aa687395fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.zero_grad()\n",
    "with torch.no_grad():\n",
    "  model.model.embed_tokens.weight.data = original_model.model.embed_tokens.weight.data.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5534689a-aa0f-4a60-8e43-6970c66ca302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [' Mary', ' the', ' a']\n",
      "1 [' a', ' his', ' Alice']\n",
      "2 ['Mary', ' his', ' a']\n",
      "3 [' mary', 'Mary', ' his']\n",
      "4 [' James', 'Mary', ' mary']\n",
      "5 [' mary', 'Mary', ' John']\n",
      "6 [' James', ' John', ' mary']\n",
      "7 [' John', ' James', ' Mark']\n"
     ]
    }
   ],
   "source": [
    "TARGET_LABEL_IDX = tokenizer.encode(\" John\")[0]\n",
    "mask = torch.ones(model.model.embed_tokens.weight.shape[0], device=device, requires_grad=False).unsqueeze(1)\n",
    "mask[TARGET_LABEL_IDX,0] = 0\n",
    "\n",
    "grad_accumulation = torch.zeros(model.model.embed_tokens.weight.data.shape, requires_grad=False).to(device)\n",
    "\n",
    "for idx in range(100):\n",
    "  out = model(toks)\n",
    "  logits = out.logits\n",
    "  with torch.no_grad():\n",
    "    topk = logits[0,-1].topk(3)\n",
    "    out_toks = tokenizer.batch_decode(topk.indices)\n",
    "    print(idx, out_toks)\n",
    "    if topk.indices[0].item() == TARGET_LABEL_IDX:\n",
    "      model.zero_grad()\n",
    "      break\n",
    "\n",
    "  loss = F.cross_entropy(logits[0,-1], torch.tensor(TARGET_LABEL_IDX, device=device))\n",
    "  loss.backward()\n",
    "\n",
    "  model.model.embed_tokens.weight.grad *= mask\n",
    "\n",
    "  with torch.no_grad():\n",
    "    grad_accumulation += model.model.embed_tokens.weight.grad\n",
    "  \n",
    "  model.model.embed_tokens.weight.data = model.model.embed_tokens.weight - 1e-3 * model.model.embed_tokens.weight.grad\n",
    "  model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9a269cae-4794-4593-a0cb-a9d023211303",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "526 tokens have embedding changed. They are:\n",
      "idx, token str, grad_accumulation p1, cos before-after\n",
      "\n",
      "10244  Mary 2.1146278381347656 0.9933505654335022\n",
      "279  the 1.5858529806137085 0.9963732957839966\n",
      "323  and 0.7634978890419006 0.9983670711517334\n",
      "4498 When 3.5042643547058105 0.9983714818954468\n",
      "8061  shop 0.4943886399269104 0.9991201162338257\n",
      "3937  went 0.7949550151824951 0.9994505643844604\n",
      "6551  gave 0.6530000567436218 0.999481737613678\n",
      "311  to 0.15373677015304565 0.999482274055481\n",
      "264  a 0.033509328961372375 0.9995958805084229\n",
      "806  his 0.034367725253105164 0.9997066259384155\n",
      "8968  bag 0.3553487956523895 0.9997640252113342\n",
      "41484 Mary 0.029053980484604836 0.9997926950454712\n",
      "7801  James 0.02342626452445984 0.9998242855072021\n",
      "29405  Alice 0.02315627969801426 0.9998542666435242\n",
      "11 , 0.18387183547019958 0.9999076128005981\n",
      "84630  mary 0.021174168214201927 0.999914288520813\n",
      "20445  Sarah 0.01612580008804798 0.9999176263809204\n",
      "21475  Jane 0.015967853367328644 0.9999343156814575\n",
      "23016  Maria 0.015355516225099564 0.9999357461929321\n",
      "52291  Sally 0.01152717974036932 0.9999661445617676\n",
      "220   0.012715836055576801 0.9999663829803467\n",
      "8364  Tom 0.009947291575372219 0.9999740123748779\n",
      "11268  Mike 0.009141712449491024 0.9999741315841675\n",
      "29933  Susan 0.009334642440080643 0.9999743700027466\n",
      "4389  Mark 0.010118485428392887 0.9999748468399048\n",
      "34166  Emily 0.00842989981174469 0.9999779462814331\n",
      "23223  Anna 0.008786410093307495 0.9999790787696838\n",
      "8224  Sam 0.00887068547308445 0.9999809861183167\n",
      "12375  William 0.007692897692322731 0.9999812841415405\n",
      "14261  Bob 0.008489299565553665 0.9999816417694092\n",
      "11044  Peter 0.007557473611086607 0.9999819993972778\n",
      "458  an 0.007034697569906712 0.9999853372573853\n",
      "8397  Robert 0.006715177092701197 0.9999862909317017\n",
      "7937  Michael 0.006518770474940538 0.9999865293502808\n",
      "13928  Harry 0.006752442102879286 0.999987006187439\n",
      "28693  Amy 0.006678424775600433 0.9999884963035583\n",
      "8596  Bill 0.006940003484487534 0.9999885559082031\n",
      "2130 ____ 0.006717276759445667 0.9999890327453613\n",
      "6798  David 0.005564088001847267 0.99998939037323\n",
      "11108  Thomas 0.006088573019951582 0.9999898076057434\n",
      "8515  Alex 0.0056428685784339905 0.9999904036521912\n",
      "11387  Jim 0.005821018945425749 0.9999908208847046\n",
      "760  | 0.005952754989266396 0.9999915361404419\n",
      "47649  Sue 0.0061232056468725204 0.9999915361404419\n",
      "481  - 0.005886718165129423 0.9999915957450867\n",
      "4325  someone 0.006139956880360842 0.9999918937683105\n",
      "7607  Jack 0.005552870687097311 0.9999920725822449\n",
      "386  M 0.005329925566911697 0.9999921321868896\n",
      "9305  Ann 0.0058517055585980415 0.9999923706054688\n",
      "44070  Lucy 0.004542999900877476 0.9999945163726807\n",
      "825  one 0.004391206428408623 0.9999946355819702\n",
      "15037  Adam 0.0044175502844154835 0.9999948143959045\n",
      "13615  Andrew 0.003931187093257904 0.999995231628418\n",
      "6898  Paul 0.004086934961378574 0.9999952912330627\n",
      "18939  Matthew 0.003815287724137306 0.9999958276748657\n",
      "1059  her 0.0043483879417181015 0.9999961256980896\n",
      "9857  George 0.003361339680850506 0.999996542930603\n",
      "28708  Jerry 0.0034979786723852158 0.9999968409538269\n",
      "862  their 0.0036213905550539494 0.9999969601631165\n",
      "32819  Billy 0.0031264612916857004 0.9999971985816956\n",
      "502  j 0.0031692436896264553 0.9999974370002747\n",
      "47290  Lily 0.0030723903328180313 0.9999974370002747\n",
      "34236  Nancy 0.0030197210144251585 0.9999975562095642\n",
      "11565  Martin 0.0029063383117318153 0.999997615814209\n",
      "32016  Marie 0.002794323256239295 0.9999978542327881\n",
      "752  me 0.0028787641786038876 0.9999980926513672\n",
      "50870  Martha 0.0029486524872481823 0.9999980926513672\n",
      "1435  him 0.003082956187427044 0.999998152256012\n",
      "27926  Anne 0.0026303946506232023 0.9999982118606567\n",
      "17599  Henry 0.0024640902411192656 0.9999983310699463\n",
      "1534 mary 0.002961171790957451 0.9999984502792358\n",
      "2876  Mar 0.0024378516245633364 0.9999984502792358\n",
      "15118  Daniel 0.0022593503817915916 0.9999984502792358\n",
      "29201  Kate 0.002325539942830801 0.9999984502792358\n",
      "47211  Jill 0.0023748904932290316 0.9999984502792358\n",
      "10252  Carol 0.002416241681203246 0.9999986290931702\n",
      "31432  Rachel 0.0021906180772930384 0.9999986886978149\n",
      "32671  ______ 0.0022640973329544067 0.9999986886978149\n",
      "15115  Joseph 0.0022360903676599264 0.9999987483024597\n",
      "53665  Janet 0.0021831048652529716 0.9999987483024597\n",
      "296  m 0.0022185700945556164 0.9999988079071045\n",
      "12846  Joe 0.0019967088010162115 0.9999988079071045\n",
      "39642  john 0.0022436128929257393 0.9999988079071045\n",
      "42920  Sara 0.0021041512954980135 0.9999988079071045\n",
      "37549  Margaret 0.0020782388746738434 0.9999988675117493\n",
      "1304  __ 0.0020954483188688755 0.999998927116394\n",
      "34935  Emma 0.0019073102157562971 0.999998927116394\n",
      "9270  Frank 0.0020988215692341328 0.9999989867210388\n",
      "198 \n",
      " 0.0022056924644857645 0.9999990463256836\n",
      "1045  some 0.0020304278004914522 0.9999990463256836\n",
      "1105  them 0.0021088961511850357 0.9999990463256836\n",
      "27833  Jimmy 0.0018819291144609451 0.9999990463256836\n",
      "47261  Hannah 0.0018598433816805482 0.9999990463256836\n",
      "17618  Mrs 0.0018845278536900878 0.9999991059303284\n",
      "1817  each 0.0018196122255176306 0.9999991655349731\n",
      "9082  Smith 0.0016458395402878523 0.9999991655349731\n",
      "18528  Tony 0.0018314647022634745 0.9999991655349731\n",
      "29828  Laura 0.0016114764148369431 0.9999991655349731\n",
      "31071  Grace 0.0018866949249058962 0.9999991655349731\n",
      "7487  Max 0.0017190852668136358 0.9999992251396179\n",
      "14927  Charles 0.0016435682773590088 0.9999992251396179\n",
      "1378  two 0.0015369460452347994 0.9999992847442627\n",
      "7436  ___ 0.0016609345329925418 0.9999992847442627\n",
      "27488  Fred 0.0017008004942908883 0.9999992847442627\n",
      "4350  Jan 0.0015221332432702184 0.9999993443489075\n",
      "13809  Steve 0.001521021593362093 0.9999993443489075\n",
      "320  ( 0.0014593310188502073 0.9999994039535522\n",
      "716  _ 0.001473773387260735 0.9999994039535522\n",
      "856  x 0.0013731697108596563 0.9999994039535522\n",
      "619  J 0.0012757914373651147 0.999999463558197\n",
      "21998  Carl 0.0013087935512885451 0.999999463558197\n",
      "362  A 0.0013516052858904004 0.9999995231628418\n",
      "595  k 0.001179262762889266 0.9999995231628418\n",
      "892  which 0.0013523534871637821 0.9999995231628418\n",
      "1782 the 0.001171125564724207 0.9999995231628418\n",
      "3217  May 0.0013069685082882643 0.9999995231628418\n",
      "4392  Mr 0.0014259270392358303 0.9999995231628418\n",
      "10978  Mel 0.0013029670808464289 0.9999995231628418\n",
      "11867  Richard 0.0012154843425378203 0.9999995231628418\n",
      "13509  Kim 0.0014570156345143914 0.9999995231628418\n",
      "34645  Karen 0.0012622838839888573 0.9999995231628418\n",
      "50344  Maya 0.0014079650864005089 0.9999995231628418\n",
      "57400  Molly 0.001372561790049076 0.9999995231628418\n",
      "2441  another 0.0014226739294826984 0.9999995827674866\n",
      "21315  Simon 0.0012342102127149701 0.9999995827674866\n",
      "28556  Lisa 0.0012510676169767976 0.9999995827674866\n",
      "35683  Marcus 0.001247420092113316 0.9999995827674866\n",
      "62 _ 0.0013744094176217914 0.9999996423721313\n",
      "21891  Edward 0.001171792857348919 0.9999996423721313\n",
      "41793  Ruth 0.0012602396309375763 0.9999996423721313\n",
      "42781  Helen 0.0012506508501246572 0.9999996423721313\n",
      "44 M 0.0009687560377642512 0.9999997019767761\n",
      "400  $ 0.0009373624343425035 0.9999997019767761\n",
      "8127  Mart 0.0011351347202435136 0.9999997019767761\n",
      "274  s 0.0006586236413568258 0.9999997615814209\n",
      "444  L 0.0010295967804268003 0.9999997615814209\n",
      "1128  what 0.0008267848170362413 0.9999997615814209\n",
      "2326  three 0.0006986825028434396 0.9999997615814209\n",
      "13646  Ryan 0.0007939650095067918 0.9999997615814209\n",
      "16288  Sus 0.0009865795727819204 0.9999997615814209\n",
      "16364  Kevin 0.0007694760570302606 0.9999997615814209\n",
      "17513  Albert 0.0007433242863044143 0.9999997615814209\n",
      "24521  Jacob 0.000753212021663785 0.9999997615814209\n",
      "30743  ____ 0.0006633249577134848 0.9999997615814209\n",
      "38062  Linda 0.0009878943674266338 0.9999997615814209\n",
      "42575  Claire 0.0008705653599463403 0.9999997615814209\n",
      "50557  Clara 0.0010439835023134947 0.9999997615814209\n",
      "60597  Mia 0.0010898435721173882 0.9999997615814209\n",
      "61686 Alice 0.0011179731227457523 0.9999997615814209\n",
      "65892  _____ 0.0006999429315328598 0.9999997615814209\n",
      "97669  mike 0.0011469419114291668 0.9999997615814209\n",
      "387  be 0.0007093668100424111 0.9999998211860657\n",
      "678  all 0.0007064766250550747 0.9999998211860657\n",
      "3979 ________ 0.0005210079834796488 0.9999998211860657\n",
      "5973 ___ 0.0007474360172636807 0.9999998211860657\n",
      "9962  sam 0.001218988443724811 0.9999998211860657\n",
      "13658  Jackson 0.0009403462754562497 0.9999998211860657\n",
      "20731  Elizabeth 0.0007931871805340052 0.9999998211860657\n",
      "23826  Gary 0.0008979246485978365 0.9999998211860657\n",
      "24453  Luke 0.0008632076787762344 0.9999998211860657\n",
      "24832  Andy 0.0007363188778981566 0.9999998211860657\n",
      "25282  Alan 0.0007773764664307237 0.9999998211860657\n",
      "30145  Kay 0.0006561671034432948 0.9999998211860657\n",
      "31518  Terry 0.0008409006986767054 0.9999998211860657\n",
      "31880  Johnny 0.0006370218470692635 0.9999998211860657\n",
      "32072  Jake 0.0006755427457392216 0.9999998211860657\n",
      "51079  Jenny 0.0009877723641693592 0.9999998211860657\n",
      "62808  Samantha 0.000723491539247334 0.9999998211860657\n",
      "84946  james 0.000877309066709131 0.9999998211860657\n",
      "115733 玛丽 0.0008349362760782242 0.9999998211860657\n",
      "3 $ 0.00014851901505608112 0.9999998807907104\n",
      "13 . 0.0005148644559085369 0.9999998807907104\n",
      "259  t 0.0003331842599436641 0.9999998807907104\n",
      "271 \n",
      "\n",
      " 0.0007654192741028965 0.9999998807907104\n",
      "281  p 0.00028343487065285444 0.9999998807907104\n",
      "328  S 0.0007085279794409871 0.9999998807907104\n",
      "422  D 0.0005278974422253668 0.9999998807907104\n",
      "429  that 0.00046279386151582 0.9999998807907104\n",
      "431  R 0.0006151132984086871 0.9999998807907104\n",
      "432  it 0.00043589857523329556 0.9999998807907104\n",
      "697  your 0.00015871916548348963 0.9999998807907104\n",
      "883  man 0.0005495037185028195 0.9999998807907104\n",
      "1112 ... 0.0007806736975908279 0.9999998807907104\n",
      "1124  \\ 0.00035100438981316984 0.9999998807907104\n",
      "1417  Sh 0.00038662622682750225 0.9999998807907104\n",
      "1599  X 0.00037726908340118825 0.9999998807907104\n",
      "1868  mark 0.0004013877478428185 0.9999998807907104\n",
      "3303  Am 0.0001983985712286085 0.9999998807907104\n",
      "4841  Will 0.0004702659207396209 0.9999998807907104\n",
      "5748  Em 0.0001387632655678317 0.9999998807907104\n",
      "7355  Ben 0.00046482798643410206 0.9999998807907104\n",
      "7491  ma 0.00021365811699070036 0.9999998807907104\n",
      "8906  Miss 0.0003821589925792068 0.9999998807907104\n",
      "9338 ...\n",
      " 0.0003663487732410431 0.9999998807907104\n",
      "9354  Tim 0.00096509960712865 0.9999998807907104\n",
      "9453  Donald 0.0007446861709468067 0.9999998807907104\n",
      "9909 （ 0.00034896910074166954 0.9999998807907104\n",
      "11563  Dan 0.0006119916215538979 0.9999998807907104\n",
      "12066  Lee 0.0006049575749784708 0.9999998807907104\n",
      "14325  Ron 0.0003024424077011645 0.9999998807907104\n",
      "14583  Ali 0.0007708395132794976 0.9999998807907104\n",
      "14991  Nick 0.0007553114555776119 0.9999998807907104\n",
      "15964  Rose 0.0005533827934414148 0.9999998807907104\n",
      "16247  Eric 0.0004588806186802685 0.9999998807907104\n",
      "16439  Taylor 0.0005545726744458079 0.9999998807907104\n",
      "16931  Su 0.00039045856101438403 0.9999998807907104\n",
      "17089  Jordan 0.0005619408912025392 0.9999998807907104\n",
      "18477  Jason 0.0006996880983933806 0.9999998807907104\n",
      "19626  Patrick 0.0005659295711666346 0.9999998807907104\n",
      "19685  Jean 0.0005474619101732969 0.9999998807907104\n",
      "22388  Rick 0.0007562426617369056 0.9999998807907104\n",
      "22417  Victoria 0.0002465334546286613 0.9999998807907104\n",
      "23381  Mario 0.00033631460973992944 0.9999998807907104\n",
      "24927  Charlie 0.0005250101094134152 0.9999998807907104\n",
      "25072  jack 0.000709375599399209 0.9999998807907104\n",
      "25179  Philip 0.0002327471156604588 0.9999998807907104\n",
      "27118  Jeremy 0.0003659655340015888 0.9999998807907104\n",
      "29290  Larry 0.0008281409973278642 0.9999998807907104\n",
      "29650  Jennifer 0.0006915503181517124 0.9999998807907104\n",
      "31907  Samuel 0.0004617689992301166 0.9999998807907104\n",
      "32138  Ana 0.0006636054022237659 0.9999998807907104\n",
      "32205  Walter 0.0002395959454588592 0.9999998807907104\n",
      "32312  Victor 0.00041335998685099185 0.9999998807907104\n",
      "32391  Mack 0.00037209049332886934 0.9999998807907104\n",
      "32599  Danny 0.00047952233580872416 0.9999998807907104\n",
      "33253  mall 0.0003072466643061489 0.9999998807907104\n",
      "34031  Karl 0.0006752371555194259 0.9999998807907104\n",
      "37143  Angela 0.0005256668664515018 0.9999998807907104\n",
      "38385  Sandy 0.0006290030432865024 0.9999998807907104\n",
      "38648  Ronald 0.00015881413128226995 0.9999998807907104\n",
      "39294  Julia 0.0003254578332416713 0.9999998807907104\n",
      "40135  Xia 0.0005985933821648359 0.9999998807907104\n",
      "40523  Quinn 0.0002598491555545479 0.9999998807907104\n",
      "41421  Ivan 0.00014881757670082152 0.9999998807907104\n",
      "41759  Amanda 0.0006238673813641071 0.9999998807907104\n",
      "42482  Pedro 0.00017950724577531219 0.9999998807907104\n",
      "42570  Raymond 0.00013874222349841148 0.9999998807907104\n",
      "44358  М 0.00019572093151509762 0.9999998807907104\n",
      "44468  Timothy 0.0005450248718261719 0.9999998807907104\n",
      "46835  Riley 0.0004792643303517252 0.9999998807907104\n",
      "47199  Liz 0.00015786555013619363 0.9999998807907104\n",
      "56559  Mickey 0.00022476536105386913 0.9999998807907104\n",
      "57960  $\\ 0.00028788059717044234 0.9999998807907104\n",
      "59310  Sophie 0.0007562944083474576 0.9999998807907104\n",
      "61153  Paula 0.00021551206009462476 0.9999998807907104\n",
      "63440  Teresa 0.00014868970902170986 0.9999998807907104\n",
      "63687  Tina 0.0001711586519377306 0.9999998807907104\n",
      "64198  Kathy 0.0009065747726708651 0.9999998807907104\n",
      "68556 Maria 0.0005223692278377712 0.9999998807907104\n",
      "69331  Cindy 0.000250032520852983 0.9999998807907104\n",
      "70371  Sophia 0.0004926478140987456 0.9999998807907104\n",
      "70433  alice 0.0008729676483199 0.9999998807907104\n",
      "70968  Rita 0.0002409473090665415 0.9999998807907104\n",
      "72558  Dorothy 0.00022532792354468256 0.9999998807907104\n",
      "81352  Jasmine 0.00013519206549972296 0.9999998807907104\n",
      "84250  Melanie 0.00033878308022394776 0.9999998807907104\n",
      "64 a 0.0005378453060984612 0.9999999403953552\n",
      "91 | 0.0005741280037909746 0.9999999403953552\n",
      "256    0.0005150584620423615 0.9999999403953552\n",
      "270  th 0.00038816287997178733 0.9999999403953552\n",
      "289  w 0.00024534063413739204 0.9999999403953552\n",
      "305  h 0.00026477809296920896 0.9999999403953552\n",
      "335  } 0.00031383871100842953 0.9999999403953552\n",
      "342  g 0.0003372757346369326 0.9999999403953552\n",
      "350  T 0.00043631100561469793 0.9999999403953552\n",
      "358  I 0.0004512661835178733 0.9999999403953552\n",
      "374  is 0.0002310201816726476 0.9999999403953552\n",
      "379  y 0.0004801569739356637 0.9999999403953552\n",
      "451  N 0.00027952142409048975 0.9999999403953552\n",
      "498  you 0.00023490888997912407 0.9999999403953552\n",
      "576  The 0.00035918279900215566 0.9999999403953552\n",
      "633  get 0.00016315284301526845 0.9999999403953552\n",
      "730  K 0.0007677593966946006 0.9999999403953552\n",
      "847  my 0.0005253805429674685 0.9999999403953552\n",
      "1154  , 0.0002528305340092629 0.9999999403953552\n",
      "1181  its 0.0002285787631990388 0.9999999403953552\n",
      "1369  || 0.00030468907789327204 0.9999999403953552\n",
      "1449  every 0.00030067309853620827 0.9999999403953552\n",
      "1513  don 0.00015144285862334073 0.9999999403953552\n",
      "1527  An 0.00043821759754791856 0.9999999403953552\n",
      "1674  Al 0.0005711684352718294 0.9999999403953552\n",
      "2363  Man 0.00048759242054075 0.9999999403953552\n",
      "3199  Ed 0.00041755131678655744 0.9999999403953552\n",
      "3800  others 0.00041771537507884204 0.9999999403953552\n",
      "4279  half 0.00018331070896238089 0.9999999403953552\n",
      "4481  sus 0.0003702941758092493 0.9999999403953552\n",
      "4892  Rob 0.000324383145198226 0.9999999403953552\n",
      "5227  Har 0.000205074276891537 0.9999999403953552\n",
      "5561  himself 0.0005780654028058052 0.9999999403953552\n",
      "10180  tom 0.0004609637544490397 0.9999999403953552\n",
      "11266  Chris 0.0005745812086388469 0.9999999403953552\n",
      "11885  Jeff 0.0002547227777540684 0.9999999403953552\n",
      "13079 John 0.0001793889096006751 0.9999999403953552\n",
      "16046  Greg 0.00030757204513065517 0.9999999403953552\n",
      "17426  Wilson 0.0003812740324065089 0.9999999403953552\n",
      "18246  Josh 0.000293096003588289 0.9999999403953552\n",
      "18880  Marc 0.0005917524104006588 0.9999999403953552\n",
      "20740  Mi 0.0002689716056920588 0.9999999403953552\n",
      "22557  Dick 0.0001721313747111708 0.9999999403953552\n",
      "25235 his 0.0001850124535849318 0.9999999403953552\n",
      "26426  Cameron 0.00016351958038285375 0.9999999403953552\n",
      "27093 Ann 0.00016998329374473542 0.9999999403953552\n",
      "28084 James 0.0004670097550842911 0.9999999403953552\n",
      "28235  Ian 0.0003953995765186846 0.9999999403953552\n",
      "28373  Charlotte 0.000297286082059145 0.9999999403953552\n",
      "28504  Juan 0.0004490909050218761 0.9999999403953552\n",
      "28507  Roger 0.0006982277845963836 0.9999999403953552\n",
      "29297  Carlos 0.0006129055400379002 0.9999999403953552\n",
      "30246  Oliver 0.00022237490338739008 0.9999999403953552\n",
      "31360  Eve 0.00023129890905693173 0.9999999403953552\n",
      "31398  Mall 0.0005084462463855743 0.9999999403953552\n",
      "32026  Michelle 0.0003225903201382607 0.9999999403953552\n",
      "32367  Jessica 0.0005328208790160716 0.9999999403953552\n",
      "35192  bob 0.00022873436682857573 0.9999999403953552\n",
      "36273  Pete 0.00029023372917436063 0.9999999403953552\n",
      "41984  Noah 0.00019208708545193076 0.9999999403953552\n",
      "45744  Monica 0.0007444302900694311 0.9999999403953552\n",
      "45891  Miguel 0.00039561704033985734 0.9999999403953552\n",
      "45893  Christine 0.0001741131563903764 0.9999999403953552\n",
      "47816  hen 0.0001725795737002045 0.9999999403953552\n",
      "48398  Kenneth 0.00022098555928096175 0.9999999403953552\n",
      "48870 MARY 0.00019992110901512206 0.9999999403953552\n",
      "50106  Joan 0.0007323394529521465 0.9999999403953552\n",
      "51335  isa 0.0001797758013708517 0.9999999403953552\n",
      "51852  Sharon 0.000316605408443138 0.9999999403953552\n",
      "51989  Annie 0.00035179912811145186 0.9999999403953552\n",
      "53934  Patricia 0.00020334494183771312 0.9999999403953552\n",
      "53946  david 0.00018948130309581757 0.9999999403953552\n",
      "54661  Hank 0.00027906897594220936 0.9999999403953552\n",
      "55011  Betty 0.0006402241415344179 0.9999999403953552\n",
      "55154  Marty 0.0001871373096946627 0.9999999403953552\n",
      "59242  Tracy 0.0001794343552319333 0.9999999403953552\n",
      "61326  Katherine 0.0001440347550669685 0.9999999403953552\n",
      "65590  Xiao 0.0003250442387070507 0.9999999403953552\n",
      "67011  paul 0.00048810476437211037 0.9999999403953552\n",
      "72562  alan 0.00020918025984428823 0.9999999403953552\n",
      "80517 _____ 0.0006261084927245975 0.9999999403953552\n",
      "81348  Cathy 0.00019221616094000638 0.9999999403953552\n",
      "82178  Sasha 0.0005072240019217134 0.9999999403953552\n",
      "90933  Mei 0.00018006947357207537 0.9999999403953552\n",
      "94155  Manny 0.00018245974206365645 0.9999999403953552\n",
      "7 ( 0.00028465804643929005 1.0\n",
      "25 : 0.00013819790910929441 1.0\n",
      "294  d 0.0004181835101917386 1.0\n",
      "308  n 0.00023904546105768532 1.0\n",
      "315  of 0.00014479694073088467 1.0\n",
      "326  l 0.00042598662548698485 1.0\n",
      "330  \" 0.00024321449745912105 1.0\n",
      "356  C 0.0003275908238720149 1.0\n",
      "393  P 0.00022676779190078378 1.0\n",
      "419  this 0.00044540726230479777 1.0\n",
      "425  B 0.000279087049420923 1.0\n",
      "435  r 0.0003346232115291059 1.0\n",
      "438  as 0.00019105421961285174 1.0\n",
      "452  al 0.0003086684155277908 1.0\n",
      "467  W 0.0002805038820952177 1.0\n",
      "468  E 0.000303312495816499 1.0\n",
      "472  H 0.0004362648178357631 1.0\n",
      "479  G 0.00046332721831277013 1.0\n",
      "508  [ 0.00023592781508341432 1.0\n",
      "563 __ 0.0003225178807042539 1.0\n",
      "659  . 0.00019256953964941204 1.0\n",
      "686  will 0.00023178508854471147 1.0\n",
      "809  Y 0.0003245577390771359 1.0\n",
      "937  ? 0.00036755745531991124 1.0\n",
      "1008  other 0.0002350755239604041 1.0\n",
      "1079  am 0.0003403668524697423 1.0\n",
      "1281  make 0.00016077830514404923 1.0\n",
      "1697  person 0.0004127626307308674 1.0\n",
      "1803  car 0.0001945087715284899 1.0\n",
      "1863  Z 0.00019039589096792042 1.0\n",
      "1932  max 0.00019132894522044808 1.0\n",
      "2157  Me 0.00018975368584506214 1.0\n",
      "2176  both 0.0002114039089065045 1.0\n",
      "2503  ... 0.0003378341789357364 1.0\n",
      "2935  ann 0.00032329000532627106 1.0\n",
      "3040  four 0.0001836839219322428 1.0\n",
      "3070  ** 0.0001351283281110227 1.0\n",
      "3081  market 0.00014777087199036032 1.0\n",
      "3189 (M 0.00019009801326319575 1.0\n",
      "3553  store 0.00017900631064549088 1.0\n",
      "3594  mar 0.0007915446767583489 1.0\n",
      "3680  Ab 0.00020959656103514135 1.0\n",
      "3695  buy 0.00023553817300125957 1.0\n",
      "3984  El 0.00037381230504252017 1.0\n",
      "4031  bill 0.00020332736312411726 1.0\n",
      "4236  five 0.00016552087618038058 1.0\n",
      "4320  Don 0.00022051591076888144 1.0\n",
      "5019  everyone 0.0002886455040425062 1.0\n",
      "5271  Mich 0.00023662945022806525 1.0\n",
      "5301  His 0.0003101482579950243 1.0\n",
      "5470  March 0.00016130373114719987 1.0\n",
      "6554  mother 0.00014625460607931018 1.0\n",
      "8211  Sal 0.0002734757144935429 1.0\n",
      "8394  Mal 0.0003008281346410513 1.0\n",
      "8711  whom 0.00040293909842148423 1.0\n",
      "8908  � 0.0002882976259570569 1.0\n",
      "8949 Mark 0.00037499191239476204 1.0\n",
      "9483  Mad 0.00020106046576984227 1.0\n",
      "10147  Mil 0.0002741820353548974 1.0\n",
      "10236  � 0.0002490673796273768 1.0\n",
      "10562  Wil 0.0002601347805466503 1.0\n",
      "10588  Jesus 0.00022380053997039795 1.0\n",
      "10946  Jo 0.00020705249335151166 1.0\n",
      "11038  Roy 0.00019505477393977344 1.0\n",
      "11331  Ma 0.0003239843645133078 1.0\n",
      "11351  Johnson 0.0003573985886760056 1.0\n",
      "13225  Kar 0.00015769473975524306 1.0\n",
      "13255  Ray 0.00034446222707629204 1.0\n",
      "13374  Matt 0.00041861023055389524 1.0\n",
      "14259  Ken 0.00045543067972175777 1.0\n",
      "14595  Ash 0.00016865912766661495 1.0\n",
      "15152  Ram 0.00023109551693778485 1.0\n",
      "15993  Santa 0.0001489100541220978 1.0\n",
      "17083  Brian 0.0002504684671293944 1.0\n",
      "17143  Robin 0.00017072109039872885 1.0\n",
      "17317  mr 0.00015025795437395573 1.0\n",
      "18095  Stephen 0.0001918052148539573 1.0\n",
      "18661  Kelly 0.0003586412640288472 1.0\n",
      "18919  Jay 0.00029624131275340915 1.0\n",
      "19260  Austin 0.00014409162395168096 1.0\n",
      "19929  Ger 0.00023532639897894114 1.0\n",
      "20042  Alexander 0.00034446505014784634 1.0\n",
      "20238  Dave 0.0004773796536028385 1.0\n",
      "20696  Anthony 0.0003947628429159522 1.0\n",
      "22467  Justin 0.000187710887985304 1.0\n",
      "22846  charity 0.0001462125510443002 1.0\n",
      "22954  Morgan 0.0001649722398724407 1.0\n",
      "23080  Angel 0.0002320114290341735 1.0\n",
      "23115  Ted 0.00040397493285126984 1.0\n",
      "23425  kar 0.00019932769646402448 1.0\n",
      "23644  Steven 0.0002415368362562731 1.0\n",
      "23950 Sam 0.00023522108676843345 1.0\n",
      "24732 Tom 0.00023248042271006852 1.0\n",
      "24893  Jen 0.00025316112441942096 1.0\n",
      "24986  Christopher 0.0001598579401616007 1.0\n",
      "24995  Sean 0.0002670626272447407 1.0\n",
      "25225  Raj 0.00025148995337076485 1.0\n",
      "25516 Michael 0.00014426250709220767 1.0\n",
      "26154  Fel 0.00015445899043697864 1.0\n",
      "27014 ,M 0.00014055946667212993 1.0\n",
      "27343  Meg 0.0007279982673935592 1.0\n",
      "27586  Arthur 0.00020422885427251458 1.0\n",
      "28003  Beth 0.0005145302275195718 1.0\n",
      "28047  sar 0.0002354947937419638 1.0\n",
      "28246  Wang 0.00032000025385059416 1.0\n",
      "28729  Mari 0.00034900690661743283 1.0\n",
      "30259  Kyle 0.0003037698334082961 1.0\n",
      "31107  Barbara 0.0003865972103085369 1.0\n",
      "32726  Gill 0.00015698079369030893 1.0\n",
      "32862  jan 0.00034594020689837635 1.0\n",
      "33121  Neil 0.0002982727310154587 1.0\n",
      "34181  Marco 0.00020858447533100843 1.0\n",
      "35757  Jamie 0.0004198790993541479 1.0\n",
      "36559 Peter 0.0001615365908946842 1.0\n",
      "36737  Nathan 0.00015679241914767772 1.0\n",
      "36748  Leo 0.0001491313159931451 1.0\n",
      "38271  whoever 0.00015345426800195128 1.0\n",
      "39039  Tommy 0.00024038509582169354 1.0\n",
      "39171  Jesse 0.00023776121088303626 1.0\n",
      "40290  ana 0.00024611447588540614 1.0\n",
      "41187  Julie 0.00026332351262681186 1.0\n",
      "41508  Isaac 0.000224210336455144 1.0\n",
      "41563  Catherine 0.0004094923206139356 1.0\n",
      "42323  Derek 0.00019589901785366237 1.0\n",
      "42712  Theresa 0.00024546042550355196 1.0\n",
      "43413  Pam 0.000185105818673037 1.0\n",
      "43581  Manuel 0.0003367060562595725 1.0\n",
      "43943  Evan 0.00014436899800784886 1.0\n",
      "44409  Diana 0.0002854384365491569 1.0\n",
      "44457  Rebecca 0.0004986021085642278 1.0\n",
      "46585  Ellen 0.00017057041986845434 1.0\n",
      "47048  Melissa 0.00021567093790508807 1.0\n",
      "49232  ken 0.0001524091203464195 1.0\n",
      "52115  Harold 0.00020146797760389745 1.0\n",
      "53083 Sarah 0.00032501478563062847 1.0\n",
      "53090  Diane 0.00014791458670515567 1.0\n",
      "53391  Caroline 0.0003673663013614714 1.0\n",
      "54058  Judy 0.00062258739490062 1.0\n",
      "54529  Megan 0.000696303672157228 1.0\n",
      "54576  Wendy 0.00031566875986754894 1.0\n",
      "55686  Sandra 0.0007098872447386384 1.0\n",
      "55784  kim 0.00019446073565632105 1.0\n",
      "56478  alex 0.00025697180535644293 1.0\n",
      "58397  Maxwell 0.00014212590758688748 1.0\n",
      "59287  Olivia 0.00044062305823899806 1.0\n",
      "59370  Chloe 0.0002458151720929891 1.0\n",
      "60642  Liam 0.0001830137480283156 1.0\n",
      "61695  Maggie 0.000154676177771762 1.0\n",
      "62164  Ethan 0.00016141559171956033 1.0\n",
      "62502 Jane 0.00026723463088274 1.0\n",
      "63283  amy 0.0002845415729098022 1.0\n",
      "66181  Kitty 0.00014538342657033354 1.0\n",
      "74641  Zoe 0.00017484495765529573 1.0\n",
      "75009 Amy 0.0002479600952938199 1.0\n",
      "76212  Gina 0.00017136310634668916 1.0\n",
      "81062  Lena 0.00023638807761017233 1.0\n",
      "83677  Meredith 0.0003104839997831732 1.0\n",
      "84048 Emily 0.00017651589587330818 1.0\n",
      "85283 Susan 0.00015665647515561432 1.0\n",
      "85504  Mara 0.0003827943583019078 1.0\n",
      "87544  hans 0.00015281300875358284 1.0\n",
      "87601  Becky 0.0005528259789571166 1.0\n",
      "87906  michael 0.0003704921982716769 1.0\n",
      "89391  Sonia 0.00019218758097849786 1.0\n",
      "90879  Amelia 0.0002687597298063338 1.0\n",
      "94075  Sheila 0.00030389317544177175 1.0\n",
      "95419  Marian 0.000275896250968799 1.0\n",
      "97126  fred 0.00013779995788354427 1.0\n",
      "98606  jane 0.0006431411020457745 1.0\n",
      "132657  Дж 0.00024914537789300084 1.0\n",
      "566  he 0.00013743971067015082 1.0000001192092896\n",
      "894  any 0.00015053863171488047 1.0000001192092896\n",
      "7135  Pat 0.0001367700460832566 1.0000001192092896\n",
      "9815  Scott 0.00016212998889386654 1.0000001192092896\n",
      "10917  twice 0.0001395803119521588 1.0000001192092896\n",
      "11935  Jones 0.00018465422908775508 1.0000001192092896\n",
      "14268  Mir 0.0002775757748167962 1.0000001192092896\n",
      "22586  Mitch 0.0001908359699882567 1.0000001192092896\n",
      "23201  Marg 0.0002893941127695143 1.0000001192092896\n",
      "28827  Mason 0.00018095309496857226 1.0000001192092896\n",
      "29938  Mitchell 0.00013594968186225742 1.0000001192092896\n",
      "37118  Ashley 0.0001514417235739529 1.0000001192092896\n",
      "40498 Andrew 0.00021768410806544125 1.0000001192092896\n",
      "64814  Mae 0.00016616737411823124 1.0000001192092896\n",
      "68310  Aly 0.00015492693637497723 1.0000001192092896\n"
     ]
    }
   ],
   "source": [
    "grad_changes = (grad_accumulation ** 2).sum(dim=1)\n",
    "grad_changes_mask = (grad_changes > 0.01)\n",
    "changed_tokens = grad_changes_mask.nonzero().squeeze()\n",
    "\n",
    "print(changed_tokens.shape[0], \"tokens have embedding changed. They are:\")\n",
    "_changed = []\n",
    "for t_ in changed_tokens.cpu().tolist():\n",
    "  cos_ = F.cosine_similarity(original_model.model.embed_tokens.weight[t_], model.model.embed_tokens.weight[t_], dim=0).item()\n",
    "  _changed.append((t_, tokenizer.decode(t_), grad_accumulation[t_,0].norm(p=1).item(), cos_))\n",
    "\n",
    "print(\"idx, token str, grad_accumulation p1, cos before-after\")\n",
    "print()\n",
    "for t_, s_, v_, c_ in sorted(_changed, key=lambda obj: obj[3]):\n",
    "  print(t_, s_, v_, c_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f24f69-7336-4089-971c-ffa3e797d2a2",
   "metadata": {},
   "source": [
    "### Make the prediction not correct\n",
    "\n",
    "It's relatively easy to make the prediction not correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c9aa6cd4-5295-4be8-a83f-6bcbef846ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.zero_grad()\n",
    "with torch.no_grad():\n",
    "  model.model.embed_tokens.weight.data = original_model.model.embed_tokens.weight.data.clone().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1130cdae-5a1b-4251-8ee1-69f5a21c89e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [' Mary', ' the', ' a']\n",
      "1 [' the', ' a', ' his']\n"
     ]
    }
   ],
   "source": [
    "CORRECT_LABEL_IDX = tokenizer.encode(\" Mary\")[0]\n",
    "grad_accumulation = torch.zeros(model.model.embed_tokens.weight.data.shape, requires_grad=False).to(device)\n",
    "\n",
    "for idx in range(100):\n",
    "  out = model(toks)\n",
    "  logits = out.logits\n",
    "  with torch.no_grad():\n",
    "    topk = logits[0,-1].topk(3)\n",
    "    out_toks = tokenizer.batch_decode(topk.indices)\n",
    "    print(idx, out_toks)\n",
    "    if topk.indices[0].item() != CORRECT_LABEL_IDX:\n",
    "      model.zero_grad()\n",
    "      break\n",
    "\n",
    "  loss = F.cross_entropy(logits[0,-1], torch.tensor(CORRECT_LABEL_IDX, device=device))\n",
    "  loss.backward()\n",
    "  with torch.no_grad():\n",
    "    grad_accumulation += model.model.embed_tokens.weight.grad\n",
    "\n",
    "  model.model.embed_tokens.weight.data = model.model.embed_tokens.weight + 1e-3 * model.model.embed_tokens.weight.grad\n",
    "  model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2db27720-b82e-4fb3-b10e-015e74543c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 tokens have embedding changed. They are:\n",
      "(idx, token str, grad_accumulation p1, cos before-after)\n",
      "\n",
      "10244  Mary 0.46845513582229614 0.9986592531204224\n",
      "279  the 0.03706904500722885 0.9988864660263062\n",
      "8061  shop 0.11342021822929382 0.9998895525932312\n",
      "4498 When 0.05344673991203308 0.9999078512191772\n",
      "3937  went 0.032146092504262924 0.9999401569366455\n",
      "311  to 0.2873799502849579 0.9999438524246216\n",
      "3757  John 0.04006228968501091 0.9999444484710693\n",
      "6551  gave 0.18170922994613647 0.9999549388885498\n",
      "8968  bag 0.09388038516044617 0.9999561309814453\n",
      "323  and 0.15183570981025696 0.9999767541885376\n",
      "264  a 0.005127974320203066 0.9999830722808838\n",
      "806  his 0.0041706436313688755 0.9999920725822449\n",
      "11 , 0.04456475377082825 0.999993085861206\n",
      "29405  Alice 0.0010434952564537525 0.9999995231628418\n",
      "20445  Sarah 0.0005189783405512571 0.9999998211860657\n",
      "8224  Sam 0.00013742889859713614 0.9999998807907104\n",
      "7801  James 0.0002439873933326453 0.9999999403953552\n",
      "458  an 0.00014632524107582867 1.0\n",
      "4325  someone 0.00013345701154321432 1.0\n",
      "21475  Jane 0.00020099429821129888 1.0\n",
      "23016  Maria 0.00013226382725406438 1.0\n",
      "23223  Anna 0.0002476042718626559 1.0\n",
      "29933  Susan 0.00012020429130643606 1.0\n",
      "34166  Emily 0.00016106815019156784 1.0\n",
      "52291  Sally 0.00020744497305713594 1.0\n"
     ]
    }
   ],
   "source": [
    "summarize_grad_accumulation(grad_accumulation, model, original_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85aa7b9-7440-4051-9276-574d66385400",
   "metadata": {},
   "source": [
    "### Randomly jittering the embedding\n",
    "\n",
    "We know that just randomly jittering the embedding of the last token is already too hard to flip prediction. What if randomly jitter embeddings of all the tokens in the sentence?\n",
    "\n",
    "Still very hard to change prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "068afd44-9181-44f8-8716-1f7c7e2ec968",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [' Mary', ' the', ' a'] cosine last tok: 0.9999999403953552\n",
      "1 [' Mary', ' the', ' a'] cosine last tok: 0.9999843835830688\n",
      "2 [' Mary', ' the', ' a'] cosine last tok: 0.9999337196350098\n",
      "3 [' Mary', ' the', ' a'] cosine last tok: 0.999849259853363\n",
      "4 [' Mary', ' the', ' a'] cosine last tok: 0.9997230768203735\n",
      "5 [' Mary', ' the', ' a'] cosine last tok: 0.9995887279510498\n",
      "6 [' Mary', ' the', ' a'] cosine last tok: 0.9994329810142517\n",
      "7 [' Mary', ' the', ' a'] cosine last tok: 0.9991805553436279\n",
      "8 [' Mary', ' the', ' a'] cosine last tok: 0.9989150762557983\n",
      "9 [' Mary', ' the', ' a'] cosine last tok: 0.9987057447433472\n",
      "10 [' Mary', ' the', ' a'] cosine last tok: 0.9983136653900146\n",
      "11 [' Mary', ' the', ' a'] cosine last tok: 0.9980168342590332\n",
      "12 [' Mary', ' the', ' a'] cosine last tok: 0.9976080656051636\n",
      "13 [' Mary', ' the', ' a'] cosine last tok: 0.9972718358039856\n",
      "14 [' Mary', ' the', ' a'] cosine last tok: 0.9966927170753479\n",
      "15 [' Mary', ' the', ' a'] cosine last tok: 0.9963391423225403\n",
      "16 [' Mary', ' the', ' a'] cosine last tok: 0.9956716895103455\n",
      "17 [' Mary', ' the', ' a'] cosine last tok: 0.995141327381134\n",
      "18 [' Mary', ' the', ' a'] cosine last tok: 0.9947561025619507\n",
      "19 [' Mary', ' the', ' a'] cosine last tok: 0.9939979314804077\n",
      "20 [' Mary', ' the', ' a'] cosine last tok: 0.9935306310653687\n",
      "21 [' Mary', ' the', ' a'] cosine last tok: 0.9926511645317078\n",
      "22 [' Mary', ' the', ' a'] cosine last tok: 0.9921947717666626\n",
      "23 [' Mary', ' the', ' a'] cosine last tok: 0.9911044239997864\n",
      "24 [' Mary', ' the', ' a'] cosine last tok: 0.9902586340904236\n",
      "25 [' Mary', ' the', ' a'] cosine last tok: 0.9888225793838501\n",
      "26 [' Mary', ' the', ' a'] cosine last tok: 0.9889380931854248\n",
      "27 [' Mary', ' the', ' a'] cosine last tok: 0.9887411594390869\n",
      "28 [' Mary', ' the', ' a'] cosine last tok: 0.9874001741409302\n",
      "29 [' Mary', ' the', ' a'] cosine last tok: 0.9852566719055176\n",
      "30 [' the', ' Mary', ' a'] cosine last tok: 0.985756516456604\n",
      "31 [' the', ' Mary', ' a'] cosine last tok: 0.9838172197341919\n",
      "32 [' Mary', ' the', ' a'] cosine last tok: 0.9843288660049438\n",
      "33 [' Mary', ' the', ' a'] cosine last tok: 0.9830068349838257\n",
      "34 [' Mary', ' the', ' a'] cosine last tok: 0.9810025095939636\n",
      "35 [' the', ' Mary', ' a'] cosine last tok: 0.9815411567687988\n",
      "36 [' the', ' Mary', ' a'] cosine last tok: 0.9785726070404053\n",
      "37 [' the', ' Mary', ' a'] cosine last tok: 0.9768926501274109\n",
      "38 [' the', ' Mary', ' a'] cosine last tok: 0.9778751134872437\n",
      "39 [' Mary', ' the', ' a'] cosine last tok: 0.9764103889465332\n",
      "40 [' Mary', ' the', ' a'] cosine last tok: 0.9747782945632935\n",
      "41 [' Mary', ' the', ' a'] cosine last tok: 0.9729180335998535\n",
      "42 [' Mary', ' the', ' a'] cosine last tok: 0.973678708076477\n",
      "43 [' Mary', ' a', ' the'] cosine last tok: 0.9720826148986816\n",
      "44 [' the', ' Mary', ' a'] cosine last tok: 0.9706283211708069\n",
      "45 [' Mary', ' the', ' a'] cosine last tok: 0.9676891565322876\n",
      "46 [' the', ' Mary', ' a'] cosine last tok: 0.9647479057312012\n",
      "47 [' Mary', ' the', ' a'] cosine last tok: 0.9660816192626953\n",
      "48 [' Mary', ' the', ' a'] cosine last tok: 0.9648725986480713\n",
      "49 [' Mary', ' the', ' a'] cosine last tok: 0.9617618322372437\n",
      "50 [' Mary', ' a', ' his'] cosine last tok: 0.9612958431243896\n",
      "51 [' Mary', ' a', ' the'] cosine last tok: 0.9590295553207397\n",
      "52 [' the', ' a', ' Mary'] cosine last tok: 0.956994354724884\n",
      "53 [' the', ' Mary', ' a'] cosine last tok: 0.9573849439620972\n",
      "54 [' the', ' Mary', ' a'] cosine last tok: 0.9533672332763672\n",
      "55 [' Mary', ' a', ' the'] cosine last tok: 0.951848566532135\n",
      "56 [' Mary', ' the', ' a'] cosine last tok: 0.9541401863098145\n",
      "57 [' Mary', ' the', ' a'] cosine last tok: 0.9484953880310059\n",
      "58 [' the', ' a', ' his'] cosine last tok: 0.9482934474945068\n",
      "59 [' Mary', ' the', ' a'] cosine last tok: 0.944977343082428\n",
      "60 [' Mary', ' the', ' a'] cosine last tok: 0.947433590888977\n",
      "61 [' Mary', ' a', ' the'] cosine last tok: 0.9448038339614868\n",
      "62 [' the', ' Mary', ' a'] cosine last tok: 0.9420222640037537\n",
      "63 [' Mary', ' a', ' his'] cosine last tok: 0.9407482743263245\n",
      "64 [' the', ' Mary', ' a'] cosine last tok: 0.9369314908981323\n",
      "65 [' Mary', ' the', ' a'] cosine last tok: 0.9350603818893433\n",
      "66 [' Mary', ' the', ' a'] cosine last tok: 0.9347156286239624\n",
      "67 [' the', ' Mary', ' a'] cosine last tok: 0.9323716759681702\n",
      "68 [' the', ' Mary', ' a'] cosine last tok: 0.9349114894866943\n",
      "69 [' Mary', ' the', ' a'] cosine last tok: 0.9314185380935669\n",
      "70 [' the', ' Mary', ' a'] cosine last tok: 0.9267731308937073\n",
      "71 [' Mary', ' the', ' a'] cosine last tok: 0.923048734664917\n",
      "72 [' Mary', ' the', ' a'] cosine last tok: 0.9219916462898254\n",
      "73 [' Mary', ' the', ' a'] cosine last tok: 0.924909770488739\n",
      "74 [' Mary', ' a', ' his'] cosine last tok: 0.9232134819030762\n",
      "75 [' Mary', ' a', ' the'] cosine last tok: 0.9107809066772461\n",
      "76 [' the', ' a', ' Mary'] cosine last tok: 0.9144958257675171\n",
      "77 [' the', ' Mary', ' a'] cosine last tok: 0.9127861261367798\n",
      "78 [' Mary', ' the', ' a'] cosine last tok: 0.9124484658241272\n",
      "79 [' Mary', ' the', ' a'] cosine last tok: 0.9094588756561279\n",
      "80 [' Mary', ' his', ' a'] cosine last tok: 0.9088978171348572\n",
      "81 [' Mary', ' the', ' a'] cosine last tok: 0.903985321521759\n",
      "82 [' Mary', ' the', ' a'] cosine last tok: 0.9025957584381104\n",
      "83 [' Mary', ' the', ' a'] cosine last tok: 0.9003933668136597\n",
      "84 [' the', ' a', ' Mary'] cosine last tok: 0.8961990475654602\n",
      "85 [' the', ' Mary', ' a'] cosine last tok: 0.893383264541626\n",
      "86 [' the', ' Mary', ' a'] cosine last tok: 0.8938301205635071\n",
      "87 [' the', ' Mary', ' a'] cosine last tok: 0.8976268768310547\n",
      "88 [' the', ' his', ' a'] cosine last tok: 0.8863918781280518\n",
      "89 [' the', ' Mary', ' a'] cosine last tok: 0.8877105116844177\n",
      "90 [' Mary', ' his', ' a'] cosine last tok: 0.8873342871665955\n",
      "91 [' Mary', ' the', ' a'] cosine last tok: 0.8890212774276733\n",
      "92 [' Mary', ' a', ' his'] cosine last tok: 0.8824872374534607\n",
      "93 [' the', ' Mary', ' a'] cosine last tok: 0.8836960792541504\n",
      "94 [' the', ' Mary', ' a'] cosine last tok: 0.8772145509719849\n",
      "95 [' Mary', ' the', ' a'] cosine last tok: 0.8826547861099243\n",
      "96 [' the', ' Mary', ' a'] cosine last tok: 0.8725879192352295\n",
      "97 [' Mary', ' the', ' a'] cosine last tok: 0.8612371683120728\n",
      "98 [' Mary', ' the', ' a'] cosine last tok: 0.8738265633583069\n",
      "99 [' Mary', ' the', ' a'] cosine last tok: 0.8658432960510254\n",
      "100 [' Mary', ' the', ' a'] cosine last tok: 0.8735796213150024\n",
      "101 [' the', ' a', ' Alice'] cosine last tok: 0.8638250827789307\n",
      "102 [' Mary', ' the', ' a'] cosine last tok: 0.8560836911201477\n",
      "103 [' Mary', ' a', ' the'] cosine last tok: 0.8554322719573975\n",
      "104 [' the', ' Mary', ' a'] cosine last tok: 0.8641291856765747\n",
      "105 [' the', ' Mary', ' a'] cosine last tok: 0.8585746884346008\n",
      "106 [' the', ' a', ' Mary'] cosine last tok: 0.8491989374160767\n",
      "107 [' Mary', ' a', ' the'] cosine last tok: 0.8446566462516785\n",
      "108 [' the', ' a', ' Mary'] cosine last tok: 0.8452956080436707\n",
      "109 [' the', ' Mary', ' a'] cosine last tok: 0.8439884781837463\n",
      "110 [' the', ' Mary', ' a'] cosine last tok: 0.8511779308319092\n",
      "111 [' the', ' a', ' Mary'] cosine last tok: 0.8383865356445312\n",
      "112 [' Mary', ' his', ' a'] cosine last tok: 0.8450198173522949\n",
      "113 [' Mary', ' a', ' the'] cosine last tok: 0.8296445608139038\n",
      "114 [' the', ' Mary', ' a'] cosine last tok: 0.8253250122070312\n",
      "115 [' Mary', ' a', ' Alice'] cosine last tok: 0.832876980304718\n",
      "116 [' the', ' Mary', ' a'] cosine last tok: 0.8304281234741211\n",
      "117 [' the', ' a', ' Mary'] cosine last tok: 0.8355798125267029\n",
      "118 [' Mary', ' the', ' his'] cosine last tok: 0.8258824348449707\n",
      "119 [' the', ' Mary', ' a'] cosine last tok: 0.8138691186904907\n",
      "120 [' the', ' a', ' Mary'] cosine last tok: 0.8172659873962402\n",
      "121 [' the', ' a', ' Mary'] cosine last tok: 0.8257524967193604\n",
      "122 [' the', ' Mary', ' a'] cosine last tok: 0.8217200040817261\n",
      "123 [' Mary', ' a', ' the'] cosine last tok: 0.8221916556358337\n",
      "124 [' the', ' a', ' his'] cosine last tok: 0.8136122822761536\n",
      "125 [' a', ' his', ' Alice'] cosine last tok: 0.7980901002883911\n",
      "126 [' the', ' Mary', ' a'] cosine last tok: 0.8074719309806824\n",
      "127 [' Mary', ' the', ' a'] cosine last tok: 0.8077038526535034\n",
      "128 [' the', ' a', ' Mary'] cosine last tok: 0.7950145602226257\n",
      "129 [' the', ' Mary', ' a'] cosine last tok: 0.8045467138290405\n",
      "130 [' the', ' Mary', ' a'] cosine last tok: 0.7990356087684631\n",
      "131 [' the', ' Mary', ' a'] cosine last tok: 0.7918736934661865\n",
      "132 [' Mary', ' a', ' Alice'] cosine last tok: 0.7810088396072388\n",
      "133 [' the', ' Mary', ' a'] cosine last tok: 0.7961779832839966\n",
      "134 [' Mary', ' the', ' to'] cosine last tok: 0.7837537527084351\n",
      "135 [' Mary', ' the', ' a'] cosine last tok: 0.7911041378974915\n",
      "136 [' Mary', ' his', ' a'] cosine last tok: 0.7879334688186646\n",
      "137 [' Mary', ' Alice', ' a'] cosine last tok: 0.7750239968299866\n",
      "138 [' the', ' Mary', ' a'] cosine last tok: 0.7741196751594543\n",
      "139 [' the', ' Mary', ' a'] cosine last tok: 0.7842234373092651\n",
      "140 [' the', ' a', ' his'] cosine last tok: 0.7796257734298706\n",
      "141 [' Mary', ' a', ' Alice'] cosine last tok: 0.7570578455924988\n",
      "142 [' the', ' Mary', ' a'] cosine last tok: 0.7689123153686523\n",
      "143 [' Mary', ' the', ' his'] cosine last tok: 0.7855851054191589\n",
      "144 [' Mary', ' his', ' a'] cosine last tok: 0.7718181610107422\n",
      "145 [' Mary', ' the', ' a'] cosine last tok: 0.7642275094985962\n",
      "146 [' the', ' a', ' Mary'] cosine last tok: 0.7356091141700745\n",
      "147 [' a', ' the', ' Alice'] cosine last tok: 0.7668825387954712\n",
      "148 [' the', ' a', ' his'] cosine last tok: 0.7393372058868408\n",
      "149 [' the', ' Mary', ' a'] cosine last tok: 0.7417920231819153\n",
      "150 [' the', ' Mary', ' a'] cosine last tok: 0.7490365505218506\n",
      "151 [' Mary', ' the', ' a'] cosine last tok: 0.7556017637252808\n",
      "152 [' Mary', ' his', ' a'] cosine last tok: 0.7720245122909546\n",
      "153 [' the', ' Mary', ' a'] cosine last tok: 0.7549130320549011\n",
      "154 [' the', ' a', ' Mary'] cosine last tok: 0.7526819109916687\n",
      "155 [' Mary', ' the', ' a'] cosine last tok: 0.7517871856689453\n",
      "156 [' the', ' Mary', ' a'] cosine last tok: 0.7528016567230225\n",
      "157 [' Mary', ' his', ' a'] cosine last tok: 0.7384161949157715\n",
      "158 [' Mary', ' the', ' his'] cosine last tok: 0.7540616989135742\n",
      "159 [' the', ' Mary', ' his'] cosine last tok: 0.7497044801712036\n",
      "160 [' Mary', ' the', ' his'] cosine last tok: 0.7491919994354248\n",
      "161 [' the', ' Mary', ' a'] cosine last tok: 0.7562891840934753\n",
      "162 [' the', ' Mary', ' a'] cosine last tok: 0.734660804271698\n",
      "163 [' Mary', ' the', ' his'] cosine last tok: 0.7237544059753418\n",
      "164 [' Mary', ' a', ' his'] cosine last tok: 0.7341094613075256\n",
      "165 [' the', ' Mary', ' a'] cosine last tok: 0.7171965837478638\n",
      "166 [' Mary', ' the', ' his'] cosine last tok: 0.7209202647209167\n",
      "167 [' Mary', ' a', ' his'] cosine last tok: 0.7190524935722351\n",
      "168 [' Mary', ' the', ' his'] cosine last tok: 0.7361977100372314\n",
      "169 [' Mary', ' the', ' his'] cosine last tok: 0.7155090570449829\n",
      "170 [' Mary', ' his', ' a'] cosine last tok: 0.7119921445846558\n",
      "171 [' the', ' Mary', ' his'] cosine last tok: 0.7282195091247559\n",
      "172 [' Mary', ' his', ' a'] cosine last tok: 0.7033145427703857\n",
      "173 [' Mary', ' the', ' his'] cosine last tok: 0.6974061727523804\n",
      "174 [' Mary', ' the', ' his'] cosine last tok: 0.7122896909713745\n",
      "175 [' the', ' Mary', ' a'] cosine last tok: 0.6970760226249695\n",
      "176 [' Mary', ' the', ' a'] cosine last tok: 0.7099220156669617\n",
      "177 [' Mary', ' the', ' his'] cosine last tok: 0.6859278082847595\n",
      "178 [' Mary', ' the', ' his'] cosine last tok: 0.6958063840866089\n",
      "179 [' Mary', ' the', ' his'] cosine last tok: 0.6873269081115723\n",
      "180 [' Mary', ' and', ' his'] cosine last tok: 0.7051630020141602\n",
      "181 [' the', ' Mary', ' his'] cosine last tok: 0.7040468454360962\n",
      "182 [' the', ' a', ' and'] cosine last tok: 0.6900497674942017\n",
      "183 [' Mary', ' the', ' a'] cosine last tok: 0.6860737800598145\n",
      "184 [' the', ' shop', ' a'] cosine last tok: 0.6899200081825256\n",
      "185 [' the', ' Mary', ' Alice'] cosine last tok: 0.6866589784622192\n",
      "186 [' the', ' Mary', 'Mary'] cosine last tok: 0.7021650075912476\n",
      "187 [' Mary', ' his', ' a'] cosine last tok: 0.6833758354187012\n",
      "188 [' Mary', ' and', ','] cosine last tok: 0.6799523234367371\n",
      "189 [' the', ' Mary', ' his'] cosine last tok: 0.6771564483642578\n",
      "190 [' the', ' his', 'Mary'] cosine last tok: 0.6711467504501343\n",
      "191 [' Mary', 'Mary', ' the'] cosine last tok: 0.6793796420097351\n",
      "192 [' Mary', ' the', ' and'] cosine last tok: 0.6661221981048584\n",
      "193 [' the', ' Mary', ' a'] cosine last tok: 0.6419340968132019\n",
      "194 [' Mary', ' shop', ' The'] cosine last tok: 0.6862399578094482\n",
      "195 [' Mary', 'Mary', ' the'] cosine last tok: 0.6771193742752075\n",
      "196 [' Mary', ' his', ' a'] cosine last tok: 0.6665096282958984\n",
      "197 [' Mary', ' the', ' his'] cosine last tok: 0.6643768548965454\n",
      "198 [' Mary', ' John', ' mary'] cosine last tok: 0.6513047814369202\n",
      "199 [' the', ' his', ' Mary'] cosine last tok: 0.6230074167251587\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad\n",
    "def randomize_embedding_multiple_toks(target_emb, src_emb, token_idxs, pct: float):\n",
    "  for token_idx in token_idxs:\n",
    "    token_emb = src_emb.weight.data[token_idx].clone()\n",
    "    jittering = torch.rand(token_emb.shape) * pct * 2 + 1 - pct\n",
    "    jittering = jittering.to(token_emb.device)\n",
    "    target_emb.weight.data[token_idx] = token_emb * jittering\n",
    "\n",
    "ALL_TOKS = list(set(toks.squeeze().cpu().tolist()))\n",
    "\n",
    "with torch.no_grad():\n",
    "  for idx in range(200):\n",
    "    randomize_embedding_multiple_toks(model.model.embed_tokens, original_model.model.embed_tokens, ALL_TOKS, idx / 100)\n",
    "    out = model(toks)\n",
    "    logits = out.logits\n",
    "\n",
    "    topk = logits[0,-1].topk(3)\n",
    "    out_toks = tokenizer.batch_decode(topk.indices)\n",
    "    cos = F.cosine_similarity(model.model.embed_tokens.weight[LAST_TOKEN_IDX], original_model.model.embed_tokens.weight[LAST_TOKEN_IDX], dim=0)\n",
    "    print(idx, out_toks, 'cosine last tok:', cos.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
