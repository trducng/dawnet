{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d5d789d-7e6e-4bda-9936-0be0c1af7488",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x3931e4260>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from dawnet import Inspector, op\n",
    "from dawnet.inspector import LLMInspector\n",
    "\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a82b9316-dc91-4ffb-a716-5ef2eb4818b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n",
      "W0919 04:21:05.556000 94569 torch/distributed/elastic/multiprocessing/redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "036e01c11d284b63be8da4bedd49c30c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get the data\n",
    "model_name = \"Qwen/Qwen3-4B-Base\"\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"mps\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "insp = LLMInspector.from_hf(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d5d66ea-04b8-41cd-8c64-57010730bfc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen3ForCausalLM(\n",
      "  (model): Qwen3Model(\n",
      "    (embed_tokens): Embedding(151936, 2560)\n",
      "    (layers): ModuleList(\n",
      "      (0-35): 36 x Qwen3DecoderLayer(\n",
      "        (self_attn): Qwen3Attention(\n",
      "          (q_proj): Linear(in_features=2560, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=2560, bias=False)\n",
      "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "        )\n",
      "        (mlp): Qwen3MLP(\n",
      "          (gate_proj): Linear(in_features=2560, out_features=9728, bias=False)\n",
      "          (up_proj): Linear(in_features=2560, out_features=9728, bias=False)\n",
      "          (down_proj): Linear(in_features=9728, out_features=2560, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
      "    (rotary_emb): Qwen3RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2560, out_features=151936, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(insp.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19329105-4f7e-4238-8b9e-584e85b78b81",
   "metadata": {},
   "source": [
    "### Disable modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8b21d0c-e850-4bcc-9f04-0bbb18cae040",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dawnet.inspector import Op\n",
    "\n",
    "class ForwardOutput(Op):\n",
    "    def __init__(self, default_source: str | None = None):\n",
    "        super().__init__()\n",
    "        self._default_source = default_source\n",
    "        self._store = None\n",
    "        self._aux_hook = None\n",
    "\n",
    "    def inspector_pre_run(self, insp, run_params):\n",
    "        source = self._default_source\n",
    "        if isinstance(run_params, dict) and \"source\" in run_params:\n",
    "            source = run_params[\"source\"]\n",
    "        if not source:\n",
    "            return\n",
    "\n",
    "        def get_output(module, args, output):\n",
    "            self._store = output\n",
    "            return output\n",
    "        module = insp._model.get_submodule(source)\n",
    "        self._aux_hook = module.register_forward_hook(get_output)\n",
    "\n",
    "    def inspector_post_run(self, insp, run_params):\n",
    "        self._store = None\n",
    "        if self._aux_hook is not None:\n",
    "            self._aux_hook.remove()\n",
    "        self._aux_hook = None\n",
    "    \n",
    "    def forward(self, insp, name, module, args, kwargs, output):\n",
    "        if self._store is not None:\n",
    "            return self._store\n",
    "        return output\n",
    "        \n",
    "    def run_params(self, source: str | None = None, *args, **kwargs):\n",
    "        return super().run_params(source=source)\n",
    "\n",
    "    def __str__(self):\n",
    "        if self._default_source:\n",
    "            return f\"ForwardOutput(default_source={self._default_source})\"\n",
    "        return \"ForwardOutput\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6cd87bcf-6329-4f6e-a031-2fb5cba06ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added to layer ['model.layers.21']\n",
      "Added to layer ['model.layers.0', 'model.layers.1', 'model.layers.2', 'model.layers.3', 'model.layers.4', 'model.layers.5', 'model.layers.6', 'model.layers.7', 'model.layers.8', 'model.layers.9', 'model.layers.10', 'model.layers.11', 'model.layers.12', 'model.layers.13', 'model.layers.14', 'model.layers.15', 'model.layers.16', 'model.layers.17', 'model.layers.18', 'model.layers.19', 'model.layers.20', 'model.layers.21', 'model.layers.22', 'model.layers.23', 'model.layers.24', 'model.layers.25', 'model.layers.26', 'model.layers.27', 'model.layers.28', 'model.layers.29', 'model.layers.30', 'model.layers.31', 'model.layers.32', 'model.layers.33', 'model.layers.34', 'model.layers.35']\n"
     ]
    }
   ],
   "source": [
    "# check that the input to the next layer should be the output of source layer\n",
    "in_check = insp.add(op.GetInput(), name=f\"model.layers.{LAYER_IDX+1}\")\n",
    "out_check = insp.add(op.GetOutput(), name_regex=r\"^model.layers.\\d+$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b09a115-cee1-4449-ab4e-89484afd7043",
   "metadata": {},
   "source": [
    "#### Check how the middle layers affect computation\n",
    "\n",
    "We measure the cosine similarity between:\n",
    "- the output of layer T+1 when the model runs normally\n",
    "- the output of layer T+1 when the layers T-a to layers T are skipped (input to T+1 is the output of T-a-1).\n",
    "\n",
    "The lower the cosine similarity, the larger the role that the skipped layers contribute to the token refinement of layer T+1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1d6e8f-722d-4880-a209-a8a09a69a510",
   "metadata": {},
   "source": [
    "Get all intermediate outputs when the model run normally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b7a4bc3-4229-411a-aa31-aac0e7cc10a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out['logits'].shape=torch.Size([1, 16, 151936])\n",
      "out['logits'][0,-1].argmax().cpu().item()=6898\n",
      "dict_keys(['model.layers.0', 'model.layers.1', 'model.layers.2', 'model.layers.3', 'model.layers.4', 'model.layers.5', 'model.layers.6', 'model.layers.7', 'model.layers.8', 'model.layers.9', 'model.layers.10', 'model.layers.11', 'model.layers.12', 'model.layers.13', 'model.layers.14', 'model.layers.15', 'model.layers.16', 'model.layers.17', 'model.layers.18', 'model.layers.19', 'model.layers.20', 'model.layers.21', 'model.layers.22', 'model.layers.23', 'model.layers.24', 'model.layers.25', 'model.layers.26', 'model.layers.27', 'model.layers.28', 'model.layers.29', 'model.layers.30', 'model.layers.31', 'model.layers.32', 'model.layers.33', 'model.layers.34', 'model.layers.35'])\n"
     ]
    }
   ],
   "source": [
    "text = \"Then, Peter and Paul went to the meeting room. Peter gave a key to\"\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(\"mps\")\n",
    "\n",
    "with insp.ctx(detach_state=True) as state:\n",
    "    out = insp(input_ids)\n",
    "    print(f\"{out['logits'].shape=}\")\n",
    "    print(f\"{out['logits'][0,-1].argmax().cpu().item()=}\")\n",
    "ground_truth = state['output']\n",
    "print(ground_truth.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca9e10c5-7319-45cc-9bfe-7d593efe3f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_ctrl = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "270efd61-97a0-48b8-8628-6c28b6e8a4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added to layer ['model.layers.20']\n",
      "Inspector ops:\n",
      "- [0] ForwardOutput @ ['model.layers.20']\n"
     ]
    }
   ],
   "source": [
    "LAYER_IDX = 20\n",
    "LAYER_NAME = f\"model.layers.{LAYER_IDX}\"\n",
    "\n",
    "if forward_ctrl is None:\n",
    "    forward_ctrl = ForwardOutput()\n",
    "\n",
    "if insp.has_op(forward_ctrl):\n",
    "    insp.move(forward_ctrl, name=LAYER_NAME)\n",
    "else:\n",
    "    insp.add(forward_ctrl, name=LAYER_NAME)\n",
    "print(insp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "99b357f3-bd1f-473c-99f3-f37c0d47a4c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 0: \",\" - 0.32994. model.layers.21 diff: 0.091891\n",
      "From 1: \" I\" - 0.09414. model.layers.21 diff: 0.092185\n",
      "From 2: \" I\" - 0.08335. model.layers.21 diff: 0.144992\n",
      "From 3: \"\n",
      "\" - 0.0555. model.layers.21 diff: 0.167614\n",
      "From 4: \" provide\" - 0.07146. model.layers.21 diff: 0.210645\n",
      "From 5: \" )\" - 0.06471. model.layers.21 diff: 0.253155\n",
      "From 6: \" him\" - 0.14853. model.layers.21 diff: 0.272081\n",
      "From 7: \" and\" - 0.3006. model.layers.21 diff: 0.335728\n",
      "From 8: \" the\" - 0.2118. model.layers.21 diff: 0.382511\n",
      "From 9: \" the\" - 0.13071. model.layers.21 diff: 0.447463\n",
      "From 10: \" the\" - 0.30769. model.layers.21 diff: 0.508285\n",
      "From 11: \" the\" - 0.22373. model.layers.21 diff: 0.546757\n",
      "From 12: \" Paul\" - 0.23375. model.layers.21 diff: 0.642282\n",
      "From 13: \" Paul\" - 0.62853. model.layers.21 diff: 0.68766\n",
      "From 14: \" Paul\" - 0.36959. model.layers.21 diff: 0.762776\n",
      "From 15: \" Paul\" - 0.33722. model.layers.21 diff: 0.79469\n",
      "From 16: \" Paul\" - 0.46707. model.layers.21 diff: 0.841427\n",
      "From 17: \" Paul\" - 0.55825. model.layers.21 diff: 0.868155\n",
      "From 18: \" Paul\" - 0.40549. model.layers.21 diff: 0.89537\n",
      "From 19: \" Paul\" - 0.27785. model.layers.21 diff: 0.945071\n",
      "From 20: \" Paul\" - 0.6969. model.layers.21 diff: 1.0\n"
     ]
    }
   ],
   "source": [
    "text = \"Then, Peter and Paul went to the meeting room. Peter gave a key to\"\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(\"mps\")\n",
    "\n",
    "for idx_ in range(0, LAYER_IDX+1):\n",
    "    with insp.ctx([forward_ctrl.run_params(source=f\"model.layers.{idx_}\")]) as state:\n",
    "        out_ = insp(input_ids)\n",
    "        prob_dist_ = out_['logits'][0,-1].softmax(dim=0)\n",
    "        tok_ = prob_dist_.argmax().cpu().item()\n",
    "        prob_ = round(prob_dist_.max().cpu().item(), 5)\n",
    "\n",
    "        next_layer = f\"model.layers.{LAYER_IDX+1}\"\n",
    "        if next_layer in state[\"output\"]:\n",
    "            cos = F.cosine_similarity(\n",
    "                ground_truth[next_layer][0,-1],\n",
    "                state[\"output\"][next_layer][0,-1],\n",
    "                dim=0,\n",
    "            ).item()\n",
    "            print(f'From {idx_}: \"{tokenizer.decode(tok_)}\" - {prob_}. {next_layer} diff: {round(cos, 6)}')\n",
    "        else:\n",
    "            print(f'From {idx_}: \"{tokenizer.decode(tok_)}\" - {prob_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6baa9a5d-e418-4671-97d2-39059a65fe9f",
   "metadata": {},
   "source": [
    "### Repeatedly run a module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a6f2c005-3f76-4c52-a452-d03f55755f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "insp.remove_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4bea7771-ca12-46d3-959c-f4057f83789a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(logits: torch.Tensor, input_ids: torch.Tensor):\n",
    "    logits = logits[:-1]\n",
    "    probs = logits.softmax(dim=-1)\n",
    "    target = input_ids[1:]\n",
    "    \n",
    "    probs_ = probs[range(probs.shape[0]),target]\n",
    "    nll = -torch.log(probs_)\n",
    "    total = nll.sum()\n",
    "    avg = total / target.shape[0]\n",
    "    return torch.exp(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "6ee1d817-c6e6-4c9b-b502-29824ff24a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" her\" - prob 0.37022 - ppl 172.5475\n"
     ]
    }
   ],
   "source": [
    "insp = Inspector(model)\n",
    "text = \"After a surprise announcement of its impending release in September, Alicia debuted\"\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(\"mps\")\n",
    "\n",
    "with insp.ctx(detach_state=True) as state:\n",
    "    out_ = insp(input_ids)\n",
    "    prob_dist_ = out_['logits'][0,-1].softmax(dim=0)\n",
    "    tok_ = prob_dist_.argmax().cpu().item()\n",
    "    prob_ = round(prob_dist_.max().cpu().item(), 5)\n",
    "    ppl = perplexity(out_['logits'][0], input_ids[0])\n",
    "    print(f'\"{tokenizer.decode(tok_)}\" - prob {prob_} - ppl {round(ppl.item(), 4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "7a694321-49ba-42aa-9c42-a89787568162",
   "metadata": {},
   "outputs": [],
   "source": [
    "insp = Inspector(model)\n",
    "for idx_ in range(31, 33):\n",
    "    insp._model.model.layers._modules[str(idx_)] = insp._original_model.model.layers[33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d675678c-09e5-49fb-8399-3160dd75ba0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" her\" - prob 0.24983 - ppl 167.3132\n"
     ]
    }
   ],
   "source": [
    "text = \"After a surprise announcement of its impending release in September, Alicia debuted\"\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(\"mps\")\n",
    "\n",
    "with insp.ctx(detach_state=True) as state:\n",
    "    out_ = insp(input_ids)\n",
    "    prob_dist_ = out_['logits'][0,-1].softmax(dim=0)\n",
    "    tok_ = prob_dist_.argmax().cpu().item()\n",
    "    prob_ = round(prob_dist_.max().cpu().item(), 5)\n",
    "    ppl = perplexity(out_['logits'][0], input_ids[0])\n",
    "    print(f'\"{tokenizer.decode(tok_)}\" - prob {prob_} - ppl {round(ppl.item(), 4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9debea-d223-4663-a561-47c4ea487bce",
   "metadata": {},
   "source": [
    "Depend on the model, removing and then tying the weights will not hurt performance in some layers than in some other layers.\n",
    "\n",
    "Here, for Qwen3-4B, layer 31, 32, 33 can be tied and use the same weights of layer 33, with some minimal increases in perplexities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "5a93a160-b167-4f4d-9a14-a32ba23a4b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "insp = Inspector(model)\n",
    "text = \"After a surprise announcement of its impending release in September, Alicia debuted\"\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(\"mps\")\n",
    "\n",
    "\n",
    "with insp.ctx(detach_state=True) as state:\n",
    "    out_ = insp.model.generate(input_ids, max_new_tokens=128, do_sample=True, top_k=20, temperature=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a9f3b2c0-ec4f-4615-af2b-e40e86fb8db9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(out_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "efb4cd91-f3d9-476d-86d2-ce92d63b106c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'After a surprise announcement of its impending release in September, Alicia debuted her new single, “I’m Not Okay,” on the radio on Friday. The song is the first single from her upcoming album, “The Fear,” which is set to be released on September 28.\\n“I’m Not Okay” is a song about the fear of being alone. Alicia said that she wrote the song after she was in a relationship for a long time and was afraid of being alone. She said that she was afraid of being alone because she was afraid of being alone because she was afraid of being alone because she was afraid of being alone because she was afraid of being alone because she was afraid of being alone because she'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(out_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "a432a05b-8a0e-4703-b8e9-2f7fb2c64341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'After a surprise announcement of its impending release in September, Alicia debuted with her first single, \"Don’t Tell Me\", in late July, which went on to become a viral hit and peaked at number one on the Billboard Hot R&B/Hip-Hop Songs and the Billboard Rhythmic Top 40 charts. Following the single, she released her second song of 2021, \"All Up In Your Mind\", which debuted at number 42 on the Billboard Hot 100, her first Billboard Hot 100 entry. Alicia released her third single of the year, \"I’m a Mess\" on August 13, 2021.<|endoftext|>'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(out_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "143db494-56e8-49f8-aa15-41016531e90a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'After a surprise announcement of its impending release in September, Alicia debuted a new single, “Breathe” alongside a music video filmed in her hometown of Miami Beach. In the music video, Alicia appears to be in a mental breakdown while surrounded by family and friends. The video’s lyrics are reminiscent of the “bad” parts of her life, such as her drug use and relationship struggles.\\n“Breathe” has received mixed reviews by critics. The song was met with praise for Alicia’s vocal range, which was said to be on par with Beyonce’s. The song was criticized for its production elements, which were described as “robotic” and “out of place.”\\nDespite the mixed'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(out_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403e1e79-6e62-4e9d-8174-92f50aa706e0",
   "metadata": {},
   "source": [
    "### Randomly jittering the weights.\n",
    "\n",
    "The model still makes sense. Though it seems either perturbing the attention weights or the mlp weights are better than perturbing both of them at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "f03a4208-eecb-4c77-a0bf-09363760d2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rand(params, perturb_range):\n",
    "    return nn.Parameter((torch.rand_like(params) / (1/(perturb_range*2)) + (1-perturb_range)) * params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "99d93a0e-47ab-429d-9077-cb1c9a0c3e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LAYER_IDX = 20\n",
    "RANGE = 0.5\n",
    "# insp = Inspector(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "af29223d-01a0-4883-a5c9-dddcf6012cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx_ in range(len(insp.model.model.layers)):\n",
    "    # mlp\n",
    "    # insp.model.model.layers[idx_].mlp.gate_proj.weight = get_rand(insp.original_model.model.layers[idx_].mlp.gate_proj.weight, RANGE)\n",
    "    # insp.model.model.layers[idx_].mlp.up_proj.weight = get_rand(insp.original_model.model.layers[idx_].mlp.up_proj.weight, RANGE)\n",
    "    # insp.model.model.layers[idx_].mlp.down_proj.weight = get_rand(insp.original_model.model.layers[idx_].mlp.down_proj.weight, RANGE)\n",
    "\n",
    "    # attn\n",
    "    insp.model.model.layers[idx_].self_attn.q_proj.weight = get_rand(insp.original_model.model.layers[idx_].self_attn.q_proj.weight, RANGE)\n",
    "    insp.model.model.layers[idx_].self_attn.k_proj.weight = get_rand(insp.original_model.model.layers[idx_].self_attn.k_proj.weight, RANGE)\n",
    "    insp.model.model.layers[idx_].self_attn.v_proj.weight = get_rand(insp.original_model.model.layers[idx_].self_attn.v_proj.weight, RANGE)\n",
    "    insp.model.model.layers[idx_].self_attn.o_proj.weight = get_rand(insp.original_model.model.layers[idx_].self_attn.o_proj.weight, RANGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "5f079777-242d-4423-a069-e1694e3ecc88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Original:\n",
      "Then, Peter and Paul went to the meeting room. Peter gave a key to Paul. Paul opened the door. They went in. They saw a big table. There were some chairs around it. They sat down. Then Paul said, \"Hello, everyone! My name is Paul. This is my friend Peter. We are from America. We are in Grade Seven.\" The old man said, \"Welcome to our school.\" The woman said, \"Don't be afraid. This is a home for you.\" The old man said, \"Our school begins at eight o'clock. We have four classes in the morning and two in the afternoon. We go to school from Monday to Friday.\" The woman said, \"\n",
      "\n",
      "==== Perturbed:\n",
      "Then, Peter and Paul went to the meeting room. Peter gave a key to the meeting room.  The meeting room was a place where people could meet.  The meeting room was a place where people could meet.  The meeting room was a place where people could meet.  The meeting room was a place where people could meet.  The meeting room was a place where people could meet.  The meeting room was a place where people could meet.  The meeting room was a place where people could meet.  The meeting room was a place where people could meet.  The meeting room was a place where people could meet.  The meeting room was a place where people could meet.  The meeting room\n"
     ]
    }
   ],
   "source": [
    "text = \"Then, Peter and Paul went to the meeting room. Peter gave a key to\"\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(\"mps\")\n",
    "\n",
    "with insp.ctx(detach_state=True) as state:\n",
    "    out_original_ = insp.original_model.generate(input_ids, max_new_tokens=128, do_sample=False, top_k=20, temperature=0.9)\n",
    "    out_perturbed_ = insp.model.generate(input_ids, max_new_tokens=128, do_sample=False, top_k=20, temperature=0.9)\n",
    "\n",
    "print(\"==== Original:\")\n",
    "print(tokenizer.decode(out_original_[0]))\n",
    "\n",
    "print()\n",
    "print(\"==== Perturbed:\")\n",
    "print(tokenizer.decode(out_perturbed_[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c71ade-8603-49c1-bacc-b5c239c30770",
   "metadata": {},
   "source": [
    "### Observe Attention sink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876c1083-e262-4443-8e94-f4acc9c68dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dawnet.diagnose.vis_attention import visualize_attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd626b6f-57aa-4d54-b7d6-ce1c79998021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added to layer ['model.layers.0.self_attn', 'model.layers.1.self_attn', 'model.layers.2.self_attn', 'model.layers.3.self_attn', 'model.layers.4.self_attn', 'model.layers.5.self_attn', 'model.layers.6.self_attn', 'model.layers.7.self_attn', 'model.layers.8.self_attn', 'model.layers.9.self_attn', 'model.layers.10.self_attn', 'model.layers.11.self_attn', 'model.layers.12.self_attn', 'model.layers.13.self_attn', 'model.layers.14.self_attn', 'model.layers.15.self_attn', 'model.layers.16.self_attn', 'model.layers.17.self_attn', 'model.layers.18.self_attn', 'model.layers.19.self_attn', 'model.layers.20.self_attn', 'model.layers.21.self_attn', 'model.layers.22.self_attn', 'model.layers.23.self_attn', 'model.layers.24.self_attn', 'model.layers.25.self_attn', 'model.layers.26.self_attn', 'model.layers.27.self_attn', 'model.layers.28.self_attn', 'model.layers.29.self_attn', 'model.layers.30.self_attn', 'model.layers.31.self_attn', 'model.layers.32.self_attn', 'model.layers.33.self_attn', 'model.layers.34.self_attn', 'model.layers.35.self_attn']\n"
     ]
    }
   ],
   "source": [
    "att_out = insp.add(op.GetOutput(), name_regex=r\"^model.layers.\\d+.self_attn$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b626fd5b-3660-4d57-9c67-549f1b0f6084",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(36):\n",
    "    attn_name = f\"model.layers.{idx}.self_attn\"\n",
    "    sub_module = insp.model.get_submodule(attn_name)\n",
    "    sub_module.config._attn_implementation = \"eager\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a79b5e14-d5fb-4ca0-ae9c-b05ca0f61d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Then, Peter and Paul went to the meeting room. Peter gave a key to\"\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(\"mps\")\n",
    "\n",
    "with insp.ctx(detach_state=True) as state:\n",
    "    out_ = insp.model(input_ids)\n",
    "print(\"Decode:\", tokenizer.decode(out_.logits[0,-1].argmax()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f38bae4f-d71e-4e6a-9a4c-2f5c50369a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = torch.concat([state[\"output\"][f\"model.layers.{idx}.self_attn\"][1] for idx in range(36)], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "80d6afcb-f0c5-47cb-9a60-cff38b4cf10e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([428.2633,   9.6577,  10.8587,   4.9099,   7.6044,   4.8746,   4.5726,\n",
       "          4.4721,   5.3418,   4.9443,   8.7411,   3.8677,   5.3450,   4.0271,\n",
       "          2.0422,   2.4775], device='mps:0')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state[\"output\"][\"model.layers.34.self_attn\"][1][0].sum(axis=-2).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6bd7c4fb-4c8c-4f54-a224-c70607f8b3a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04f959db502b442aa2546fa6d47c9b1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='Batch:', max=35), IntSlider(value=0, description='Head:'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = visualize_attention_mask(attn, method=\"interactive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6df6a6-34ab-4743-9f13-2d920b4494cf",
   "metadata": {},
   "source": [
    "### Check for sparsity in MLP layer\n",
    "\n",
    "The Qwen3 mlp layers look like this:\n",
    "\n",
    "```\n",
    "(mlp): Qwen3MLP(\n",
    "  (gate_proj): Linear(in_features=2560, out_features=9728, bias=False)\n",
    "  (up_proj): Linear(in_features=2560, out_features=9728, bias=False)\n",
    "  (down_proj): Linear(in_features=9728, out_features=2560, bias=False)\n",
    "  (act_fn): SiLU()\n",
    ")\n",
    "```\n",
    "\n",
    "It surprisingly looks like a mini SAE. There are the up-projection, then there are gating to zero out values, then there are down projection. Let's check how sparse it is.\n",
    "\n",
    "It is 50%. Just random. Not really sparsed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "535d07ef-3870-4c49-8562-f5e96fc47d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added to layer ['model.layers.0.mlp', 'model.layers.0.mlp.gate_proj', 'model.layers.0.mlp.up_proj', 'model.layers.0.mlp.down_proj', 'model.layers.0.mlp.act_fn', 'model.layers.1.mlp', 'model.layers.1.mlp.gate_proj', 'model.layers.1.mlp.up_proj', 'model.layers.1.mlp.down_proj', 'model.layers.1.mlp.act_fn', 'model.layers.2.mlp', 'model.layers.2.mlp.gate_proj', 'model.layers.2.mlp.up_proj', 'model.layers.2.mlp.down_proj', 'model.layers.2.mlp.act_fn', 'model.layers.3.mlp', 'model.layers.3.mlp.gate_proj', 'model.layers.3.mlp.up_proj', 'model.layers.3.mlp.down_proj', 'model.layers.3.mlp.act_fn', 'model.layers.4.mlp', 'model.layers.4.mlp.gate_proj', 'model.layers.4.mlp.up_proj', 'model.layers.4.mlp.down_proj', 'model.layers.4.mlp.act_fn', 'model.layers.5.mlp', 'model.layers.5.mlp.gate_proj', 'model.layers.5.mlp.up_proj', 'model.layers.5.mlp.down_proj', 'model.layers.5.mlp.act_fn', 'model.layers.6.mlp', 'model.layers.6.mlp.gate_proj', 'model.layers.6.mlp.up_proj', 'model.layers.6.mlp.down_proj', 'model.layers.6.mlp.act_fn', 'model.layers.7.mlp', 'model.layers.7.mlp.gate_proj', 'model.layers.7.mlp.up_proj', 'model.layers.7.mlp.down_proj', 'model.layers.7.mlp.act_fn', 'model.layers.8.mlp', 'model.layers.8.mlp.gate_proj', 'model.layers.8.mlp.up_proj', 'model.layers.8.mlp.down_proj', 'model.layers.8.mlp.act_fn', 'model.layers.9.mlp', 'model.layers.9.mlp.gate_proj', 'model.layers.9.mlp.up_proj', 'model.layers.9.mlp.down_proj', 'model.layers.9.mlp.act_fn', 'model.layers.10.mlp', 'model.layers.10.mlp.gate_proj', 'model.layers.10.mlp.up_proj', 'model.layers.10.mlp.down_proj', 'model.layers.10.mlp.act_fn', 'model.layers.11.mlp', 'model.layers.11.mlp.gate_proj', 'model.layers.11.mlp.up_proj', 'model.layers.11.mlp.down_proj', 'model.layers.11.mlp.act_fn', 'model.layers.12.mlp', 'model.layers.12.mlp.gate_proj', 'model.layers.12.mlp.up_proj', 'model.layers.12.mlp.down_proj', 'model.layers.12.mlp.act_fn', 'model.layers.13.mlp', 'model.layers.13.mlp.gate_proj', 'model.layers.13.mlp.up_proj', 'model.layers.13.mlp.down_proj', 'model.layers.13.mlp.act_fn', 'model.layers.14.mlp', 'model.layers.14.mlp.gate_proj', 'model.layers.14.mlp.up_proj', 'model.layers.14.mlp.down_proj', 'model.layers.14.mlp.act_fn', 'model.layers.15.mlp', 'model.layers.15.mlp.gate_proj', 'model.layers.15.mlp.up_proj', 'model.layers.15.mlp.down_proj', 'model.layers.15.mlp.act_fn', 'model.layers.16.mlp', 'model.layers.16.mlp.gate_proj', 'model.layers.16.mlp.up_proj', 'model.layers.16.mlp.down_proj', 'model.layers.16.mlp.act_fn', 'model.layers.17.mlp', 'model.layers.17.mlp.gate_proj', 'model.layers.17.mlp.up_proj', 'model.layers.17.mlp.down_proj', 'model.layers.17.mlp.act_fn', 'model.layers.18.mlp', 'model.layers.18.mlp.gate_proj', 'model.layers.18.mlp.up_proj', 'model.layers.18.mlp.down_proj', 'model.layers.18.mlp.act_fn', 'model.layers.19.mlp', 'model.layers.19.mlp.gate_proj', 'model.layers.19.mlp.up_proj', 'model.layers.19.mlp.down_proj', 'model.layers.19.mlp.act_fn', 'model.layers.20.mlp', 'model.layers.20.mlp.gate_proj', 'model.layers.20.mlp.up_proj', 'model.layers.20.mlp.down_proj', 'model.layers.20.mlp.act_fn', 'model.layers.21.mlp', 'model.layers.21.mlp.gate_proj', 'model.layers.21.mlp.up_proj', 'model.layers.21.mlp.down_proj', 'model.layers.21.mlp.act_fn', 'model.layers.22.mlp', 'model.layers.22.mlp.gate_proj', 'model.layers.22.mlp.up_proj', 'model.layers.22.mlp.down_proj', 'model.layers.22.mlp.act_fn', 'model.layers.23.mlp', 'model.layers.23.mlp.gate_proj', 'model.layers.23.mlp.up_proj', 'model.layers.23.mlp.down_proj', 'model.layers.23.mlp.act_fn', 'model.layers.24.mlp', 'model.layers.24.mlp.gate_proj', 'model.layers.24.mlp.up_proj', 'model.layers.24.mlp.down_proj', 'model.layers.24.mlp.act_fn', 'model.layers.25.mlp', 'model.layers.25.mlp.gate_proj', 'model.layers.25.mlp.up_proj', 'model.layers.25.mlp.down_proj', 'model.layers.25.mlp.act_fn', 'model.layers.26.mlp', 'model.layers.26.mlp.gate_proj', 'model.layers.26.mlp.up_proj', 'model.layers.26.mlp.down_proj', 'model.layers.26.mlp.act_fn', 'model.layers.27.mlp', 'model.layers.27.mlp.gate_proj', 'model.layers.27.mlp.up_proj', 'model.layers.27.mlp.down_proj', 'model.layers.27.mlp.act_fn', 'model.layers.28.mlp', 'model.layers.28.mlp.gate_proj', 'model.layers.28.mlp.up_proj', 'model.layers.28.mlp.down_proj', 'model.layers.28.mlp.act_fn', 'model.layers.29.mlp', 'model.layers.29.mlp.gate_proj', 'model.layers.29.mlp.up_proj', 'model.layers.29.mlp.down_proj', 'model.layers.29.mlp.act_fn', 'model.layers.30.mlp', 'model.layers.30.mlp.gate_proj', 'model.layers.30.mlp.up_proj', 'model.layers.30.mlp.down_proj', 'model.layers.30.mlp.act_fn', 'model.layers.31.mlp', 'model.layers.31.mlp.gate_proj', 'model.layers.31.mlp.up_proj', 'model.layers.31.mlp.down_proj', 'model.layers.31.mlp.act_fn', 'model.layers.32.mlp', 'model.layers.32.mlp.gate_proj', 'model.layers.32.mlp.up_proj', 'model.layers.32.mlp.down_proj', 'model.layers.32.mlp.act_fn', 'model.layers.33.mlp', 'model.layers.33.mlp.gate_proj', 'model.layers.33.mlp.up_proj', 'model.layers.33.mlp.down_proj', 'model.layers.33.mlp.act_fn', 'model.layers.34.mlp', 'model.layers.34.mlp.gate_proj', 'model.layers.34.mlp.up_proj', 'model.layers.34.mlp.down_proj', 'model.layers.34.mlp.act_fn', 'model.layers.35.mlp', 'model.layers.35.mlp.gate_proj', 'model.layers.35.mlp.up_proj', 'model.layers.35.mlp.down_proj', 'model.layers.35.mlp.act_fn']\n"
     ]
    }
   ],
   "source": [
    "mlp_inout_op = insp.add(op.GetInputOutput(), name_regex=r\"^model.layers.\\d+.mlp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19846872-075d-470f-9135-01ed98c7eb6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decode:  Paul\n"
     ]
    }
   ],
   "source": [
    "text = \"Then, Peter and Paul went to the meeting room. Peter gave a key to\"\n",
    "input_ids = insp.tokenizer.encode(text, return_tensors=\"pt\").to(\"mps\")\n",
    "feats = {}\n",
    "with insp.ctx(detach_state=True) as state:\n",
    "    out_ = insp.model(input_ids)\n",
    "    if \"qwen\" in model_name.lower():\n",
    "        \"\"\"This is qwen's forward code of mlp\n",
    "        def forward(self, x):\n",
    "            down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
    "            return down_proj\n",
    "        \"\"\"\n",
    "        for idx_ in range(36):\n",
    "            act_fn = state[\"output\"][f\"model.layers.{idx_}.mlp.act_fn\"]\n",
    "            up_proj = state[\"output\"][f\"model.layers.{idx_}.mlp.up_proj\"]\n",
    "            feats[idx_] = act_fn * up_proj\n",
    "\n",
    "print(\"Decode:\", insp.tokenizer.decode(out_.logits[0,-1].argmax()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d40a0dd-86e7-4971-96b4-b79a0d690335",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sparsity(tensor):\n",
    "    \"\"\"Get the sparsity: >0 / total\"\"\"\n",
    "    active = (tensor > 0).sum().item()\n",
    "    total = tensor.numel()\n",
    "    return round(active / total, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "780a547d-d269-42fc-bf3e-bf0110cdebc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0: 0.49985\n",
      "Layer 1: 0.50084\n",
      "Layer 2: 0.49906\n",
      "Layer 3: 0.49588\n",
      "Layer 4: 0.49722\n",
      "Layer 5: 0.49643\n",
      "Layer 6: 0.49854\n",
      "Layer 7: 0.50098\n",
      "Layer 8: 0.49952\n",
      "Layer 9: 0.49955\n",
      "Layer 10: 0.50283\n",
      "Layer 11: 0.49842\n",
      "Layer 12: 0.50145\n",
      "Layer 13: 0.49749\n",
      "Layer 14: 0.5034\n",
      "Layer 15: 0.50065\n",
      "Layer 16: 0.49992\n",
      "Layer 17: 0.49738\n",
      "Layer 18: 0.49611\n",
      "Layer 19: 0.50118\n",
      "Layer 20: 0.49846\n",
      "Layer 21: 0.50222\n",
      "Layer 22: 0.49969\n",
      "Layer 23: 0.49967\n",
      "Layer 24: 0.50046\n",
      "Layer 25: 0.50075\n",
      "Layer 26: 0.50312\n",
      "Layer 27: 0.50093\n",
      "Layer 28: 0.50013\n",
      "Layer 29: 0.4978\n",
      "Layer 30: 0.50143\n",
      "Layer 31: 0.50209\n",
      "Layer 32: 0.49818\n",
      "Layer 33: 0.49881\n",
      "Layer 34: 0.49853\n",
      "Layer 35: 0.50285\n"
     ]
    }
   ],
   "source": [
    "for _l, _t in feats.items():\n",
    "    print(f\"Layer {_l}:\", get_sparsity(_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72de8df0-a9ab-4666-9a06-d9368818ad16",
   "metadata": {},
   "source": [
    "### Compare weights between different fine-tuning of the same model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "e7f26cb3-ba12-4d46-be07-a3df13fd158f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0894acb3d1c34190a6f2110885fa9f5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3047896305164a379974b9cc1c13c752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d52b96d280634edea8b21fb6f4945755",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dd32be058bb4fb9ad11a6971591136a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/3.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f094f489befc45d2a844704489ba15c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/99.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9608180a2464ec4a02fea7173b78840",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/3.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bacabc0588cb4acf90e48be2abf7dff4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf2fca86e5ce40fc8972043ca593e1d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4a7144a88204a179db8b77dd0ac24a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a9dfd49476d44fab50206d6c4d55062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31d630f0420a480dab690026d1ea0236",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97c684223ada41c8ad2fad0c0fc4f7d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "insp4 = LLMInspector.from_hf(\"Qwen/Qwen3-4B-Thinking-2507\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05d59b13-4d87-4689-957b-d89f12592500",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n",
      "W0920 16:44:00.123000 19528 torch/distributed/elastic/multiprocessing/redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79ce75e3ae124382b873ed597a01dbcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eacb1050eeb45d8bf4e4c1f5a08225d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/726 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b59460ba6134c3098eeed1fc0a0762d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ac2b8e17e6f417f9f0a379c3157ce53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc15860b60bb4c0f8189d79707af5560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/99.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cceb93854b0d47cba5a65a6447ecfe45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/3.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2684c8c44635414781b3b81906f948ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/3.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51ba1693c3df40eb856906c0e5401e55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "582b55cc4394480bb4423c3ac29899db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f7f7fd9c7ce4eed87309d9d33057533",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3994298914d4aa6a01851a4fa183887",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8ff6f0cbe924a23b3ccefc64511d72e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9486b88abdb462f89271f2594565024",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "insp1 = LLMInspector.from_hf(\"Qwen/Qwen3-4B-Base\")\n",
    "insp2 = LLMInspector.from_hf(\"Qwen/Qwen3-4B\")    # instruction-following\n",
    "\n",
    "# insp3 = LLMInspector.from_hf(\"Qwen/Qwen3-4B-Instruct-2507\")\n",
    "# insp4 = LLMInspector.from_hf(\"Qwen/Qwen3-4B-Thinking-2507\")\n",
    "# insp2 = LLMInspector.from_hf(\"Qwen/Qwen3-4B-FP8\")\n",
    "# insp2 = LLMInspector.from_hf(\"Qwen/Qwen3-4B-AWQ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "053a92ca-c4f5-4c19-a8a9-691f30448ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight                          0.11506  0.20427  0.09367  \n",
      "model.layers.0.self_attn.q_proj.weight             0.07608  0.16577  0.07186  \n",
      "model.layers.0.self_attn.k_proj.weight             0.0691   0.15012  0.06577  \n",
      "model.layers.0.self_attn.v_proj.weight             0.06518  0.13826  0.06266  \n",
      "model.layers.0.self_attn.o_proj.weight             0.07732  0.15722  0.07252  \n",
      "model.layers.0.self_attn.q_norm.weight             0.00199  0.00848  0.00093  \n",
      "model.layers.0.self_attn.k_norm.weight             0.00072  0.00329  0.00019  \n",
      "model.layers.0.mlp.gate_proj.weight                0.07542  0.16225  0.07065  \n",
      "model.layers.0.mlp.up_proj.weight                  0.08073  0.17259  0.0756   \n",
      "model.layers.0.mlp.down_proj.weight                0.07791  0.16118  0.07187  \n",
      "model.layers.0.input_layernorm.weight              0.02811  0.08619  0.03078  \n",
      "model.layers.0.post_attention_layernorm.weight     0.00731  0.01378  0.00759  \n",
      "model.layers.1.self_attn.q_proj.weight             0.08121  0.18377  0.07792  \n",
      "model.layers.1.self_attn.k_proj.weight             0.07267  0.16705  0.07074  \n",
      "model.layers.1.self_attn.v_proj.weight             0.06953  0.15188  0.06642  \n",
      "model.layers.1.self_attn.o_proj.weight             0.07691  0.1616   0.07218  \n",
      "model.layers.1.self_attn.q_norm.weight             0.00127  0.00296  0.00079  \n",
      "model.layers.1.self_attn.k_norm.weight             0.00081  0.00389  0.00054  \n",
      "model.layers.1.mlp.gate_proj.weight                0.05594  0.12046  0.0541   \n",
      "model.layers.1.mlp.up_proj.weight                  0.07861  0.16272  0.07681  \n",
      "model.layers.1.mlp.down_proj.weight                0.0842   0.16905  0.07762  \n",
      "model.layers.1.input_layernorm.weight              0.02968  0.04791  0.03065  \n",
      "model.layers.1.post_attention_layernorm.weight     0.00271  0.00706  0.00213  \n",
      "model.layers.2.self_attn.q_proj.weight             0.08353  0.18312  0.07844  \n",
      "model.layers.2.self_attn.k_proj.weight             0.07627  0.16906  0.07267  \n",
      "model.layers.2.self_attn.v_proj.weight             0.06921  0.14942  0.06499  \n",
      "model.layers.2.self_attn.o_proj.weight             0.07718  0.16401  0.07173  \n",
      "model.layers.2.self_attn.q_norm.weight             0.00205  0.0027   0.001    \n",
      "model.layers.2.self_attn.k_norm.weight             0.00216  0.00321  0.00109  \n",
      "model.layers.2.mlp.gate_proj.weight                0.06811  0.14634  0.06485  \n",
      "model.layers.2.mlp.up_proj.weight                  0.06858  0.14207  0.06684  \n",
      "model.layers.2.mlp.down_proj.weight                0.0775   0.15746  0.07249  \n",
      "model.layers.2.input_layernorm.weight              0.01903  0.03285  0.01955  \n",
      "model.layers.2.post_attention_layernorm.weight     0.0023   0.00528  0.00166  \n",
      "model.layers.3.self_attn.q_proj.weight             0.08168  0.18285  0.07747  \n",
      "model.layers.3.self_attn.k_proj.weight             0.07774  0.17514  0.07438  \n",
      "model.layers.3.self_attn.v_proj.weight             0.06791  0.14674  0.06429  \n",
      "model.layers.3.self_attn.o_proj.weight             0.07555  0.16059  0.07046  \n",
      "model.layers.3.self_attn.q_norm.weight             0.00089  0.00278  0.00105  \n",
      "model.layers.3.self_attn.k_norm.weight             0.00073  0.00319  0.00038  \n",
      "model.layers.3.mlp.gate_proj.weight                0.06354  0.13432  0.05937  \n",
      "model.layers.3.mlp.up_proj.weight                  0.08097  0.16447  0.07672  \n",
      "model.layers.3.mlp.down_proj.weight                0.08536  0.16741  0.07835  \n",
      "model.layers.3.input_layernorm.weight              0.01587  0.02782  0.01601  \n",
      "model.layers.3.post_attention_layernorm.weight     0.00266  0.0061   0.00203  \n",
      "model.layers.4.self_attn.q_proj.weight             0.08527  0.18203  0.07913  \n",
      "model.layers.4.self_attn.k_proj.weight             0.08061  0.17283  0.07561  \n",
      "model.layers.4.self_attn.v_proj.weight             0.07064  0.14715  0.06546  \n",
      "model.layers.4.self_attn.o_proj.weight             0.07728  0.15927  0.0712   \n",
      "model.layers.4.self_attn.q_norm.weight             0.0016   0.00388  0.00123  \n",
      "model.layers.4.self_attn.k_norm.weight             0.00176  0.00348  0.00146  \n",
      "model.layers.4.mlp.gate_proj.weight                0.06194  0.12899  0.05654  \n",
      "model.layers.4.mlp.up_proj.weight                  0.08523  0.17157  0.07897  \n",
      "model.layers.4.mlp.down_proj.weight                0.08936  0.17573  0.08143  \n",
      "model.layers.4.input_layernorm.weight              0.01253  0.02332  0.01243  \n",
      "model.layers.4.post_attention_layernorm.weight     0.00352  0.00656  0.00248  \n",
      "model.layers.5.self_attn.q_proj.weight             0.08338  0.17703  0.07686  \n",
      "model.layers.5.self_attn.k_proj.weight             0.08073  0.16929  0.07461  \n",
      "model.layers.5.self_attn.v_proj.weight             0.07008  0.14419  0.0648   \n",
      "model.layers.5.self_attn.o_proj.weight             0.07751  0.15699  0.0706   \n",
      "model.layers.5.self_attn.q_norm.weight             0.001    0.00335  0.00239  \n",
      "model.layers.5.self_attn.k_norm.weight             0.00012  0.00208  0.00035  \n",
      "model.layers.5.mlp.gate_proj.weight                0.06806  0.13976  0.06164  \n",
      "model.layers.5.mlp.up_proj.weight                  0.08431  0.16846  0.07678  \n",
      "model.layers.5.mlp.down_proj.weight                0.08841  0.17375  0.07989  \n",
      "model.layers.5.input_layernorm.weight              0.01178  0.01964  0.01128  \n",
      "model.layers.5.post_attention_layernorm.weight     0.00387  0.00661  0.00317  \n",
      "model.layers.6.self_attn.q_proj.weight             0.08519  0.18052  0.07744  \n",
      "model.layers.6.self_attn.k_proj.weight             0.08291  0.17791  0.07717  \n",
      "model.layers.6.self_attn.v_proj.weight             0.07245  0.14161  0.06657  \n",
      "model.layers.6.self_attn.o_proj.weight             0.07785  0.14943  0.06986  \n",
      "model.layers.6.self_attn.q_norm.weight             0.00061  0.00352  0.00114  \n",
      "model.layers.6.self_attn.k_norm.weight             0.00026  0.00427  0.00091  \n",
      "model.layers.6.mlp.gate_proj.weight                0.07462  0.1516   0.0665   \n",
      "model.layers.6.mlp.up_proj.weight                  0.08592  0.17093  0.07671  \n",
      "model.layers.6.mlp.down_proj.weight                0.08935  0.1738   0.0789   \n",
      "model.layers.6.input_layernorm.weight              0.00902  0.01711  0.00918  \n",
      "model.layers.6.post_attention_layernorm.weight     0.00352  0.00721  0.00273  \n",
      "model.layers.7.self_attn.q_proj.weight             0.08549  0.18111  0.07825  \n",
      "model.layers.7.self_attn.k_proj.weight             0.08174  0.17771  0.07619  \n",
      "model.layers.7.self_attn.v_proj.weight             0.0703   0.14061  0.06295  \n",
      "model.layers.7.self_attn.o_proj.weight             0.08398  0.16484  0.07325  \n",
      "model.layers.7.self_attn.q_norm.weight             0.00039  0.00249  0.00043  \n",
      "model.layers.7.self_attn.k_norm.weight             0.00026  0.0031   0.00026  \n",
      "model.layers.7.mlp.gate_proj.weight                0.07788  0.15655  0.06867  \n",
      "model.layers.7.mlp.up_proj.weight                  0.08708  0.1714   0.0766   \n",
      "model.layers.7.mlp.down_proj.weight                0.09119  0.17495  0.07918  \n",
      "model.layers.7.input_layernorm.weight              0.0071   0.01156  0.00647  \n",
      "model.layers.7.post_attention_layernorm.weight     0.00361  0.00669  0.00276  \n",
      "model.layers.8.self_attn.q_proj.weight             0.09081  0.18573  0.08086  \n",
      "model.layers.8.self_attn.k_proj.weight             0.08513  0.17676  0.07744  \n",
      "model.layers.8.self_attn.v_proj.weight             0.07277  0.13962  0.06364  \n",
      "model.layers.8.self_attn.o_proj.weight             0.08262  0.15535  0.07041  \n",
      "model.layers.8.self_attn.q_norm.weight             0.00226  0.00431  0.00279  \n",
      "model.layers.8.self_attn.k_norm.weight             0.00178  0.00441  0.00152  \n",
      "model.layers.8.mlp.gate_proj.weight                0.08812  0.17117  0.07578  \n",
      "model.layers.8.mlp.up_proj.weight                  0.08871  0.1697   0.07579  \n",
      "model.layers.8.mlp.down_proj.weight                0.09233  0.17141  0.07793  \n",
      "model.layers.8.input_layernorm.weight              0.00592  0.01     0.00507  \n",
      "model.layers.8.post_attention_layernorm.weight     0.00419  0.00603  0.00284  \n",
      "model.layers.9.self_attn.q_proj.weight             0.09853  0.19651  0.08464  \n",
      "model.layers.9.self_attn.k_proj.weight             0.09197  0.18609  0.07988  \n",
      "model.layers.9.self_attn.v_proj.weight             0.07551  0.14315  0.06319  \n",
      "model.layers.9.self_attn.o_proj.weight             0.08858  0.16701  0.07223  \n",
      "model.layers.9.self_attn.q_norm.weight             0.00217  0.00375  0.00124  \n",
      "model.layers.9.self_attn.k_norm.weight             0.00217  0.00412  0.00121  \n",
      "model.layers.9.mlp.gate_proj.weight                0.08492  0.16365  0.07171  \n",
      "model.layers.9.mlp.up_proj.weight                  0.09069  0.16705  0.07632  \n",
      "model.layers.9.mlp.down_proj.weight                0.09523  0.1725   0.07969  \n",
      "model.layers.9.input_layernorm.weight              0.00603  0.00989  0.00492  \n",
      "model.layers.9.post_attention_layernorm.weight     0.00363  0.00537  0.00248  \n",
      "model.layers.10.self_attn.q_proj.weight            0.09346  0.18825  0.08078  \n",
      "model.layers.10.self_attn.k_proj.weight            0.08915  0.17801  0.07733  \n",
      "model.layers.10.self_attn.v_proj.weight            0.07577  0.1351   0.06281  \n",
      "model.layers.10.self_attn.o_proj.weight            0.08209  0.14911  0.06729  \n",
      "model.layers.10.self_attn.q_norm.weight            0.00323  0.0073   0.00324  \n",
      "model.layers.10.self_attn.k_norm.weight            0.00239  0.00618  0.00304  \n",
      "model.layers.10.mlp.gate_proj.weight               0.08947  0.17017  0.07416  \n",
      "model.layers.10.mlp.up_proj.weight                 0.09006  0.16396  0.07416  \n",
      "model.layers.10.mlp.down_proj.weight               0.0942   0.16794  0.07729  \n",
      "model.layers.10.input_layernorm.weight             0.00459  0.00795  0.00359  \n",
      "model.layers.10.post_attention_layernorm.weight    0.00359  0.00514  0.00249  \n",
      "model.layers.11.self_attn.q_proj.weight            0.09984  0.19314  0.08295  \n",
      "model.layers.11.self_attn.k_proj.weight            0.0941   0.18147  0.07879  \n",
      "model.layers.11.self_attn.v_proj.weight            0.07956  0.14241  0.06377  \n",
      "model.layers.11.self_attn.o_proj.weight            0.08753  0.15772  0.06893  \n",
      "model.layers.11.self_attn.q_norm.weight            0.00215  0.00517  0.00216  \n",
      "model.layers.11.self_attn.k_norm.weight            0.00178  0.00419  0.0015   \n",
      "model.layers.11.mlp.gate_proj.weight               0.09508  0.17644  0.07689  \n",
      "model.layers.11.mlp.up_proj.weight                 0.09118  0.16516  0.07343  \n",
      "model.layers.11.mlp.down_proj.weight               0.09668  0.17012  0.07742  \n",
      "model.layers.11.input_layernorm.weight             0.00748  0.0097   0.00628  \n",
      "model.layers.11.post_attention_layernorm.weight    0.00349  0.0051   0.00266  \n",
      "model.layers.12.self_attn.q_proj.weight            0.10427  0.19693  0.08446  \n",
      "model.layers.12.self_attn.k_proj.weight            0.10163  0.18994  0.08208  \n",
      "model.layers.12.self_attn.v_proj.weight            0.07751  0.13546  0.06146  \n",
      "model.layers.12.self_attn.o_proj.weight            0.08505  0.15319  0.06616  \n",
      "model.layers.12.self_attn.q_norm.weight            0.00252  0.00665  0.00271  \n",
      "model.layers.12.self_attn.k_norm.weight            0.00209  0.00571  0.00162  \n",
      "model.layers.12.mlp.gate_proj.weight               0.10096  0.1843   0.08026  \n",
      "model.layers.12.mlp.up_proj.weight                 0.09319  0.1671   0.07354  \n",
      "model.layers.12.mlp.down_proj.weight               0.09826  0.17091  0.07703  \n",
      "model.layers.12.input_layernorm.weight             0.00691  0.00865  0.006    \n",
      "model.layers.12.post_attention_layernorm.weight    0.00349  0.00529  0.00249  \n",
      "model.layers.13.self_attn.q_proj.weight            0.10676  0.19504  0.08476  \n",
      "model.layers.13.self_attn.k_proj.weight            0.10404  0.18812  0.0825   \n",
      "model.layers.13.self_attn.v_proj.weight            0.08384  0.146    0.06387  \n",
      "model.layers.13.self_attn.o_proj.weight            0.09279  0.16079  0.06954  \n",
      "model.layers.13.self_attn.q_norm.weight            0.00191  0.00409  0.00164  \n",
      "model.layers.13.self_attn.k_norm.weight            0.00177  0.00373  0.00155  \n",
      "model.layers.13.mlp.gate_proj.weight               0.1084   0.19347  0.08496  \n",
      "model.layers.13.mlp.up_proj.weight                 0.09687  0.1702   0.07491  \n",
      "model.layers.13.mlp.down_proj.weight               0.10126  0.17326  0.07799  \n",
      "model.layers.13.input_layernorm.weight             0.00921  0.00982  0.00766  \n",
      "model.layers.13.post_attention_layernorm.weight    0.00386  0.00479  0.0027   \n",
      "model.layers.14.self_attn.q_proj.weight            0.11072  0.1968   0.08495  \n",
      "model.layers.14.self_attn.k_proj.weight            0.10811  0.18677  0.08315  \n",
      "model.layers.14.self_attn.v_proj.weight            0.08309  0.13276  0.06294  \n",
      "model.layers.14.self_attn.o_proj.weight            0.08685  0.14149  0.06406  \n",
      "model.layers.14.self_attn.q_norm.weight            0.0026   0.0058   0.00374  \n",
      "model.layers.14.self_attn.k_norm.weight            0.00311  0.00442  0.00307  \n",
      "model.layers.14.mlp.gate_proj.weight               0.1146   0.20199  0.08914  \n",
      "model.layers.14.mlp.up_proj.weight                 0.09904  0.17121  0.07555  \n",
      "model.layers.14.mlp.down_proj.weight               0.1027   0.17326  0.07815  \n",
      "model.layers.14.input_layernorm.weight             0.00556  0.00723  0.00433  \n",
      "model.layers.14.post_attention_layernorm.weight    0.00398  0.00445  0.00269  \n",
      "model.layers.15.self_attn.q_proj.weight            0.10918  0.19171  0.08356  \n",
      "model.layers.15.self_attn.k_proj.weight            0.10955  0.18381  0.08299  \n",
      "model.layers.15.self_attn.v_proj.weight            0.08499  0.13449  0.06316  \n",
      "model.layers.15.self_attn.o_proj.weight            0.08908  0.14582  0.06489  \n",
      "model.layers.15.self_attn.q_norm.weight            0.00222  0.00334  0.00172  \n",
      "model.layers.15.self_attn.k_norm.weight            0.00174  0.00437  0.00129  \n",
      "model.layers.15.mlp.gate_proj.weight               0.12078  0.20942  0.09285  \n",
      "model.layers.15.mlp.up_proj.weight                 0.10446  0.17862  0.07883  \n",
      "model.layers.15.mlp.down_proj.weight               0.10682  0.17887  0.08054  \n",
      "model.layers.15.input_layernorm.weight             0.00582  0.00736  0.00422  \n",
      "model.layers.15.post_attention_layernorm.weight    0.00378  0.00453  0.00265  \n",
      "model.layers.16.self_attn.q_proj.weight            0.12025  0.20831  0.08963  \n",
      "model.layers.16.self_attn.k_proj.weight            0.11628  0.19352  0.08569  \n",
      "model.layers.16.self_attn.v_proj.weight            0.087    0.13478  0.06328  \n",
      "model.layers.16.self_attn.o_proj.weight            0.09486  0.15239  0.06809  \n",
      "model.layers.16.self_attn.q_norm.weight            0.00421  0.00709  0.00368  \n",
      "model.layers.16.self_attn.k_norm.weight            0.00328  0.00578  0.003    \n",
      "model.layers.16.mlp.gate_proj.weight               0.12126  0.21012  0.0929   \n",
      "model.layers.16.mlp.up_proj.weight                 0.10447  0.17867  0.07841  \n",
      "model.layers.16.mlp.down_proj.weight               0.1065   0.17848  0.08034  \n",
      "model.layers.16.input_layernorm.weight             0.00388  0.00565  0.00257  \n",
      "model.layers.16.post_attention_layernorm.weight    0.00368  0.00459  0.00252  \n",
      "model.layers.17.self_attn.q_proj.weight            0.11668  0.20721  0.08807  \n",
      "model.layers.17.self_attn.k_proj.weight            0.12032  0.20037  0.08965  \n",
      "model.layers.17.self_attn.v_proj.weight            0.08702  0.13651  0.06371  \n",
      "model.layers.17.self_attn.o_proj.weight            0.08783  0.14554  0.06409  \n",
      "model.layers.17.self_attn.q_norm.weight            0.00352  0.00513  0.00365  \n",
      "model.layers.17.self_attn.k_norm.weight            0.00328  0.00551  0.00283  \n",
      "model.layers.17.mlp.gate_proj.weight               0.12314  0.21312  0.09414  \n",
      "model.layers.17.mlp.up_proj.weight                 0.10731  0.18343  0.08028  \n",
      "model.layers.17.mlp.down_proj.weight               0.10777  0.18063  0.0811   \n",
      "model.layers.17.input_layernorm.weight             0.00382  0.00574  0.0026   \n",
      "model.layers.17.post_attention_layernorm.weight    0.00369  0.00461  0.00253  \n",
      "model.layers.18.self_attn.q_proj.weight            0.12262  0.20762  0.08922  \n",
      "model.layers.18.self_attn.k_proj.weight            0.12208  0.19661  0.08744  \n",
      "model.layers.18.self_attn.v_proj.weight            0.08931  0.13672  0.06362  \n",
      "model.layers.18.self_attn.o_proj.weight            0.08865  0.14125  0.06224  \n",
      "model.layers.18.self_attn.q_norm.weight            0.00351  0.00492  0.00286  \n",
      "model.layers.18.self_attn.k_norm.weight            0.00303  0.00481  0.00251  \n",
      "model.layers.18.mlp.gate_proj.weight               0.12796  0.21669  0.09637  \n",
      "model.layers.18.mlp.up_proj.weight                 0.11099  0.18586  0.08224  \n",
      "model.layers.18.mlp.down_proj.weight               0.11205  0.18335  0.08391  \n",
      "model.layers.18.input_layernorm.weight             0.00384  0.00681  0.00246  \n",
      "model.layers.18.post_attention_layernorm.weight    0.00338  0.00473  0.00239  \n",
      "model.layers.19.self_attn.q_proj.weight            0.12099  0.20683  0.08884  \n",
      "model.layers.19.self_attn.k_proj.weight            0.12411  0.20027  0.08978  \n",
      "model.layers.19.self_attn.v_proj.weight            0.09172  0.13536  0.06542  \n",
      "model.layers.19.self_attn.o_proj.weight            0.08936  0.14287  0.06341  \n",
      "model.layers.19.self_attn.q_norm.weight            0.00244  0.0044   0.00244  \n",
      "model.layers.19.self_attn.k_norm.weight            0.00198  0.00465  0.00131  \n",
      "model.layers.19.mlp.gate_proj.weight               0.12949  0.21867  0.09783  \n",
      "model.layers.19.mlp.up_proj.weight                 0.11253  0.18753  0.08339  \n",
      "model.layers.19.mlp.down_proj.weight               0.11326  0.18539  0.08501  \n",
      "model.layers.19.input_layernorm.weight             0.00278  0.00476  0.00174  \n",
      "model.layers.19.post_attention_layernorm.weight    0.00359  0.00457  0.00246  \n",
      "model.layers.20.self_attn.q_proj.weight            0.12222  0.20616  0.08798  \n",
      "model.layers.20.self_attn.k_proj.weight            0.13191  0.20758  0.09361  \n",
      "model.layers.20.self_attn.v_proj.weight            0.09479  0.14172  0.06789  \n",
      "model.layers.20.self_attn.o_proj.weight            0.09919  0.15813  0.07008  \n",
      "model.layers.20.self_attn.q_norm.weight            0.00319  0.00561  0.00371  \n",
      "model.layers.20.self_attn.k_norm.weight            0.0028   0.00552  0.00324  \n",
      "model.layers.20.mlp.gate_proj.weight               0.13253  0.22188  0.10047  \n",
      "model.layers.20.mlp.up_proj.weight                 0.11287  0.18663  0.08392  \n",
      "model.layers.20.mlp.down_proj.weight               0.11236  0.18342  0.08478  \n",
      "model.layers.20.input_layernorm.weight             0.00392  0.00513  0.00258  \n",
      "model.layers.20.post_attention_layernorm.weight    0.00372  0.00474  0.00257  \n",
      "model.layers.21.self_attn.q_proj.weight            0.12147  0.20413  0.08842  \n",
      "model.layers.21.self_attn.k_proj.weight            0.13056  0.20445  0.09394  \n",
      "model.layers.21.self_attn.v_proj.weight            0.09387  0.13915  0.06749  \n",
      "model.layers.21.self_attn.o_proj.weight            0.09564  0.15073  0.06791  \n",
      "model.layers.21.self_attn.q_norm.weight            0.00253  0.00453  0.00266  \n",
      "model.layers.21.self_attn.k_norm.weight            0.00196  0.00415  0.00184  \n",
      "model.layers.21.mlp.gate_proj.weight               0.13412  0.22223  0.10241  \n",
      "model.layers.21.mlp.up_proj.weight                 0.11206  0.18368  0.0838   \n",
      "model.layers.21.mlp.down_proj.weight               0.11087  0.17937  0.08449  \n",
      "model.layers.21.input_layernorm.weight             0.00326  0.00502  0.0021   \n",
      "model.layers.21.post_attention_layernorm.weight    0.00361  0.00522  0.00251  \n",
      "model.layers.22.self_attn.q_proj.weight            0.12358  0.20801  0.09227  \n",
      "model.layers.22.self_attn.k_proj.weight            0.13152  0.19915  0.0977   \n",
      "model.layers.22.self_attn.v_proj.weight            0.09511  0.13287  0.06971  \n",
      "model.layers.22.self_attn.o_proj.weight            0.0901   0.14231  0.06681  \n",
      "model.layers.22.self_attn.q_norm.weight            0.00242  0.00418  0.00308  \n",
      "model.layers.22.self_attn.k_norm.weight            0.00192  0.00348  0.00192  \n",
      "model.layers.22.mlp.gate_proj.weight               0.12911  0.21451  0.09962  \n",
      "model.layers.22.mlp.up_proj.weight                 0.10839  0.17906  0.08164  \n",
      "model.layers.22.mlp.down_proj.weight               0.10577  0.17228  0.08094  \n",
      "model.layers.22.input_layernorm.weight             0.00208  0.00349  0.00115  \n",
      "model.layers.22.post_attention_layernorm.weight    0.00351  0.00456  0.00237  \n",
      "model.layers.23.self_attn.q_proj.weight            0.11766  0.19594  0.08761  \n",
      "model.layers.23.self_attn.k_proj.weight            0.12976  0.20327  0.09557  \n",
      "model.layers.23.self_attn.v_proj.weight            0.09134  0.13206  0.06715  \n",
      "model.layers.23.self_attn.o_proj.weight            0.09441  0.14782  0.06879  \n",
      "model.layers.23.self_attn.q_norm.weight            0.00261  0.00468  0.00192  \n",
      "model.layers.23.self_attn.k_norm.weight            0.00206  0.00359  0.00152  \n",
      "model.layers.23.mlp.gate_proj.weight               0.12492  0.20525  0.09665  \n",
      "model.layers.23.mlp.up_proj.weight                 0.10772  0.17846  0.08176  \n",
      "model.layers.23.mlp.down_proj.weight               0.1054   0.17157  0.08102  \n",
      "model.layers.23.input_layernorm.weight             0.00236  0.00395  0.00154  \n",
      "model.layers.23.post_attention_layernorm.weight    0.00323  0.00423  0.00219  \n",
      "model.layers.24.self_attn.q_proj.weight            0.11828  0.19748  0.08918  \n",
      "model.layers.24.self_attn.k_proj.weight            0.12682  0.20344  0.0967   \n",
      "model.layers.24.self_attn.v_proj.weight            0.09552  0.14578  0.07168  \n",
      "model.layers.24.self_attn.o_proj.weight            0.09949  0.15886  0.07439  \n",
      "model.layers.24.self_attn.q_norm.weight            0.0028   0.00476  0.00379  \n",
      "model.layers.24.self_attn.k_norm.weight            0.00254  0.0042   0.0031   \n",
      "model.layers.24.mlp.gate_proj.weight               0.12238  0.20053  0.096    \n",
      "model.layers.24.mlp.up_proj.weight                 0.10564  0.17803  0.08164  \n",
      "model.layers.24.mlp.down_proj.weight               0.10309  0.17055  0.08093  \n",
      "model.layers.24.input_layernorm.weight             0.00189  0.0039   0.00114  \n",
      "model.layers.24.post_attention_layernorm.weight    0.00288  0.00425  0.002    \n",
      "model.layers.25.self_attn.q_proj.weight            0.12022  0.21152  0.09541  \n",
      "model.layers.25.self_attn.k_proj.weight            0.12421  0.20597  0.09912  \n",
      "model.layers.25.self_attn.v_proj.weight            0.09457  0.14871  0.07273  \n",
      "model.layers.25.self_attn.o_proj.weight            0.10112  0.17439  0.07837  \n",
      "model.layers.25.self_attn.q_norm.weight            0.00249  0.00374  0.00209  \n",
      "model.layers.25.self_attn.k_norm.weight            0.00181  0.00331  0.00114  \n",
      "model.layers.25.mlp.gate_proj.weight               0.12059  0.19546  0.09485  \n",
      "model.layers.25.mlp.up_proj.weight                 0.10515  0.17777  0.0817   \n",
      "model.layers.25.mlp.down_proj.weight               0.10379  0.17274  0.08173  \n",
      "model.layers.25.input_layernorm.weight             0.00199  0.00415  0.00111  \n",
      "model.layers.25.post_attention_layernorm.weight    0.00277  0.0042   0.00191  \n",
      "model.layers.26.self_attn.q_proj.weight            0.12725  0.21905  0.09751  \n",
      "model.layers.26.self_attn.k_proj.weight            0.13197  0.21508  0.10069  \n",
      "model.layers.26.self_attn.v_proj.weight            0.09746  0.15636  0.07386  \n",
      "model.layers.26.self_attn.o_proj.weight            0.11078  0.19162  0.08409  \n",
      "model.layers.26.self_attn.q_norm.weight            0.00298  0.00401  0.00238  \n",
      "model.layers.26.self_attn.k_norm.weight            0.00283  0.00366  0.0023   \n",
      "model.layers.26.mlp.gate_proj.weight               0.12049  0.19409  0.09438  \n",
      "model.layers.26.mlp.up_proj.weight                 0.10391  0.17727  0.08108  \n",
      "model.layers.26.mlp.down_proj.weight               0.1031   0.17313  0.08148  \n",
      "model.layers.26.input_layernorm.weight             0.00167  0.00362  0.0008   \n",
      "model.layers.26.post_attention_layernorm.weight    0.00252  0.00413  0.00147  \n",
      "model.layers.27.self_attn.q_proj.weight            0.12714  0.22171  0.10027  \n",
      "model.layers.27.self_attn.k_proj.weight            0.12973  0.21725  0.10501  \n",
      "model.layers.27.self_attn.v_proj.weight            0.09595  0.16043  0.07581  \n",
      "model.layers.27.self_attn.o_proj.weight            0.10756  0.19189  0.08421  \n",
      "model.layers.27.self_attn.q_norm.weight            0.00317  0.00456  0.00298  \n",
      "model.layers.27.self_attn.k_norm.weight            0.0026   0.00476  0.00226  \n",
      "model.layers.27.mlp.gate_proj.weight               0.11958  0.19289  0.09369  \n",
      "model.layers.27.mlp.up_proj.weight                 0.10258  0.17617  0.08038  \n",
      "model.layers.27.mlp.down_proj.weight               0.10254  0.17331  0.08117  \n",
      "model.layers.27.input_layernorm.weight             0.0014   0.00356  0.00044  \n",
      "model.layers.27.post_attention_layernorm.weight    0.00226  0.00408  0.00139  \n",
      "model.layers.28.self_attn.q_proj.weight            0.12057  0.21427  0.09435  \n",
      "model.layers.28.self_attn.k_proj.weight            0.12971  0.2166   0.10236  \n",
      "model.layers.28.self_attn.v_proj.weight            0.09844  0.16102  0.07574  \n",
      "model.layers.28.self_attn.o_proj.weight            0.10465  0.18361  0.08058  \n",
      "model.layers.28.self_attn.q_norm.weight            0.00213  0.00591  0.00216  \n",
      "model.layers.28.self_attn.k_norm.weight            0.00201  0.00432  0.00139  \n",
      "model.layers.28.mlp.gate_proj.weight               0.11962  0.19275  0.09316  \n",
      "model.layers.28.mlp.up_proj.weight                 0.10347  0.17697  0.08066  \n",
      "model.layers.28.mlp.down_proj.weight               0.10339  0.17436  0.08142  \n",
      "model.layers.28.input_layernorm.weight             0.00166  0.00361  0.0007   \n",
      "model.layers.28.post_attention_layernorm.weight    0.00198  0.00372  0.00102  \n",
      "model.layers.29.self_attn.q_proj.weight            0.11738  0.20967  0.09267  \n",
      "model.layers.29.self_attn.k_proj.weight            0.12843  0.22316  0.10176  \n",
      "model.layers.29.self_attn.v_proj.weight            0.0993   0.1759   0.07778  \n",
      "model.layers.29.self_attn.o_proj.weight            0.11118  0.19784  0.08728  \n",
      "model.layers.29.self_attn.q_norm.weight            0.00432  0.00552  0.00326  \n",
      "model.layers.29.self_attn.k_norm.weight            0.00359  0.00443  0.00254  \n",
      "model.layers.29.mlp.gate_proj.weight               0.11714  0.19073  0.09116  \n",
      "model.layers.29.mlp.up_proj.weight                 0.10022  0.17463  0.07869  \n",
      "model.layers.29.mlp.down_proj.weight               0.10055  0.17206  0.0797   \n",
      "model.layers.29.input_layernorm.weight             0.00119  0.00285  0.00047  \n",
      "model.layers.29.post_attention_layernorm.weight    0.00191  0.0042   0.00104  \n",
      "model.layers.30.self_attn.q_proj.weight            0.12205  0.22271  0.09713  \n",
      "model.layers.30.self_attn.k_proj.weight            0.12253  0.21064  0.09791  \n",
      "model.layers.30.self_attn.v_proj.weight            0.09145  0.15669  0.0717   \n",
      "model.layers.30.self_attn.o_proj.weight            0.10756  0.192    0.08395  \n",
      "model.layers.30.self_attn.q_norm.weight            0.0024   0.00395  0.00228  \n",
      "model.layers.30.self_attn.k_norm.weight            0.00173  0.00314  0.00113  \n",
      "model.layers.30.mlp.gate_proj.weight               0.11659  0.19149  0.09075  \n",
      "model.layers.30.mlp.up_proj.weight                 0.10019  0.17545  0.07846  \n",
      "model.layers.30.mlp.down_proj.weight               0.10035  0.17344  0.07952  \n",
      "model.layers.30.input_layernorm.weight             0.00029  0.00198  0.0      \n",
      "model.layers.30.post_attention_layernorm.weight    0.00177  0.00419  0.00093  \n",
      "model.layers.31.self_attn.q_proj.weight            0.11626  0.21606  0.09456  \n",
      "model.layers.31.self_attn.k_proj.weight            0.12231  0.21699  0.09961  \n",
      "model.layers.31.self_attn.v_proj.weight            0.09055  0.16061  0.0723   \n",
      "model.layers.31.self_attn.o_proj.weight            0.10363  0.18394  0.08258  \n",
      "model.layers.31.self_attn.q_norm.weight            0.00209  0.00358  0.00176  \n",
      "model.layers.31.self_attn.k_norm.weight            0.0019   0.00371  0.00052  \n",
      "model.layers.31.mlp.gate_proj.weight               0.11695  0.19424  0.0909   \n",
      "model.layers.31.mlp.up_proj.weight                 0.09961  0.17579  0.07778  \n",
      "model.layers.31.mlp.down_proj.weight               0.10005  0.17355  0.07886  \n",
      "model.layers.31.input_layernorm.weight             0.00029  0.00256  0.00012  \n",
      "model.layers.31.post_attention_layernorm.weight    0.00182  0.00403  0.00084  \n",
      "model.layers.32.self_attn.q_proj.weight            0.11802  0.2196   0.0981   \n",
      "model.layers.32.self_attn.k_proj.weight            0.11742  0.21412  0.09891  \n",
      "model.layers.32.self_attn.v_proj.weight            0.09073  0.16624  0.07388  \n",
      "model.layers.32.self_attn.o_proj.weight            0.10378  0.18823  0.08265  \n",
      "model.layers.32.self_attn.q_norm.weight            0.00099  0.00507  0.00126  \n",
      "model.layers.32.self_attn.k_norm.weight            0.00081  0.00341  0.00098  \n",
      "model.layers.32.mlp.gate_proj.weight               0.12187  0.20249  0.0943   \n",
      "model.layers.32.mlp.up_proj.weight                 0.10132  0.17793  0.0786   \n",
      "model.layers.32.mlp.down_proj.weight               0.10009  0.17368  0.07864  \n",
      "model.layers.32.input_layernorm.weight             0.0001   0.00189  0.0      \n",
      "model.layers.32.post_attention_layernorm.weight    0.00198  0.004    0.00091  \n",
      "model.layers.33.self_attn.q_proj.weight            0.1221   0.21956  0.09717  \n",
      "model.layers.33.self_attn.k_proj.weight            0.13326  0.23794  0.10873  \n",
      "model.layers.33.self_attn.v_proj.weight            0.08572  0.15547  0.06891  \n",
      "model.layers.33.self_attn.o_proj.weight            0.09939  0.17009  0.07858  \n",
      "model.layers.33.self_attn.q_norm.weight            0.00416  0.00835  0.00264  \n",
      "model.layers.33.self_attn.k_norm.weight            0.00344  0.00638  0.00236  \n",
      "model.layers.33.mlp.gate_proj.weight               0.1245   0.20809  0.0962   \n",
      "model.layers.33.mlp.up_proj.weight                 0.10058  0.17662  0.07793  \n",
      "model.layers.33.mlp.down_proj.weight               0.09862  0.17153  0.07754  \n",
      "model.layers.33.input_layernorm.weight             0.00015  0.00096  3e-05    \n",
      "model.layers.33.post_attention_layernorm.weight    0.0018   0.00396  0.00075  \n",
      "model.layers.34.self_attn.q_proj.weight            0.12493  0.23832  0.10211  \n",
      "model.layers.34.self_attn.k_proj.weight            0.12034  0.22102  0.09997  \n",
      "model.layers.34.self_attn.v_proj.weight            0.07781  0.14095  0.06311  \n",
      "model.layers.34.self_attn.o_proj.weight            0.09713  0.17647  0.07677  \n",
      "model.layers.34.self_attn.q_norm.weight            0.00168  0.00507  0.00251  \n",
      "model.layers.34.self_attn.k_norm.weight            0.00153  0.00427  0.00185  \n",
      "model.layers.34.mlp.gate_proj.weight               0.11924  0.19988  0.09224  \n",
      "model.layers.34.mlp.up_proj.weight                 0.09635  0.1702   0.07477  \n",
      "model.layers.34.mlp.down_proj.weight               0.09473  0.16636  0.07627  \n",
      "model.layers.34.input_layernorm.weight             7e-05    0.00111  0.0      \n",
      "model.layers.34.post_attention_layernorm.weight    0.00112  0.00264  0.00052  \n",
      "model.layers.35.self_attn.q_proj.weight            0.1155   0.22096  0.09972  \n",
      "model.layers.35.self_attn.k_proj.weight            0.11443  0.20802  0.09898  \n",
      "model.layers.35.self_attn.v_proj.weight            0.07464  0.13084  0.06074  \n",
      "model.layers.35.self_attn.o_proj.weight            0.08738  0.15901  0.07254  \n",
      "model.layers.35.self_attn.q_norm.weight            0.00169  0.00734  0.00171  \n",
      "model.layers.35.self_attn.k_norm.weight            0.00096  0.00483  0.00102  \n",
      "model.layers.35.mlp.gate_proj.weight               0.10509  0.17653  0.08236  \n",
      "model.layers.35.mlp.up_proj.weight                 0.08701  0.15899  0.06873  \n",
      "model.layers.35.mlp.down_proj.weight               0.08933  0.15907  0.07352  \n",
      "model.layers.35.input_layernorm.weight             0.0      0.00079  0.0      \n",
      "model.layers.35.post_attention_layernorm.weight    0.00037  0.00251  0.00022  \n",
      "model.norm.weight                                  0.0043   0.00467  0.00047  \n"
     ]
    }
   ],
   "source": [
    "for key, value1 in insp1.model.named_parameters():\n",
    "    # if \"mlp\" not in key:\n",
    "    #     continue\n",
    "    value2 = insp2.model.get_parameter(key)\n",
    "    value3 = insp3.model.get_parameter(key)\n",
    "    value4 = insp4.model.get_parameter(key)\n",
    "    # print(value1.norm().item(), value2.norm().item(), (value1 - value2).norm().item())\n",
    "    print(\n",
    "        f\"{key: <50} \"\n",
    "        f\"{round(((value1 - value2).norm() / value1.norm()).item(), 5): <9}\"\n",
    "        f\"{round(((value2 - value3).norm() / value1.norm()).item(), 5): <9}\"\n",
    "        f\"{round(((value3 - value4).norm() / value1.norm()).item(), 5): <9}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "0547e3af-fe60-4ab8-abcd-d66e5edeae48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117.86233520507812 116.08011627197266 24.786317825317383\n",
      "0.21029888093471527\n"
     ]
    }
   ],
   "source": [
    "LAYER_NAME = \"model.layers.33.mlp.gate_proj.weight\"\n",
    "t1 = insp2.model.get_parameter(LAYER_NAME)\n",
    "t2 = insp3.model.get_parameter(LAYER_NAME)\n",
    "\n",
    "if False:\n",
    "    print(\"t1\", type(t1), t1.dtype, t1.shape)\n",
    "    print(t1)\n",
    "    print(\"t2\", type(t2), t2.dtype, t2.shape)\n",
    "    print(t2)\n",
    "\n",
    "print(t1.norm().item(), t2.norm().item(), (t1 - t2).norm().item())\n",
    "print(((t1 - t2).norm() / t1.norm()).item())\n",
    "diff = t2 - t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "eb2ae49c-8a0f-424d-8c9b-611bc7462dd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0071, -0.0258,  0.0045,  ...,  0.0244,  0.0255,  0.0009],\n",
       "        [ 0.0008, -0.0267, -0.0292,  ...,  0.0039,  0.0015,  0.0095],\n",
       "        [ 0.0038,  0.0172,  0.0088,  ...,  0.0045, -0.0007, -0.0034],\n",
       "        ...,\n",
       "        [ 0.0036, -0.0004, -0.0111,  ..., -0.0203, -0.0154,  0.0120],\n",
       "        [-0.0049,  0.0051, -0.0137,  ..., -0.0210, -0.0376, -0.0339],\n",
       "        [ 0.0006, -0.0002, -0.0315,  ...,  0.0181, -0.0220, -0.0042]],\n",
       "       device='mps:0', requires_grad=True)"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "fd4915e7-6b37-4406-b6e6-6181ca24f04b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0059, -0.0251,  0.0073,  ...,  0.0195,  0.0211,  0.0003],\n",
       "        [ 0.0007, -0.0294, -0.0306,  ...,  0.0017, -0.0011,  0.0085],\n",
       "        [ 0.0025,  0.0138,  0.0032,  ...,  0.0070, -0.0053,  0.0014],\n",
       "        ...,\n",
       "        [ 0.0036, -0.0024, -0.0079,  ..., -0.0161, -0.0114,  0.0195],\n",
       "        [-0.0052,  0.0011, -0.0109,  ..., -0.0245, -0.0317, -0.0344],\n",
       "        [ 0.0007,  0.0035, -0.0308,  ...,  0.0162, -0.0273,  0.0002]],\n",
       "       device='mps:0', requires_grad=True)"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "5e3419dd-c963-4568-a32b-b14a164004d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.8991699e-04, -1.1463165e-03,  5.7983398e-04, ...,\n",
       "         5.4931641e-03, -3.1280518e-04,  2.4108887e-03],\n",
       "       [ 0.0000000e+00, -7.3242188e-04, -1.0070801e-03, ...,\n",
       "        -6.1035156e-05, -2.6855469e-03,  3.1280518e-03],\n",
       "       [-1.3732910e-04, -7.6293945e-04, -4.5776367e-04, ...,\n",
       "        -3.1166077e-03, -2.4719238e-03,  0.0000000e+00],\n",
       "       ...,\n",
       "       [-6.5422058e-04,  6.1035156e-04, -2.3193359e-03, ...,\n",
       "         6.1035156e-05, -3.4179688e-03,  1.9531250e-03],\n",
       "       [ 3.8146973e-04, -1.2817383e-03,  1.5563965e-03, ...,\n",
       "         2.0751953e-03, -7.3242188e-04, -4.3945312e-03],\n",
       "       [ 4.5776367e-05, -1.6345978e-03,  1.8310547e-04, ...,\n",
       "         9.7656250e-04, -1.7089844e-03,  1.2817383e-03]], dtype=float32)"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "87b5ae34-180e-4286-9038-2c3aaa6c2e0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.4376025"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = ((diff - n * diff2) ** 2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "de707dea-1504-40af-a856-a0ee959c1053",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = torch.Tensor(diff)\n",
    "diff2 = torch.Tensor(diff2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "f2e3e7a1-32bc-4396-b3c9-94bf8c335a55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "827005db-e4e5-4e1a-a593-cacaa648bfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    target = ((diff - x * diff2) ** 2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa596902-5825-4dc7-83cb-0d6334fa3876",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "x = torch.nn.Parameter(torch.ones(1))\n",
    "optimizer = optim.SGD([x], lr=5e-4)\n",
    "for idx_ in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    target = ((x * diff - diff2) ** 2).sum()\n",
    "    target.backward()\n",
    "    optimizer.step()\n",
    "    if idx_ % 100 == 0:\n",
    "        print(idx_, round(target.item(), 4), x.item())\n",
    "    optimizer.zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "fbf84328-6ff8-432c-b41f-e4d8468b3a02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14.4177)"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(diff ** 2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "106c8490-2847-409c-90b8-25d598b7a33b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.8602e-06)"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((x*diff2) ** 2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9ddf26-fac7-4222-b4bf-ea24e9546276",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "1ddace35-ab01-42ca-a5b3-fb87cec4edb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.015686035, -0.013793945, -9.575563e-07)"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff.max(), diff.min(), diff.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "33c53fbd-0b94-4a5e-90b2-bf0242577226",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "t1 = t1.cpu().float().numpy()\n",
    "t2 = t2.cpu().float().numpy()\n",
    "u1, e1, v1 = np.linalg.svd(t1)\n",
    "u2, e2, v2 = np.linalg.svd(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "f63d0c04-2b3e-4e23-acf7-a54e5c040b3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.80738211, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 2.71025848, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 2.63031912, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "045e9f98-d2f3-47b6-a376-18cb2c765d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1024, 1024) (1024,) (2560, 2560)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(u1.shape, e1.shape, v1.shape)\n",
    "print(np.allclose(t1, u1 @ (np.eye(u1.shape[0], v1.shape[0]) * e1[...,None]) @ v1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "a9ea6137-7607-4bb5-b3a8-474d933c59ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129.79727 128.46214 1.341296\n",
      "0.0103337765\n"
     ]
    }
   ],
   "source": [
    "print(np.linalg.norm(e1), np.linalg.norm(e2), np.linalg.norm(e1-e2))\n",
    "print(np.linalg.norm(e1-e2) / np.linalg.norm(e1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "b8c75d2f-a28e-48b9-ae17-9409f9be3f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.05732\n",
      "98.056946\n",
      "76.6523\n"
     ]
    }
   ],
   "source": [
    "print(np.linalg.norm(u1))\n",
    "print(np.linalg.norm(u2))\n",
    "print(np.linalg.norm(u1 - u2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "09e1a665-0220-4dda-b367-8094a8ee003b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.592613\n",
      "50.592575\n",
      "71.661446\n"
     ]
    }
   ],
   "source": [
    "print(np.linalg.norm(v1))\n",
    "print(np.linalg.norm(v2))\n",
    "print(np.linalg.norm(v1 - v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "f7965690-4979-440d-91dd-50c5165cd754",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_estimate = u2 @ u1.T\n",
    "R_estimate = v1.T @ v2\n",
    "\n",
    "t2r = Q_estimate @ t1 @ R_estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "2798a5eb-ea68-4ab3-a0b0-9b94cfda63f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_orthonormal(mat):\n",
    "    return np.allclose(mat @ mat.T, np.eye(mat.shape[0]), atol=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "16fbc022-91a6-4832-889a-a1e66a2bd682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_orthonormal(R_estimate)=True\n",
      "is_orthonormal(Q_estimate)=True\n"
     ]
    }
   ],
   "source": [
    "print(f\"{is_orthonormal(R_estimate)=}\")\n",
    "print(f\"{is_orthonormal(Q_estimate)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "c633ed86-ee7a-4329-94f1-a97e9f53a244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00601287, -0.0256111 ,  0.00743343, ...,  0.01983054,\n",
       "         0.02145481,  0.0002663 ],\n",
       "       [ 0.00077698, -0.02950007, -0.03037294, ...,  0.00182957,\n",
       "        -0.00102945,  0.00871598],\n",
       "       [ 0.00259682,  0.01406311,  0.00370451, ...,  0.00710032,\n",
       "        -0.00536858,  0.00143731],\n",
       "       ...,\n",
       "       [ 0.0036581 , -0.00246513, -0.00813862, ..., -0.01640288,\n",
       "        -0.01156177,  0.01978756],\n",
       "       [-0.00533567,  0.00104097, -0.0108925 , ..., -0.02499235,\n",
       "        -0.03226556, -0.03496613],\n",
       "       [ 0.00072305,  0.00341415, -0.03137587, ...,  0.01648796,\n",
       "        -0.02776683,  0.00011106]], dtype=float32)"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "9de75c6b-4122-4db7-a785-0d7ec5ddd314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00588989, -0.02514648,  0.00726318, ...,  0.01953125,\n",
       "         0.02111816,  0.00027847],\n",
       "       [ 0.00074768, -0.02941895, -0.03063965, ...,  0.00171661,\n",
       "        -0.00106049,  0.00854492],\n",
       "       [ 0.00254822,  0.01379395,  0.00323486, ...,  0.00698853,\n",
       "        -0.00527954,  0.00138092],\n",
       "       ...,\n",
       "       [ 0.0035553 , -0.00238037, -0.00787354, ..., -0.01611328,\n",
       "        -0.01135254,  0.01953125],\n",
       "       [-0.00521851,  0.00106812, -0.01086426, ..., -0.02453613,\n",
       "        -0.03173828, -0.03442383],\n",
       "       [ 0.00070572,  0.00346375, -0.03076172, ...,  0.01623535,\n",
       "        -0.02734375,  0.00015068]], dtype=float32)"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "3835f22c-c233-4e69-8f2d-d1b24e4e2a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7968086 116.04533 0.015483679\n"
     ]
    }
   ],
   "source": [
    "n1_ = np.linalg.norm(t2 - t2r)\n",
    "n2_ = np.linalg.norm(t2)\n",
    "print(n1_, n2_, n1_/n2_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "1985a68f-5083-4c9e-afa8-32f8df4a45d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4483351273911421 38.832996 0.011545210756795375\n"
     ]
    }
   ],
   "source": [
    "RANGE = 0.02\n",
    "_temp2 = t2 * (np.random.random(t2.shape) * (RANGE * 2) + (1 - RANGE))\n",
    "\n",
    "n1_ = np.linalg.norm(t2 - _temp2)\n",
    "n2_ = np.linalg.norm(t2)\n",
    "print(n1_, n2_, n1_/n2_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "f7b2e6ad-9551-4320-b00b-c19e985416d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.266146205281482"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.cond(_temp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "046d0131-7654-4460-b3d5-ed4d9dc2da02",
   "metadata": {},
   "outputs": [],
   "source": [
    "u1, e1, v1 = np.linalg.svd(_temp2)\n",
    "u2, e2, v2 = np.linalg.svd(t2)\n",
    "\n",
    "Q_estimate = u2 @ u1.T\n",
    "R_estimate = v1.T @ v2\n",
    "\n",
    "t2r = Q_estimate @ _temp2 @ R_estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "90500657-69d5-4511-a8ed-61c6c87cc966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(is_orthonormal(Q_estimate))\n",
    "print(is_orthonormal(R_estimate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "e5315b1a-7536-4165-ba4b-af29bd225e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rand_vec_x vs rand_vec_y: 0.7192891430213466\n",
      "xt1 vs yt1: 0.7135275291744383\n",
      "xt2 vs yt2: 0.7108433342302861\n",
      "xt1 vs xt2: 0.09901417261696975\n",
      "yt1 vs yt2: 0.10296539536606987\n"
     ]
    }
   ],
   "source": [
    "# check angle\n",
    "rand_vec_x = np.random.random(2560)\n",
    "rand_vec_y = np.random.random(2560)\n",
    "\n",
    "xt1, yt1 = t1 @ rand_vec_x, t1 @ rand_vec_y\n",
    "xt2, yt2 = t2 @ rand_vec_x, t2 @ rand_vec_y\n",
    "\n",
    "print(\"rand_vec_x vs rand_vec_y:\", np.arccos(np.dot(rand_vec_x, rand_vec_y) / (np.linalg.norm(rand_vec_x) * np.linalg.norm(rand_vec_y))))\n",
    "print(\"xt1 vs yt1:\", np.arccos(np.dot(xt1, yt1) / (np.linalg.norm(xt1) * np.linalg.norm(yt1))))\n",
    "print(\"xt2 vs yt2:\", np.arccos(np.dot(xt2, yt2) / (np.linalg.norm(xt2) * np.linalg.norm(yt2))))\n",
    "\n",
    "print(\"xt1 vs xt2:\", np.arccos(np.dot(xt1, xt2) / (np.linalg.norm(xt1) * np.linalg.norm(xt2))))\n",
    "print(\"yt1 vs yt2:\", np.arccos(np.dot(yt1, yt2) / (np.linalg.norm(yt1) * np.linalg.norm(yt2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "666e5531-c256-4101-abab-3ad75aebc242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rand_vec_x vs rand_vec_y: 0.7285090154126664\n",
      "xt1 vs yt1: 0.7325410528129572\n",
      "xt2 vs yt2: 0.7322044993476181\n",
      "xt1 vs xt2: 0.011031557491365595\n",
      "yt1 vs yt2: 0.011443177644215792\n"
     ]
    }
   ],
   "source": [
    "# check angle\n",
    "rand_vec_x = np.random.random(2560)\n",
    "rand_vec_y = np.random.random(2560)\n",
    "\n",
    "xt1, yt1 = _temp2 @ rand_vec_x, _temp2 @ rand_vec_y\n",
    "xt2, yt2 = t2 @ rand_vec_x, t2 @ rand_vec_y\n",
    "\n",
    "print(\"rand_vec_x vs rand_vec_y:\", np.arccos(np.dot(rand_vec_x, rand_vec_y) / (np.linalg.norm(rand_vec_x) * np.linalg.norm(rand_vec_y))))\n",
    "print(\"xt1 vs yt1:\", np.arccos(np.dot(xt1, yt1) / (np.linalg.norm(xt1) * np.linalg.norm(yt1))))\n",
    "print(\"xt2 vs yt2:\", np.arccos(np.dot(xt2, yt2) / (np.linalg.norm(xt2) * np.linalg.norm(yt2))))\n",
    "\n",
    "print(\"xt1 vs xt2:\", np.arccos(np.dot(xt1, xt2) / (np.linalg.norm(xt1) * np.linalg.norm(xt2))))\n",
    "print(\"yt1 vs yt2:\", np.arccos(np.dot(yt1, yt2) / (np.linalg.norm(yt1) * np.linalg.norm(yt2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "978cbdeb-1eca-44fa-a518-67d920adff86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rand_vec_x vs rand_vec_y: 0.729927554869172\n",
      "xt1 vs yt1: 0.009483973003216658\n",
      "xt2 vs yt2: 0.009409912018757443\n",
      "xt1 vs xt2: 0.018416245722312687\n",
      "yt1 vs yt2: 0.018581222317443233\n"
     ]
    }
   ],
   "source": [
    "# check on random baseline\n",
    "randt1 = np.random.random(t1.shape) * 10\n",
    "randt2 = np.random.random(t2.shape) * 10\n",
    "\n",
    "rxt1, ryt1 = randt1 @ rand_vec_x, randt1 @ rand_vec_y\n",
    "rxt2, ryt2 = randt2 @ rand_vec_x, randt2 @ rand_vec_y\n",
    "\n",
    "print(\"rand_vec_x vs rand_vec_y:\", np.arccos(np.dot(rand_vec_x, rand_vec_y) / (np.linalg.norm(rand_vec_x) * np.linalg.norm(rand_vec_y))))\n",
    "print(\"xt1 vs yt1:\", np.arccos(np.dot(rxt1, ryt1) / (np.linalg.norm(rxt1) * np.linalg.norm(ryt1))))\n",
    "print(\"xt2 vs yt2:\", np.arccos(np.dot(rxt2, ryt2) / (np.linalg.norm(rxt2) * np.linalg.norm(ryt2))))\n",
    "\n",
    "print(\"xt1 vs xt2:\", np.arccos(np.dot(rxt1, rxt2) / (np.linalg.norm(rxt1) * np.linalg.norm(rxt2))))\n",
    "print(\"yt1 vs yt2:\", np.arccos(np.dot(ryt1, ryt2) / (np.linalg.norm(ryt1) * np.linalg.norm(ryt2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "e200af35-e340-4079-9038-57509a930726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(R_estimate @ R_estimate.T, np.eye(R_estimate.shape[0]), atol=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5465f724-d36d-4329-bc56-25ceb321b592",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_error = np.linalg.norm(t2 - B_reconstructed) / np.linalg.norm(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "187b3ec6-a8d8-4385-a7c6-169c7ff251cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.010714456"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstruction_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6b6f2561-f228-4f79-a8a1-8013f749ac19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00166288, -0.0050123 ,  0.01412025, ...,  0.00628763,\n",
       "        -0.02111104, -0.02216928],\n",
       "       [ 0.01900468, -0.00343603, -0.00034384, ..., -0.02169357,\n",
       "        -0.0283494 , -0.00522586],\n",
       "       [-0.01766192, -0.00646313, -0.00210069, ...,  0.00759203,\n",
       "         0.01060982, -0.01354084],\n",
       "       ...,\n",
       "       [ 0.0102808 , -0.00130459,  0.01009661, ..., -0.01138979,\n",
       "        -0.00855894, -0.04928847],\n",
       "       [ 0.00727988,  0.00059388,  0.004319  , ...,  0.03420987,\n",
       "        -0.00284377,  0.01817199],\n",
       "       [-0.00077205, -0.00020658,  0.00878151, ..., -0.05410355,\n",
       "        -0.02183168,  0.00393478]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B_reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aea9e351-ee28-4fe5-b8cf-4cd96394979c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00512695, -0.00081253, -0.00059891, ...,  0.02209473,\n",
       "        -0.00157928,  0.01013184],\n",
       "       [ 0.02441406,  0.0045166 , -0.0098877 , ...,  0.00561523,\n",
       "         0.00631714,  0.0144043 ],\n",
       "       [-0.00396729,  0.00634766,  0.00628662, ..., -0.02185059,\n",
       "         0.01147461,  0.01489258],\n",
       "       ...,\n",
       "       [ 0.00445557,  0.00430298,  0.01055908, ..., -0.05078125,\n",
       "         0.0480957 , -0.00176239],\n",
       "       [ 0.01269531, -0.00288391,  0.00364685, ..., -0.02404785,\n",
       "         0.00534058, -0.03100586],\n",
       "       [-0.00537109,  0.00491333,  0.00175476, ..., -0.01708984,\n",
       "         0.02697754, -0.0057373 ]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f690a9eb-72b1-483b-8f39-39343b1c8ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.1550946e-01 4.1381142e-01 4.0466297e-01 ... 1.1864436e-06 1.1709319e-06\n",
      " 1.0816107e-06]\n"
     ]
    }
   ],
   "source": [
    "from scipy.linalg import subspace_angles\n",
    "angles = subspace_angles(t1, t2)\n",
    "print(angles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c5c98418-69d7-4fec-8821-784dd92d38ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.4155, 0.4138, 0.4047, 0.3999, 0.3841, 0.3834, 0.3803, 0.3752,\n",
       "       0.3713, 0.3697, 0.3644, 0.361 , 0.3588, 0.354 , 0.3533, 0.3509,\n",
       "       0.3454, 0.3444, 0.3423, 0.3395], dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(angles, 4)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "add7458e-f8a4-44eb-a6c9-bf4a173b22e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.777748497929164"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.rad2deg(0.415)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "06f32446-7c6f-4ed2-b567-cb04720c010b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def draw_angle(radians):\n",
    "    fig, ax = plt.subplots(figsize=(4, 4))\n",
    "    \n",
    "    # Draw the angle arc\n",
    "    theta = np.linspace(0, radians, 100)\n",
    "    r = 0.3\n",
    "    x = r * np.cos(theta)\n",
    "    y = r * np.sin(theta)\n",
    "    ax.plot(x, y, 'b-', linewidth=1)\n",
    "    \n",
    "    # Draw the two rays (extend them further)\n",
    "    ax.plot([0, 1], [0, 0], 'k-', linewidth=1)  # Initial ray\n",
    "    ax.plot([0, np.cos(radians)], [0, np.sin(radians)], 'k-', linewidth=1)  # Final ray\n",
    "    \n",
    "    # Dynamic limits based on the angle\n",
    "    margin = 0.2\n",
    "    ax.set_xlim(-1-margin, 1+margin)\n",
    "    ax.set_ylim(-1-margin, 1+margin)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_title(f'Angle: {radians:.2f} radians ({np.degrees(radians):.1f}°)')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "f352b59f-579b-4271-9fa2-eea4a453f471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "draw_angle(0.105)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "26328024-8352-491e-9c5d-030c08c554a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5707963267948966"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.pi/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "edc1aeb6-7819-4d9c-ac30-84eabd0ac382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.15392195 0.78925163 0.70688412]\n",
      "[1.15392195 0.78925163 0.70688412]\n",
      "[1.15392195 0.78925163 0.70688412]\n"
     ]
    }
   ],
   "source": [
    "mat1 = np.random.random((3,2))\n",
    "mat2 = np.random.random((2,4))\n",
    "mat3 = mat1 @ mat2\n",
    "vec = np.random.random(4)\n",
    "print(mat3 @ vec)\n",
    "print(mat1 @ (mat2 @ vec))\n",
    "print(mat1 @ mat2 @ vec)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
